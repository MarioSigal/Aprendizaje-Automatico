{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP_2_Aprendizaje_Automatico_0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports\n"
      ],
      "metadata": {
        "id": "GJ8imGsu_vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wjfXeaWE_og8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FRAME\n"
      ],
      "metadata": {
        "id": "L_xHy5FQ_5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgdLKjU1JUOk",
        "outputId": "20a76927-3db3-4dcb-8d29-b40eb1886314"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "lCrU1I06J0fA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e932a9a5-e09d-4eb4-a167-1dbdf3026b49"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'fotos 2008-2019'   modelos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/dataset_datos_total3.csv\")"
      ],
      "metadata": {
        "id": "T8Hgq2B7uiQ7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar tokens por párrafo\n",
        "df_por_parrafos = df.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\":list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalización\": list\n",
        "}).reset_index()\n"
      ],
      "metadata": {
        "id": "I3yuvW8Esk5z"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows(): # Correctly iterate over DataFrame rows\n",
        "  bi = 0\n",
        "  bf = 0\n",
        "  bc = 0\n",
        "  if row[\"punt_inicial\"] is None:\n",
        "    bi += 1\n",
        "  if row[\"punt_final\"] is None:\n",
        "    bf += 1\n",
        "  if row[\"capitalización\"] is None:\n",
        "    bc += 1\n",
        "\n",
        "print(bi,bf,bc)"
      ],
      "metadata": {
        "id": "W-ZElgGrwoyU",
        "outputId": "916c452e-2bb1-4202-9170-c83b80df8eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4044589601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Correctly iterate over DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mbi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"punt_inicial\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0musing_cow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0musing_cow\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_single_block\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_references\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.data_manager\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 if (\n\u001b[1;32m    608\u001b[0m                     \u001b[0mobject_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m     return lib.maybe_convert_objects(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;31m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/numeric.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, device, like)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_array_function_like_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b14c685"
      },
      "source": [
        "import math\n",
        "\n",
        "def count_nans_in_df_lists(dataframe, columns_to_check=None):\n",
        "    \"\"\"\n",
        "    Cuenta los valores NaN en las listas contenidas en las columnas especificadas de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): El DataFrame a revisar.\n",
        "        columns_to_check (list, optional): Lista de nombres de columnas a revisar.\n",
        "                                          Si es None, revisa 'punt_inicial', 'punt_final', 'capitalización'.\n",
        "\n",
        "    Returns:\n",
        "        dict: Un diccionario con el conteo de NaN por columna.\n",
        "    \"\"\"\n",
        "    if columns_to_check is None:\n",
        "        columns_to_check = ['punt_inicial', 'punt_final', 'capitalización']\n",
        "\n",
        "    nan_counts = {col: 0 for col in columns_to_check}\n",
        "    total_nans_found = 0\n",
        "\n",
        "    for col in columns_to_check:\n",
        "        if col not in dataframe.columns:\n",
        "            print(f\"Advertencia: La columna '{col}' no se encuentra en el DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        for index, row_list in dataframe[col].items():\n",
        "            if isinstance(row_list, list):\n",
        "                for item in row_list:\n",
        "                    if isinstance(item, float) and math.isnan(item):\n",
        "                        nan_counts[col] += 1\n",
        "                        total_nans_found += 1\n",
        "            elif isinstance(row_list, float) and math.isnan(row_list): # Handle cases where the cell itself is NaN, not a list\n",
        "                nan_counts[col] += 1\n",
        "                total_nans_found += 1\n",
        "\n",
        "    print(f\"Resumen de NaN encontrados por columna:\")\n",
        "    for col, count in nan_counts.items():\n",
        "        print(f\"  - '{col}': {count} NaN(s)\")\n",
        "    print(f\"Total de NaN encontrados en todas las columnas: {total_nans_found}\")\n",
        "\n",
        "    return nan_counts"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f09ac04",
        "outputId": "bca1b24e-3d17-4bef-8cbb-b0053df43387"
      },
      "source": [
        "import math\n",
        "\n",
        "def filter_rows_with_nans_in_lists(df, columns):\n",
        "    \"\"\"\n",
        "    Filtra las filas de un DataFrame donde alguna de las listas en las columnas especificadas\n",
        "    contenga valores NaN.\n",
        "    \"\"\"\n",
        "    initial_rows = len(df)\n",
        "    print(f\"Filas iniciales en el DataFrame: {initial_rows}\")\n",
        "\n",
        "    # Crear una máscara booleana para las filas a mantener (True si NO hay NaN)\n",
        "    mask = pd.Series([True] * len(df), index=df.index)\n",
        "\n",
        "    for col in columns:\n",
        "        # Para cada fila, comprueba si alguna lista contiene NaN\n",
        "        # Aplicamos `.all()` para asegurarnos de que solo se considere False si hay al menos un NaN\n",
        "        col_mask = df[col].apply(lambda lst: not any(isinstance(x, float) and math.isnan(x) for x in lst) if isinstance(lst, list) else (not (isinstance(lst, float) and math.isnan(lst))))\n",
        "        mask = mask & col_mask\n",
        "\n",
        "    df_filtered = df[mask].reset_index(drop=True)\n",
        "    removed_rows = initial_rows - len(df_filtered)\n",
        "    print(f\"Filas con NaN eliminadas: {removed_rows}\")\n",
        "    print(f\"Filas restantes en el DataFrame: {len(df_filtered)}\")\n",
        "    return df_filtered\n",
        "\n",
        "# Columnas donde buscar NaN en las listas\n",
        "columns_to_check = ['punt_inicial', 'punt_final', 'capitalización']\n",
        "\n",
        "# Filtrar df_por_parrafos\n",
        "df_por_parrafos = filter_rows_with_nans_in_lists(df_por_parrafos, columns_to_check)\n",
        "\n",
        "# Opcional: Volver a verificar los NaN después de filtrar\n",
        "print(\"\\nVerificando NaN después de filtrar:\")\n",
        "count_nans_in_df_lists(df_por_parrafos)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas iniciales en el DataFrame: 136061\n",
            "Filas con NaN eliminadas: 0\n",
            "Filas restantes en el DataFrame: 136061\n",
            "\n",
            "Verificando NaN después de filtrar:\n",
            "Resumen de NaN encontrados por columna:\n",
            "  - 'punt_inicial': 0 NaN(s)\n",
            "  - 'punt_final': 0 NaN(s)\n",
            "  - 'capitalización': 0 NaN(s)\n",
            "Total de NaN encontrados en todas las columnas: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'punt_inicial': 0, 'punt_final': 0, 'capitalización': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "164dc533",
        "outputId": "638cb959-3f6b-4a6d-b262-8430326b6059"
      },
      "source": [
        "print(\"Contando NaN en df_por_parrafos...\")\n",
        "count_nans_in_df_lists(df_por_parrafos)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contando NaN en df_por_parrafos...\n",
            "Resumen de NaN encontrados por columna:\n",
            "  - 'punt_inicial': 0 NaN(s)\n",
            "  - 'punt_final': 0 NaN(s)\n",
            "  - 'capitalización': 0 NaN(s)\n",
            "Total de NaN encontrados en todas las columnas: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'punt_inicial': 0, 'punt_final': 0, 'capitalización': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos[\"token\"][1])\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "eXGAW5MJvkds",
        "outputId": "086e0a1d-682b-4923-c66e-44187b35eef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'nacimiento', 'de', 'un', 'lucha', '##dor', 'por', 'la', 'libertad', 'capítulo', '11', 'capítulo', '12', 'capítulo', '13', 'capítulo', '14', 'parte', 'cuarta', 'la', 'lucha', 'es', 'mi', 'vida', 'capítulo', '15', 'capítulo', '16', 'capítulo', '17', 'capítulo', '18', 'capítulo', '19', 'capítulo', '20', 'capítulo', '21', 'capítulo', '22', 'parte', 'quinta', 'trai', '##ción', 'capítulo', '23', 'capítulo', '24', 'capítulo', '25', 'capítulo', '26', 'capítulo', '27', 'capítulo', '28', 'capítulo', '29', 'capítulo', '30', 'capítulo', '31', 'capítulo', '32', 'capítulo', '33', 'capítulo', '34', 'capítulo', '35', 'capítulo', '36', 'capítulo', '37', 'capítulo', '38', 'capítulo', '39', 'parte', 'sexta']\n",
            "(7555044, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER & BERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "uVHH_Ow__8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##TOKENIZER & BERT EMBEDDINGS\"\"\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert.eval()\n"
      ],
      "metadata": {
        "id": "LYMyfzMG_3JA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10aa805a-9267-423c-ae79-281649775b45"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelar parámetros de BERT para ahorrar memoria\n",
        "for param in model_bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "89bEgteB49eA"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CREAR CLASE DATASET PARA PYTORCH"
      ],
      "metadata": {
        "id": "A1Zpp0aEjuXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DynamicEmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset de pytorch que calcula embeddings dinámicamente\"\"\"\n",
        "    def __init__(self, df, tokenizer, bert_model):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.embedding_matrix = bert_model.embeddings.word_embeddings.weight\n",
        "        self.embedding_matrix.requires_grad = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        token_id_list = row[\"token_id\"]\n",
        "\n",
        "        # Calcular embeddings dinámicamente\n",
        "        token_embeddings = []\n",
        "        for token_id in token_id_list:\n",
        "            if token_id is None or token_id == self.tokenizer.unk_token_id:\n",
        "                token_id = self.tokenizer.unk_token_id\n",
        "            emb = self.embedding_matrix[token_id].detach()\n",
        "            token_embeddings.append(emb)\n",
        "\n",
        "        # Convertir a tensor\n",
        "        if token_embeddings:\n",
        "            embeddings = torch.stack(token_embeddings)\n",
        "        else:\n",
        "            embeddings = torch.empty(0, self.embedding_matrix.shape[1])\n",
        "\n",
        "        # Preparar labels\n",
        "        labels = {\n",
        "            \"punt_inicial\": torch.tensor(row[\"punt_inicial\"], dtype=torch.long),\n",
        "            \"punt_final\": torch.tensor(row[\"punt_final\"], dtype=torch.long),\n",
        "            \"capitalización\": torch.tensor(row[\"capitalización\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        return embeddings, labels"
      ],
      "metadata": {
        "id": "OBX8JOtxkzcM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PADDING"
      ],
      "metadata": {
        "id": "x65RnXFBwyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    capitalizacion = pad_sequence([l[\"capitalización\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "a66xammXw1L7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA LOADER"
      ],
      "metadata": {
        "id": "B627zo2Af9ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "#print(int(len(df_por_parrafos)*0.9)) # da 122454, despues agarrar tail de eso\n",
        "df_por_parrafos_test = df_por_parrafos\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Usar el nuevo dataset con embeddings dinámicos\n",
        "dataset = DynamicEmbeddingDataset(df_por_parrafos_test, tokenizer, model_bert)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "OX_Q5HbTHeSh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Unidireccional"
      ],
      "metadata": {
        "id": "uhpy5I7nAGf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder"
      ],
      "metadata": {
        "id": "Yltb4uccAzpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "rKk-_53cAtTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder"
      ],
      "metadata": {
        "id": "mkQ1YRp9BObz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderUnidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "vEtXxjVLBRkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder–Decoder"
      ],
      "metadata": {
        "id": "8fwGhoZxBU4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloUnidireccional, self).__init__()\n",
        "        self.encoder = EncoderUnidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderUnidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "qsDT7j4uBZv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Bidireccional\n"
      ],
      "metadata": {
        "id": "swyed0hyHn2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n"
      ],
      "metadata": {
        "id": "4OvlT4HSH9tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderBidireccional, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch, seq_len, hidden_dim * 2)\n",
        "        # hidden: (num_layers * 2, batch, hidden_dim)\n",
        "        # cell:   (num_layers * 2, batch, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "LL12AahSHmwh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder"
      ],
      "metadata": {
        "id": "if_B5n0nIDlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim*2,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch, seq_len, hidden_dim * 2)\n",
        "        hidden/cell: (num_layers * 2, batch, hidden_dim)\n",
        "        \"\"\"\n",
        "        h = hidden[::2].contiguous()\n",
        "        c = cell[::2].contiguous()\n",
        "\n",
        "        outputs, _ = self.lstm(encoder_outputs, (h, c))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "eSAKUiQqIC__"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder-Decoder"
      ],
      "metadata": {
        "id": "5sjESbtwIHv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloBidireccional, self).__init__()\n",
        "        self.encoder = EncoderBidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderBidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "eDN-EGnEILMC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ENTRENAR MODELO\n"
      ],
      "metadata": {
        "id": "gg_COwTOxHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "# Dividir en train/validation\n",
        "train_df, val_df = train_test_split(df_por_parrafos, test_size=0.15, random_state=42)\n",
        "\n",
        "train_dataset = DynamicEmbeddingDataset(train_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "val_dataset = DynamicEmbeddingDataset(val_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model = ModeloBidireccional(embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Mover BERT a la misma device\n",
        "model_bert = model_bert.to(device)\n",
        "\n",
        "# Optimizador con weight decay (regularización L2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.2, patience=3\n",
        ")\n",
        "\n",
        "# Pesos diferentes para cada tarea (opcional, ajustar según importancia)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "# Gradient clipping\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "early_stop_patience = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ENTRENAMIENTO\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_loss_inicial = 0\n",
        "    total_loss_final = 0\n",
        "    total_loss_cap = 0\n",
        "\n",
        "    for embeddings, labels in train_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        loss_inicial = 100*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) #testeando, le pongo 100* a los inicial porque es muy chica\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        # Pérdida total (puedes ponderar cada tarea si lo deseas)\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping para evitar gradientes explosivos\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_loss_inicial += loss_inicial.item()\n",
        "        total_loss_final += loss_final.item()\n",
        "        total_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # VALIDACIÓN\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_loss_inicial = 0\n",
        "    val_loss_final = 0\n",
        "    val_loss_cap = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in val_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "\n",
        "            loss_inicial = 100*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])  #testeando, le pongo 100* a los inicial porque es muy chica\n",
        "\n",
        "            loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "\n",
        "            loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "            loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            val_loss_inicial += loss_inicial.item()\n",
        "            val_loss_final += loss_final.item()\n",
        "            val_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # Actualizar learning rate\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Imprimir métricas detalladas\n",
        "    print(f\"Época {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Train - Inicial: {total_loss_inicial/len(train_loader):.4f}, Final: {total_loss_final/len(train_loader):.4f}, Cap: {total_loss_cap/len(train_loader):.4f}\")\n",
        "    print(f\"  Val   - Inicial: {val_loss_inicial/len(val_loader):.4f}, Final: {val_loss_final/len(val_loader):.4f}, Cap: {val_loss_cap/len(val_loader):.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        ruta_mejor_modelo = \"/content/drive/MyDrive/modelos/mejor_modelo.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, ruta_mejor_modelo)\n",
        "        print(f\"  ✓ Mejor modelo guardado (Val Loss: {avg_val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  Sin mejora ({patience_counter}/{early_stop_patience})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= early_stop_patience:\n",
        "        print(f\"\\nEarly stopping en época {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Guardar modelo final\n",
        "ruta_modelo_final = \"/content/drive/MyDrive/modelos/modelo_final_30e.pt\"\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'val_loss': avg_val_loss,\n",
        "}, ruta_modelo_final)\n",
        "\n",
        "print(f\"\\nEntrenamiento completado!\")\n",
        "print(f\"Modelo final guardado en: {ruta_modelo_final}\")\n",
        "print(f\"Mejor modelo guardado en: {ruta_mejor_modelo}\")\n",
        "print(f\"Mejor Val Loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "T6miBWEAxWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80d62e9d",
        "outputId": "f9a8b36c-02b9-4d04-e8fd-9895b37a1f50"
      },
      "source": [
        "def check_df_consistency(dataframe):\n",
        "    \"\"\"\n",
        "    Verifica la consistencia de las longitudes de 'token', 'punt_inicial',\n",
        "    'punt_final' y 'capitalización' en cada fila del DataFrame.\n",
        "    \"\"\"\n",
        "    mismatched_instances = []\n",
        "    for idx, row in dataframe.iterrows():\n",
        "        len_tokens = len(row[\"token\"])\n",
        "        len_punt_inicial = len(row[\"punt_inicial\"])\n",
        "        len_punt_final = len(row[\"punt_final\"])\n",
        "        len_capitalizacion = len(row[\"capitalización\"])\n",
        "\n",
        "        if not (len_tokens == len_punt_inicial == len_punt_final == len_capitalizacion):\n",
        "            mismatched_instances.append(row[\"instancia_id\"])\n",
        "    return mismatched_instances\n",
        "\n",
        "print(\"Realizando verificación de consistencia en df_por_parrafos...\")\n",
        "inconsistent_ids = check_df_consistency(df_por_parrafos)\n",
        "\n",
        "if inconsistent_ids:\n",
        "    print(f\"¡Advertencia! Se encontraron {len(inconsistent_ids)} instancias con longitudes inconsistentes.\")\n",
        "    print(f\"Primeras 10 instancia_id con problemas: {inconsistent_ids[:10]}\")\n",
        "    print(\"Es recomendable filtrar estas instancias antes de proceder con el entrenamiento.\")\n",
        "else:\n",
        "    print(\"¡Excelente! Todas las instancias en df_por_parrafos tienen longitudes consistentes.\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Realizando verificación de consistencia en df_por_parrafos...\n",
            "¡Excelente! Todas las instancias en df_por_parrafos tienen longitudes consistentes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##INFERENCIA - CARGAR MODELO Y HACER PREDICCIONES\"\"\"\n",
        "\n",
        "def predecir_parrafo(texto, modelo, tokenizer, bert_model, device):\n",
        "    \"\"\"\n",
        "    Recibe un párrafo de texto y devuelve las predicciones de puntuación y capitalización.\n",
        "\n",
        "    Args:\n",
        "        texto: string con el texto a procesar\n",
        "        modelo: modelo entrenado\n",
        "        tokenizer: tokenizer de BERT\n",
        "        bert_model: modelo BERT para embeddings\n",
        "        device: 'cuda' o 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        dict con tokens y sus predicciones\n",
        "    \"\"\"\n",
        "    modelo.eval()\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "    # Obtener embeddings\n",
        "    token_embeddings = []\n",
        "    for token in tokens:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "            token_id = tokenizer.unk_token_id\n",
        "        emb = bert_model.embeddings.word_embeddings.weight[token_id].detach()\n",
        "        token_embeddings.append(emb)\n",
        "\n",
        "    # Convertir a tensor y añadir dimensión de batch\n",
        "    embeddings = torch.stack(token_embeddings).unsqueeze(0).to(device)  # (1, seq_len, 768)\n",
        "\n",
        "    # Hacer predicción\n",
        "    with torch.no_grad():\n",
        "        outputs = modelo(embeddings)\n",
        "\n",
        "    # Obtener las clases predichas\n",
        "    punt_inicial_pred = torch.argmax(outputs[\"puntuación inicial\"], dim=-1).squeeze().cpu().numpy()\n",
        "    punt_final_pred = torch.argmax(outputs[\"puntuación final\"], dim=-1).squeeze().cpu().numpy()\n",
        "    capital_pred = torch.argmax(outputs[\"capitalización\"], dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Mapeos de clases a etiquetas legibles\n",
        "    punt_inicial_map = {0: \"Sin puntuación\", 1: \"Con puntuación\"}\n",
        "    punt_final_map = {0: \"Ninguna\", 1: \"Punto\", 2: \"Coma\", 3: \"Otro\"}\n",
        "    capital_map = {0: \"Minúscula\", 1: \"Primera mayúscula\", 2: \"Todo mayúsculas\", 3: \"Otro\"}\n",
        "\n",
        "    # Crear resultado\n",
        "    resultados = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        resultados.append({\n",
        "            \"token\": token,\n",
        "            \"puntuación_inicial\": punt_inicial_map.get(int(punt_inicial_pred[i]), \"Desconocido\"),\n",
        "            \"puntuación_final\": punt_final_map.get(int(punt_final_pred[i]), \"Desconocido\"),\n",
        "            \"capitalización\": capital_map.get(int(capital_pred[i]), \"Desconocido\")\n",
        "        })\n",
        "\n",
        "    return resultados\n",
        "\n",
        "def reconstruir_texto(resultados):\n",
        "    \"\"\"\n",
        "    Reconstruye el texto con puntuación y capitalización a partir de las predicciones.\n",
        "    \"\"\"\n",
        "    texto_reconstruido = \"\"\n",
        "\n",
        "    for resultado in resultados:\n",
        "        token = resultado[\"token\"]\n",
        "\n",
        "        # Aplicar capitalización\n",
        "        if resultado[\"capitalización\"] == \"Primera mayúscula\":\n",
        "            token = token.capitalize()\n",
        "        elif resultado[\"capitalización\"] == \"Todo mayúsculas\":\n",
        "            token = token.upper()\n",
        "\n",
        "        # Añadir puntuación inicial\n",
        "        if resultado[\"puntuación_inicial\"] == \"Con puntuación\":\n",
        "            # Aquí podrías decidir qué puntuación añadir (ej. ¿, ¡, etc.)\n",
        "            pass\n",
        "\n",
        "        # Añadir el token\n",
        "        if token.startswith(\"##\"):\n",
        "            texto_reconstruido += token[2:]\n",
        "        else:\n",
        "            if texto_reconstruido:\n",
        "                texto_reconstruido += \" \" + token\n",
        "            else:\n",
        "                texto_reconstruido += token\n",
        "\n",
        "        # Añadir puntuación final\n",
        "        if resultado[\"puntuación_final\"] == \"Punto\":\n",
        "            texto_reconstruido += \".\"\n",
        "        elif resultado[\"puntuación_final\"] == \"Coma\":\n",
        "            texto_reconstruido += \",\"\n",
        "\n",
        "    return texto_reconstruido"
      ],
      "metadata": {
        "id": "jKz-D7Vl_C8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar el mejor modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Cargando el mejor modelo...\")\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/modelos/modelo_final_30e.pt\", map_location=device)\n",
        "modelo_cargado = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3)\n",
        "modelo_cargado.load_state_dict(checkpoint['model_state_dict'])\n",
        "modelo_cargado = modelo_cargado.to(device)\n",
        "print(f\"Modelo cargado (época {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n",
        "\n",
        "# Ejemplo de uso\n",
        "texto_ejemplo = \"había perdido la esperanza pero seguía creyendo porque sabía que era un tenista demasiado bueno para no ganar también aquí pero no me he alegrado cuando  ha perdido lo respeto demasiado estabas nervioso antes de la final\"\n",
        "print(f\"\\nTexto de entrada: '{texto_ejemplo}'\\n\")\n",
        "\n",
        "predicciones = predecir_parrafo(texto_ejemplo, modelo_cargado, tokenizer, model_bert, device)\n",
        "\n",
        "# Mostrar predicciones detalladas\n",
        "print(\"Predicciones por token:\")\n",
        "print(\"-\" * 80)\n",
        "for pred in predicciones:\n",
        "    print(f\"Token: {pred['token']:15s} | Punt. Inicial: {pred['puntuación_inicial']:20s} | \"\n",
        "          f\"Punt. Final: {pred['puntuación_final']:10s} | Cap: {pred['capitalización']}\")\n",
        "\n",
        "# Reconstruir texto\n",
        "texto_reconstruido = reconstruir_texto(predicciones)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Texto reconstruido: '{texto_reconstruido}'\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGf4pyY9_JId",
        "outputId": "4c3cceca-4de6-4ecf-bed3-ec12bc240a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando el mejor modelo...\n",
            "Modelo cargado (época 29, val_loss: 0.3581)\n",
            "\n",
            "Texto de entrada: 'había perdido la esperanza pero seguía creyendo porque sabía que era un tenista demasiado bueno para no ganar también aquí pero no me he alegrado cuando rafa ha perdido lo respeto demasiado estabas nervioso antes de la final'\n",
            "\n",
            "Predicciones por token:\n",
            "--------------------------------------------------------------------------------\n",
            "Token: había           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Primera mayúscula\n",
            "Token: perdido         | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: la              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: esperanza       | Punt. Inicial: Sin puntuación       | Punt. Final: Coma       | Cap: Minúscula\n",
            "Token: pero            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: seg             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##uía           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: c               | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##rey           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##endo          | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: porque          | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: sa              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##bía           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: que             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: era             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: un              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: tenis           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##ta            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: demasiado       | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: buen            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##o             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: para            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: no              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ganar           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: también         | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: aquí            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: pero            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Primera mayúscula\n",
            "Token: no              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: me              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: he              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ale             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##grado         | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: cuando          | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ra              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##fa            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Primera mayúscula\n",
            "Token: ha              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: perdido         | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: lo              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: resp            | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##eto           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: demasiado       | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: estaba          | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Primera mayúscula\n",
            "Token: ##s             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Primera mayúscula\n",
            "Token: ner             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##vios          | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: ##o             | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: antes           | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: de              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: la              | Punt. Inicial: Sin puntuación       | Punt. Final: Ninguna    | Cap: Minúscula\n",
            "Token: final           | Punt. Inicial: Sin puntuación       | Punt. Final: Punto      | Cap: Minúscula\n",
            "\n",
            "================================================================================\n",
            "Texto reconstruido: 'Había perdido la esperanza, pero seguía creyendo porque sabía que era un tenista demasiado bueno para no ganar también aquí Pero no me he alegrado cuando rafa ha perdido lo respeto demasiado Estabas nervioso antes de la final.'\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nfDDceeA72h",
        "outputId": "85718b80-0c45-4f6b-fb92-dc511b2d0f02"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final',\n",
            "       'capitalización'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    }
  ]
}