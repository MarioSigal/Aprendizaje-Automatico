{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmU/yQ/qmrwg9PGVdm6wm4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP_2_Aprendizaje_Automatico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports\n"
      ],
      "metadata": {
        "id": "GJ8imGsu_vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wjfXeaWE_og8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FRAME\n"
      ],
      "metadata": {
        "id": "L_xHy5FQ_5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#leer el dataframe\n",
        "df = pd.read_csv(\"mi_dataset.csv\")\n",
        "\n",
        "# Agrupar tokens por p√°rrafo\n",
        "df_por_parrafos = df.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\":list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalizaci√≥n\": list\n",
        "}).reset_index()"
      ],
      "metadata": {
        "id": "SlHuzgJ1_4PU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GL2SzTAh_jl",
        "outputId": "90dc16b0-4089-4dbd-d544-daab4275d427"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      instancia_id                                           token_id  \\\n",
            "0                0  [178, 179, 45006, 11945, 10413, 10104, 15761, ...   \n",
            "1                1  [10228, 24922, 21744, 82405, 11681, 96739, 106...   \n",
            "2                2  [10110, 10185, 40790, 10109, 38532, 16924, 150...   \n",
            "3                3  [10228, 12477, 74288, 90034, 11153, 10173, 335...   \n",
            "4                4  [10182, 74371, 14386, 10104, 31040, 60083, 106...   \n",
            "...            ...                                                ...   \n",
            "3278          3278  [54866, 10127, 10212, 96503, 16085, 10795, 649...   \n",
            "3279          3279  [94354, 16719, 17518, 10406, 54373, 10537, 101...   \n",
            "3280          3280  [10840, 12343, 10107, 46934, 10157, 10141, 349...   \n",
            "3281          3281  [46934, 10157, 10152, 75980, 56474, 10443, 101...   \n",
            "3282          3282  [10220, 32784, 10165, 45006, 11945, 10110, 116...   \n",
            "\n",
            "                                                  token  \\\n",
            "0     [j, k, row, ##ling, 31, de, julio, de, 1965, y...   \n",
            "1     [ha, escrito, asi, ##mismo, tres, vol√∫menes, c...   \n",
            "2     [en, 2012, cre√≥, la, plataforma, digital, pot,...   \n",
            "3     [ha, sido, gala, ##rdon, ##ada, con, numerosos...   \n",
            "4     [los, aldea, ##nos, de, peque√±o, hang, ##leto,...   \n",
            "...                                                 ...   \n",
            "3278  [sali√≥, del, com, ##parti, ##miento, sin, deja...   \n",
            "3279  [t√≠o, ver, ##non, lo, espera, ##ba, al, otro, ...   \n",
            "3280  [ad, ##i√≥, ##s, harr, ##y, le, dijo, her, ##mi...   \n",
            "3281  [harr, ##y, les, gu, ##i√±, ##√≥, un, ojo, se, v...   \n",
            "3282  [para, pet, ##er, row, ##ling, en, re, ##cuerd...   \n",
            "\n",
            "                                           punt_inicial  \\\n",
            "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "...                                                 ...   \n",
            "3278  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3279  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3280  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3281  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3282  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                             punt_final  \\\n",
            "0     [1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, ...   \n",
            "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3     [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "...                                                 ...   \n",
            "3278  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
            "3279  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
            "3280  [0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, ...   \n",
            "3281  [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3282  [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, ...   \n",
            "\n",
            "                                         capitalizaci√≥n  \n",
            "0     [1, 1, 3, 3, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, ...  \n",
            "1     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
            "2     [1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, ...  \n",
            "3     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...  \n",
            "4     [3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "...                                                 ...  \n",
            "3278  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "3279  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
            "3280  [1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "3281  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...  \n",
            "3282  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, ...  \n",
            "\n",
            "[3283 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER & BERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "uVHH_Ow__8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "LYMyfzMG_3JA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69edc42f-63b1-480c-f0c5-3c7aee0b46c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_multilingual_token_embedding(token: str):\n",
        "  \"\"\"\n",
        "    Devuelve el embedding (est√°tico) para el token.\n",
        "  \"\"\"\n",
        "  token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "  if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "    print(f\"‚ùå El token '{token}' no pertenece al vocabulario de multilingual BERT.\")\n",
        "    return None\n",
        "  embedding_vector = model.embeddings.word_embeddings.weight[token_id]\n",
        "  print(f\"‚úÖ Token: '{token}' | ID: {token_id}\")\n",
        "  print(f\"Embedding shape: {embedding_vector.shape}\")\n",
        "  return embedding_vector"
      ],
      "metadata": {
        "id": "OlxS0tN-AHBg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"a qu√© hora pasa el ciento siete\"\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "print(tokens)\n",
        "# ['a', 'qu√©', 'hora', 'pasa', 'el', 'cien', '##to', 'siete']\n",
        "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens_id)\n",
        "# [169, 38188, 24301, 26088, 10125, 99485, 10340, 28394]\n",
        "embedding_vector = model.embeddings.word_embeddings.weight[tokens_id]\n",
        "print(embedding_vector.shape)\n",
        "# torch.Size([8, 768]"
      ],
      "metadata": {
        "id": "0XB27V_SAKue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6aff7da-7adc-42dc-8703-d06177695add"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'qu√©', 'hora', 'pasa', 'el', 'cien', '##to', 'siete']\n",
            "[169, 38188, 24301, 26088, 10125, 99485, 10340, 28394]\n",
            "torch.Size([8, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Precomputo de embeddings\n",
        "embeddings_list = []\n",
        "\n",
        "for tokens in df_por_parrafos[\"token\"]:\n",
        "    token_embeddings = []\n",
        "    for token in tokens:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "            token_id = tokenizer.unk_token_id\n",
        "        embedding_vector = model.embeddings.word_embeddings.weight[token_id]\n",
        "        token_embeddings.append(embedding_vector)\n",
        "    paragraph_tensor = torch.stack(token_embeddings).detach()  # üîπ importante detach\n",
        "    embeddings_list.append(paragraph_tensor)\n",
        "\n",
        "df_por_parrafos[\"embeddings\"] = embeddings_list\n",
        "\n"
      ],
      "metadata": {
        "id": "Xj9gxOOYDtPW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos)"
      ],
      "metadata": {
        "id": "WslJy6DGGnOT",
        "outputId": "13f22665-fd6e-4f41-fa56-b5610ee330f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      instancia_id                                           token_id  \\\n",
            "0                0  [178, 179, 45006, 11945, 10413, 10104, 15761, ...   \n",
            "1                1  [10228, 24922, 21744, 82405, 11681, 96739, 106...   \n",
            "2                2  [10110, 10185, 40790, 10109, 38532, 16924, 150...   \n",
            "3                3  [10228, 12477, 74288, 90034, 11153, 10173, 335...   \n",
            "4                4  [10182, 74371, 14386, 10104, 31040, 60083, 106...   \n",
            "...            ...                                                ...   \n",
            "3278          3278  [54866, 10127, 10212, 96503, 16085, 10795, 649...   \n",
            "3279          3279  [94354, 16719, 17518, 10406, 54373, 10537, 101...   \n",
            "3280          3280  [10840, 12343, 10107, 46934, 10157, 10141, 349...   \n",
            "3281          3281  [46934, 10157, 10152, 75980, 56474, 10443, 101...   \n",
            "3282          3282  [10220, 32784, 10165, 45006, 11945, 10110, 116...   \n",
            "\n",
            "                                                  token  \\\n",
            "0     [j, k, row, ##ling, 31, de, julio, de, 1965, y...   \n",
            "1     [ha, escrito, asi, ##mismo, tres, vol√∫menes, c...   \n",
            "2     [en, 2012, cre√≥, la, plataforma, digital, pot,...   \n",
            "3     [ha, sido, gala, ##rdon, ##ada, con, numerosos...   \n",
            "4     [los, aldea, ##nos, de, peque√±o, hang, ##leto,...   \n",
            "...                                                 ...   \n",
            "3278  [sali√≥, del, com, ##parti, ##miento, sin, deja...   \n",
            "3279  [t√≠o, ver, ##non, lo, espera, ##ba, al, otro, ...   \n",
            "3280  [ad, ##i√≥, ##s, harr, ##y, le, dijo, her, ##mi...   \n",
            "3281  [harr, ##y, les, gu, ##i√±, ##√≥, un, ojo, se, v...   \n",
            "3282  [para, pet, ##er, row, ##ling, en, re, ##cuerd...   \n",
            "\n",
            "                                           punt_inicial  \\\n",
            "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "...                                                 ...   \n",
            "3278  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3279  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3280  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3281  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3282  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                             punt_final  \\\n",
            "0     [1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, ...   \n",
            "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3     [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "...                                                 ...   \n",
            "3278  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, ...   \n",
            "3279  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
            "3280  [0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, ...   \n",
            "3281  [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3282  [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, ...   \n",
            "\n",
            "                                         capitalizaci√≥n  \\\n",
            "0     [1, 1, 3, 3, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, ...   \n",
            "1     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
            "2     [1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, ...   \n",
            "3     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...   \n",
            "4     [3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "...                                                 ...   \n",
            "3278  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3279  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
            "3280  [1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "3281  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
            "3282  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, ...   \n",
            "\n",
            "                                             embeddings  \n",
            "0     [[tensor(0.0226), tensor(-0.0351), tensor(0.05...  \n",
            "1     [[tensor(0.0343), tensor(-0.0208), tensor(-0.0...  \n",
            "2     [[tensor(-0.0053), tensor(-0.0356), tensor(0.0...  \n",
            "3     [[tensor(0.0343), tensor(-0.0208), tensor(-0.0...  \n",
            "4     [[tensor(-0.0277), tensor(-0.0444), tensor(0.0...  \n",
            "...                                                 ...  \n",
            "3278  [[tensor(-0.0936), tensor(0.0076), tensor(-0.0...  \n",
            "3279  [[tensor(-0.0006), tensor(-0.0779), tensor(0.0...  \n",
            "3280  [[tensor(-0.0323), tensor(-0.0256), tensor(0.1...  \n",
            "3281  [[tensor(0.0284), tensor(-0.0253), tensor(0.01...  \n",
            "3282  [[tensor(-0.0230), tensor(-0.0116), tensor(-0....  \n",
            "\n",
            "[3283 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CREAR CLASE DATASET PARA PYTORCH"
      ],
      "metadata": {
        "id": "A1Zpp0aEjuXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    \"\"\"clase Dataset de pytorch\"\"\"\n",
        "    def __init__(self,df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        embeddings = torch.tensor(np.array(row[\"embeddings\"]),dtype=torch.float)\n",
        "\n",
        "        labels = {\n",
        "            \"punt_inicial\": torch.tensor(row[\"punt_inicial\"]),\n",
        "            \"punt_final\": torch.tensor(row[\"punt_final\"]),\n",
        "            \"capitalizaci√≥n\": torch.tensor(row[\"capitalizaci√≥n\"])\n",
        "        }\n",
        "\n",
        "        return embeddings, labels"
      ],
      "metadata": {
        "id": "OBX8JOtxkzcM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PADDING"
      ],
      "metadata": {
        "id": "x65RnXFBwyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "    capitalizacion = pad_sequence([l[\"capitalizaci√≥n\"] for l in labels_list], batch_first=True, padding_value=-100)\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "a66xammXw1L7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA LOADER"
      ],
      "metadata": {
        "id": "B627zo2Af9ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "df_por_parrafos_test = df_por_parrafos.head(100)\n",
        "print(df_por_parrafos_test)"
      ],
      "metadata": {
        "id": "OX_Q5HbTHeSh",
        "outputId": "7d3eb53f-6ecf-4d33-f5b3-f707a16189e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    instancia_id                                           token_id  \\\n",
            "0              0  [178, 179, 45006, 11945, 10413, 10104, 15761, ...   \n",
            "1              1  [10228, 24922, 21744, 82405, 11681, 96739, 106...   \n",
            "2              2  [10110, 10185, 40790, 10109, 38532, 16924, 150...   \n",
            "3              3  [10228, 12477, 74288, 90034, 11153, 10173, 335...   \n",
            "4              4  [10182, 74371, 14386, 10104, 31040, 60083, 106...   \n",
            "..           ...                                                ...   \n",
            "95            95  [10109, 26142, 31285, 10288, 10104, 46934, 101...   \n",
            "96            96  [36579, 12655, 33198, 10225, 22536, 10109, 623...   \n",
            "97            97  [13672, 18278, 10157, 12028, 10690, 12655, 332...   \n",
            "98            98  [10795, 14668, 11504, 61816, 10911, 10228, 380...   \n",
            "99            99  [18419, 66558, 31355, 46934, 10157, 10192, 115...   \n",
            "\n",
            "                                                token  \\\n",
            "0   [j, k, row, ##ling, 31, de, julio, de, 1965, y...   \n",
            "1   [ha, escrito, asi, ##mismo, tres, vol√∫menes, c...   \n",
            "2   [en, 2012, cre√≥, la, plataforma, digital, pot,...   \n",
            "3   [ha, sido, gala, ##rdon, ##ada, con, numerosos...   \n",
            "4   [los, aldea, ##nos, de, peque√±o, hang, ##leto,...   \n",
            "..                                                ...   \n",
            "95  [la, l√°, ##mpa, ##ra, de, harr, ##y, pare, ##c...   \n",
            "96  [aqu√≠, todo, sigue, como, siempre, la, dieta, ...   \n",
            "97  [yo, esto, ##y, bien, sobre, todo, gracias, a,...   \n",
            "98  [sin, embargo, esta, ma√±ana, me, ha, pasado, a...   \n",
            "99  [s√≠, pen, ##s√≥, harr, ##y, no, est√°, mal, as√≠,...   \n",
            "\n",
            "                                         punt_inicial  \\\n",
            "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "..                                                ...   \n",
            "95  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "96  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "97  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "98  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "99  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                           punt_final  \\\n",
            "0   [1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, ...   \n",
            "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3   [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "..                                                ...   \n",
            "95  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "96  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
            "97  [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "98  [0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "99  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                       capitalizaci√≥n  \\\n",
            "0   [1, 1, 3, 3, 2, 0, 0, 0, 2, 1, 1, 1, 1, 1, 0, ...   \n",
            "1   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
            "2   [1, 2, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, ...   \n",
            "3   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, ...   \n",
            "4   [3, 3, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "..                                                ...   \n",
            "95  [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "96  [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, ...   \n",
            "97  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "98  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
            "99  [1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                           embeddings  \n",
            "0   [[tensor(0.0226), tensor(-0.0351), tensor(0.05...  \n",
            "1   [[tensor(0.0343), tensor(-0.0208), tensor(-0.0...  \n",
            "2   [[tensor(-0.0053), tensor(-0.0356), tensor(0.0...  \n",
            "3   [[tensor(0.0343), tensor(-0.0208), tensor(-0.0...  \n",
            "4   [[tensor(-0.0277), tensor(-0.0444), tensor(0.0...  \n",
            "..                                                ...  \n",
            "95  [[tensor(-0.0066), tensor(-0.0325), tensor(0.0...  \n",
            "96  [[tensor(0.0081), tensor(-0.0155), tensor(-0.0...  \n",
            "97  [[tensor(0.0095), tensor(-0.0690), tensor(-0.0...  \n",
            "98  [[tensor(-0.0268), tensor(0.0029), tensor(0.05...  \n",
            "99  [[tensor(-0.0340), tensor(-0.0731), tensor(-0....  \n",
            "\n",
            "[100 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = Dataset(df_por_parrafos_test)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "tgIU8jELxC7T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Unidireccional"
      ],
      "metadata": {
        "id": "uhpy5I7nAGf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder"
      ],
      "metadata": {
        "id": "Yltb4uccAzpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "rKk-_53cAtTV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder"
      ],
      "metadata": {
        "id": "mkQ1YRp9BObz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderUnidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Funci√≥n de activaci√≥n para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"puntuaci√≥n inicial\": punt_inicial_logits,\n",
        "            \"puntuaci√≥n final\": punt_final_logits,\n",
        "            \"capitalizaci√≥n\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "vEtXxjVLBRkR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder‚ÄìDecoder"
      ],
      "metadata": {
        "id": "8fwGhoZxBU4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloUnidireccional, self).__init__()\n",
        "        self.encoder = EncoderUnidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderUnidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "qsDT7j4uBZv0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Bidireccional\n"
      ],
      "metadata": {
        "id": "swyed0hyHn2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n"
      ],
      "metadata": {
        "id": "4OvlT4HSH9tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "LL12AahSHmwh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder"
      ],
      "metadata": {
        "id": "if_B5n0nIDlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Funci√≥n de activaci√≥n para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuaci√≥n inicial\": punt_inicial_logits,\n",
        "            \"puntuaci√≥n final\": punt_final_logits,\n",
        "            \"capitalizaci√≥n\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "eSAKUiQqIC__"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder-Decoder"
      ],
      "metadata": {
        "id": "5sjESbtwIHv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloBidireccional, self).__init__()\n",
        "        self.encoder = EncoderBidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderBidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "eDN-EGnEILMC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ENTRENAR MODELO\n"
      ],
      "metadata": {
        "id": "gg_COwTOxHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # ignorar padding\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for embeddings, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(embeddings)  # diccionario con tus tres salidas\n",
        "\n",
        "        loss_inicial = criterion(outputs[\"puntuaci√≥n inicial\"].permute(0,2,1), labels[\"punt_inicial\"])\n",
        "        loss_final = criterion(outputs[\"puntuaci√≥n final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalizaci√≥n\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "T6miBWEAxWsq"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}