{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl1opeX4dFQR65bPY5Du8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP_2_Aprendizaje_Automatico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports\n"
      ],
      "metadata": {
        "id": "GJ8imGsu_vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjfXeaWE_og8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA SET\n"
      ],
      "metadata": {
        "id": "L_xHy5FQ_5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlHuzgJ1_4PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER & BERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "uVHH_Ow__8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "LYMyfzMG_3JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_multilingual_token_embedding(token: str):\n",
        "  \"\"\"\n",
        "    Devuelve el embedding (estático) para el token.\n",
        "  \"\"\"\n",
        "  token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "  if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "    print(f\"❌ El token '{token}' no pertenece al vocabulario de multilingual BERT.\")\n",
        "    return None\n",
        "  embedding_vector = model.embeddings.word_embeddings.weight[token_id]\n",
        "  print(f\"✅ Token: '{token}' | ID: {token_id}\")\n",
        "  print(f\"Embedding shape: {embedding_vector.shape}\")\n",
        "  return embedding_vector"
      ],
      "metadata": {
        "id": "OlxS0tN-AHBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"a qué hora pasa el ciento siete\"\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "print(tokens)\n",
        "# ['a', 'qué', 'hora', 'pasa', 'el', 'cien', '##to', 'siete']\n",
        "tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens_id)\n",
        "# [169, 38188, 24301, 26088, 10125, 99485, 10340, 28394]\n",
        "embedding_vector = model.embeddings.word_embeddings.weight[tokens_id]\n",
        "print(embedding_vector.shape)\n",
        "# torch.Size([8, 768]"
      ],
      "metadata": {
        "id": "0XB27V_SAKue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Unidireccional"
      ],
      "metadata": {
        "id": "uhpy5I7nAGf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder"
      ],
      "metadata": {
        "id": "Yltb4uccAzpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "rKk-_53cAtTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder"
      ],
      "metadata": {
        "id": "mkQ1YRp9BObz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderUnidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "vEtXxjVLBRkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder–Decoder"
      ],
      "metadata": {
        "id": "8fwGhoZxBU4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PunctuationRestorationModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(PunctuationRestorationModel, self).__init__()\n",
        "        self.encoder = EncoderUnidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderUnidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "qsDT7j4uBZv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Bidireccional\n"
      ],
      "metadata": {
        "id": "swyed0hyHn2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n"
      ],
      "metadata": {
        "id": "4OvlT4HSH9tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "LL12AahSHmwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decoder"
      ],
      "metadata": {
        "id": "if_B5n0nIDlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "eSAKUiQqIC__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Encoder-Decoder"
      ],
      "metadata": {
        "id": "5sjESbtwIHv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PunctuationRestorationModel(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(PunctuationRestorationModel, self).__init__()\n",
        "        self.encoder = EncoderBidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderBidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "eDN-EGnEILMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}