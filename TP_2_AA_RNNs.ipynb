{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP_2_AA_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TP2 Aprendizaje Automatico\n",
        "Archivo de los modelos basados en RNN"
      ],
      "metadata": {
        "id": "LHJFvvd8rclU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IMPORTS & DRIVE\n"
      ],
      "metadata": {
        "id": "GJ8imGsu_vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjfXeaWE_og8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rgdLKjU1JUOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER & BERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "uVHH_Ow__8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cargamos bert\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert.eval()"
      ],
      "metadata": {
        "id": "LYMyfzMG_3JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATASET"
      ],
      "metadata": {
        "id": "6qteVmN1qvss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ABRIMOS Y DAMOS FORMATO AL DATASET"
      ],
      "metadata": {
        "id": "bsVa0ABkrJ34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/colab/dataset_datos_total3.csv\")"
      ],
      "metadata": {
        "id": "T8Hgq2B7uiQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar tokens por párrafo\n",
        "df_por_parrafos = df.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\":list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalización\": list\n",
        "}).reset_index()\n"
      ],
      "metadata": {
        "id": "I3yuvW8Esk5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CREAMOS CLASE DATASET DE PYTORCH"
      ],
      "metadata": {
        "id": "A1Zpp0aEjuXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DynamicEmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset de pytorch que calcula embeddings dinámicamente\"\"\"\n",
        "    def __init__(self, df, tokenizer, bert_model, modo=\"train\"):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.embedding_matrix = bert_model.embeddings.word_embeddings.weight\n",
        "        self.embedding_matrix.requires_grad = False\n",
        "        self.modo = modo  # \"train\" o \"pred\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        token_id_list = row[\"token_id\"]\n",
        "\n",
        "        # Calcular embeddings dinámicamente\n",
        "        token_embeddings = []\n",
        "        for token_id in token_id_list:\n",
        "            if token_id is None or token_id == self.tokenizer.unk_token_id:\n",
        "                token_id = self.tokenizer.unk_token_id\n",
        "            emb = self.embedding_matrix[token_id].detach()\n",
        "            token_embeddings.append(emb)\n",
        "\n",
        "        # Convertir a tensor\n",
        "        if token_embeddings:\n",
        "            embeddings = torch.stack(token_embeddings)\n",
        "        else:\n",
        "            embeddings = torch.empty(0, self.embedding_matrix.shape[1])\n",
        "\n",
        "        if self.modo == \"pred\": #para predecir, no devuelve los labels\n",
        "            return embeddings, None\n",
        "\n",
        "        # Preparar labels\n",
        "        labels = {\n",
        "            \"punt_inicial\": torch.tensor(row[\"punt_inicial\"], dtype=torch.long),\n",
        "            \"punt_final\": torch.tensor(row[\"punt_final\"], dtype=torch.long),\n",
        "            \"capitalización\": torch.tensor(row[\"capitalización\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        return embeddings, labels"
      ],
      "metadata": {
        "id": "OBX8JOtxkzcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FUNCIÓN DE PADDING"
      ],
      "metadata": {
        "id": "x65RnXFBwyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    if labels_list[0] is None: # si estamos prediciendo, no tenemos labels\n",
        "        return embeddings_padded, None\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    capitalizacion = pad_sequence([l[\"capitalización\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "a66xammXw1L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ARQUITECTURAS"
      ],
      "metadata": {
        "id": "W89WmANfqlJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RNN Unidireccional"
      ],
      "metadata": {
        "id": "uhpy5I7nAGf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Unidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim = 768, hidden_dim1 = 128, hidden_dim2 = 32, num_layers= 2, dropout= 0.4):\n",
        "        super(RNN_Unidireccional, self).__init__()\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim1,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # Unidireccional\n",
        "            )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Salidas\n",
        "        self.punt_inicial_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 2)\n",
        "            )\n",
        "        self.punt_final_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "        self.capital_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Pasamos los embeddings por la lstm\n",
        "        outputs, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Les hacemos dropout a los outputs\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # Pasamos los outputs de la lstm por las salidas\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "rKk-_53cAtTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Bidireccional\n"
      ],
      "metadata": {
        "id": "swyed0hyHn2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Bidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim = 768, hidden_dim1 = 64, hidden_dim2 = 32, num_layers= 2, dropout= 0.4): #reducimos hidden_dim1 para equiparar con la red unidireccional( 64*2 = 128)\n",
        "        super(RNN_Bidireccional, self).__init__()\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim1,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # Bidireccional\n",
        "            )\n",
        "\n",
        "        lstm_output_dim = hidden_dim1 * 2 #128 igual que en la Unidireccinal\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Salidas\n",
        "        self.punt_inicial_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 2)\n",
        "            )\n",
        "        self.punt_final_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "        self.capital_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Pasamos los embeddings por la lstm\n",
        "        outputs, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Les hacemos dropout a los outputs\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # Pasamos los outputs de la lstm por las salidas\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }"
      ],
      "metadata": {
        "id": "shJOmhfqmuuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ENTRENAMIENTO DE LOS MODELOS"
      ],
      "metadata": {
        "id": "gg_COwTOxHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "# Dividir en train/validation/test\n",
        "train_df, temp = train_test_split(df_por_parrafos, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "val_df, test_df = train_test_split(temp, test_size=0.5, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = DynamicEmbeddingDataset(train_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "val_dataset = DynamicEmbeddingDataset(val_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "3zALNZA7iF2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ENTRENAMIENTO UNIDIRECCIONAL\n",
        "entrenamos por 30 epochs quedandonos con todos los modelos y viendo cual es el mejor segun como varia la loss, en train y val, epoch por epoch.\n",
        "\n",
        "MEJOR MODELO: ÉPOCA 12"
      ],
      "metadata": {
        "id": "FjrecG48ebez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Mover BERT a la misma device\n",
        "model_bert = model_bert.to(device)\n",
        "\n",
        "# Optimizador con weight decay (regularización L2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.2, patience=3\n",
        ")\n",
        "\n",
        "# Pesos diferentes para cada tarea (opcional, ajustar según importancia)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
        "\n",
        "# Gradient clipping\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "early_stop_patience = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ENTRENAMIENTO\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_loss_inicial = 0\n",
        "    total_loss_final = 0\n",
        "    total_loss_cap = 0\n",
        "\n",
        "    for embeddings, labels in train_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # le ponemos 2* a loss inicial para equiparar en magnitud con las demas\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        # Pérdida total\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping para evitar gradientes explosivos\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_loss_inicial += loss_inicial.item()\n",
        "        total_loss_final += loss_final.item()\n",
        "        total_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # VALIDACIÓN\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_loss_inicial = 0\n",
        "    val_loss_final = 0\n",
        "    val_loss_cap = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in val_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "\n",
        "            loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])  # le ponemos 2* a loss inicial para equiparar en magnitud con las demas\n",
        "\n",
        "            loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "\n",
        "            loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "            loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            val_loss_inicial += loss_inicial.item()\n",
        "            val_loss_final += loss_final.item()\n",
        "            val_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # Actualizar learning rate\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Imprimir métricas detalladas\n",
        "    print(f\"Época {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Train - Inicial: {total_loss_inicial/len(train_loader):.4f}, Final: {total_loss_final/len(train_loader):.4f}, Cap: {total_loss_cap/len(train_loader):.4f}\")\n",
        "    print(f\"  Val   - Inicial: {val_loss_inicial/len(val_loader):.4f}, Final: {val_loss_final/len(val_loader):.4f}, Cap: {val_loss_cap/len(val_loader):.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "    # Guardar cada epoca\n",
        "    ruta_epoca = f\"/content/drive/MyDrive/colab/modelosU/modelo_epoca_{epoch+1}.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': avg_val_loss,\n",
        "        }, ruta_epoca)\n",
        "\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        ruta_mejor_modelo = \"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, ruta_mejor_modelo)\n",
        "        print(f\"  ✓ Mejor modelo actualizado (Val Loss: {avg_val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  Sin mejora ({patience_counter}/{early_stop_patience})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= early_stop_patience:\n",
        "        print(f\"\\nEarly stopping en época {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "print(f\"\\nEntrenamiento completado!\")\n",
        "print(f\"Mejor modelo guardado en: {ruta_mejor_modelo}\")\n",
        "print(f\"Mejor Val Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "#Mostramos lo que printeo durante el entrenamiento\n",
        "\n",
        "\"\"\"\n",
        "Época 1/30\n",
        "  Train Loss: 1.4731 | Val Loss: 1.4118\n",
        "  Train - Inicial: 0.4391, Final: 0.5594, Cap: 0.4747\n",
        "  Val   - Inicial: 0.4289, Final: 0.5407, Cap: 0.4423\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.4118)\n",
        "------------------------------------------------------------\n",
        "Época 2/30\n",
        "  Train Loss: 1.4212 | Val Loss: 1.3945\n",
        "  Train - Inicial: 0.4318, Final: 0.5423, Cap: 0.4470\n",
        "  Val   - Inicial: 0.4282, Final: 0.5333, Cap: 0.4330\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3945)\n",
        "------------------------------------------------------------\n",
        "Época 3/30\n",
        "  Train Loss: 1.4088 | Val Loss: 1.3856\n",
        "  Train - Inicial: 0.4308, Final: 0.5378, Cap: 0.4401\n",
        "  Val   - Inicial: 0.4277, Final: 0.5289, Cap: 0.4290\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3856)\n",
        "------------------------------------------------------------\n",
        "Época 4/30\n",
        "  Train Loss: 1.4012 | Val Loss: 1.3827\n",
        "  Train - Inicial: 0.4304, Final: 0.5351, Cap: 0.4358\n",
        "  Val   - Inicial: 0.4276, Final: 0.5277, Cap: 0.4275\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3827)\n",
        "------------------------------------------------------------\n",
        "Época 5/30\n",
        "  Train Loss: 1.3952 | Val Loss: 1.3796\n",
        "  Train - Inicial: 0.4301, Final: 0.5329, Cap: 0.4322\n",
        "  Val   - Inicial: 0.4272, Final: 0.5269, Cap: 0.4255\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3796)\n",
        "------------------------------------------------------------\n",
        "Época 6/30\n",
        "  Train Loss: 1.3904 | Val Loss: 1.3784\n",
        "  Train - Inicial: 0.4298, Final: 0.5315, Cap: 0.4291\n",
        "  Val   - Inicial: 0.4273, Final: 0.5244, Cap: 0.4266\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3784)\n",
        "------------------------------------------------------------\n",
        "Época 7/30\n",
        "  Train Loss: 1.3865 | Val Loss: 1.3751\n",
        "  Train - Inicial: 0.4295, Final: 0.5302, Cap: 0.4268\n",
        "  Val   - Inicial: 0.4270, Final: 0.5240, Cap: 0.4241\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3751)\n",
        "------------------------------------------------------------\n",
        "Época 8/30\n",
        "  Train Loss: 1.3829 | Val Loss: 1.3750\n",
        "  Train - Inicial: 0.4292, Final: 0.5292, Cap: 0.4245\n",
        "  Val   - Inicial: 0.4270, Final: 0.5233, Cap: 0.4247\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3750)\n",
        "------------------------------------------------------------\n",
        "Época 9/30\n",
        "  Train Loss: 1.3799 | Val Loss: 1.3747\n",
        "  Train - Inicial: 0.4290, Final: 0.5283, Cap: 0.4226\n",
        "  Val   - Inicial: 0.4271, Final: 0.5231, Cap: 0.4246\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3747)\n",
        "------------------------------------------------------------\n",
        "Época 10/30\n",
        "  Train Loss: 1.3770 | Val Loss: 1.3750\n",
        "  Train - Inicial: 0.4287, Final: 0.5274, Cap: 0.4208\n",
        "  Val   - Inicial: 0.4272, Final: 0.5231, Cap: 0.4247\n",
        "  LR: 0.001000\n",
        "  Sin mejora (1/30)\n",
        "------------------------------------------------------------\n",
        "Época 11/30\n",
        "  Train Loss: 1.3750 | Val Loss: 1.3759\n",
        "  Train - Inicial: 0.4286, Final: 0.5268, Cap: 0.4197\n",
        "  Val   - Inicial: 0.4272, Final: 0.5228, Cap: 0.4260\n",
        "  LR: 0.001000\n",
        "  Sin mejora (2/30)\n",
        "------------------------------------------------------------\n",
        "Época 12/30\n",
        "  Train Loss: 1.3729 | Val Loss: 1.3747\n",
        "  Train - Inicial: 0.4285, Final: 0.5262, Cap: 0.4182\n",
        "  Val   - Inicial: 0.4271, Final: 0.5221, Cap: 0.4254\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.3747)\n",
        "------------------------------------------------------------\n",
        "Época 13/30\n",
        "  Train Loss: 1.3708 | Val Loss: 1.3758\n",
        "  Train - Inicial: 0.4282, Final: 0.5257, Cap: 0.4169\n",
        "  Val   - Inicial: 0.4272, Final: 0.5222, Cap: 0.4265\n",
        "  LR: 0.000200\n",
        "  Sin mejora (1/30)\n",
        "------------------------------------------------------------\n",
        "Época 14/30\n",
        "  Train Loss: 1.3635 | Val Loss: 1.3755\n",
        "  Train - Inicial: 0.4275, Final: 0.5231, Cap: 0.4129\n",
        "  Val   - Inicial: 0.4270, Final: 0.5218, Cap: 0.4266\n",
        "  LR: 0.000200\n",
        "  Sin mejora (2/30)\n",
        "------------------------------------------------------------\n",
        "Época 15/30\n",
        "  Train Loss: 1.3618 | Val Loss: 1.3758\n",
        "  Train - Inicial: 0.4273, Final: 0.5227, Cap: 0.4118\n",
        "  Val   - Inicial: 0.4270, Final: 0.5216, Cap: 0.4272\n",
        "  LR: 0.000200\n",
        "  Sin mejora (3/30)\n",
        "------------------------------------------------------------\n",
        "Época 16/30\n",
        "  Train Loss: 1.3604 | Val Loss: 1.3763\n",
        "  Train - Inicial: 0.4272, Final: 0.5223, Cap: 0.4109\n",
        "  Val   - Inicial: 0.4270, Final: 0.5215, Cap: 0.4277\n",
        "  LR: 0.000200\n",
        "  Sin mejora (4/30)\n",
        "------------------------------------------------------------\n",
        "Época 17/30\n",
        "  Train Loss: 1.3595 | Val Loss: 1.3765\n",
        "  Train - Inicial: 0.4271, Final: 0.5221, Cap: 0.4103\n",
        "  Val   - Inicial: 0.4270, Final: 0.5217, Cap: 0.4278\n",
        "  LR: 0.000040\n",
        "  Sin mejora (5/30)\n",
        "------------------------------------------------------------\n",
        "Época 18/30\n",
        "  Train Loss: 1.3576 | Val Loss: 1.3773\n",
        "  Train - Inicial: 0.4269, Final: 0.5213, Cap: 0.4094\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4286\n",
        "  LR: 0.000040\n",
        "  Sin mejora (6/30)\n",
        "------------------------------------------------------------\n",
        "Época 19/30\n",
        "  Train Loss: 1.3571 | Val Loss: 1.3773\n",
        "  Train - Inicial: 0.4268, Final: 0.5212, Cap: 0.4092\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4286\n",
        "  LR: 0.000040\n",
        "  Sin mejora (7/30)\n",
        "------------------------------------------------------------\n",
        "Época 20/30\n",
        "  Train Loss: 1.3567 | Val Loss: 1.3776\n",
        "  Train - Inicial: 0.4267, Final: 0.5211, Cap: 0.4089\n",
        "  Val   - Inicial: 0.4271, Final: 0.5217, Cap: 0.4288\n",
        "  LR: 0.000040\n",
        "  Sin mejora (8/30)\n",
        "------------------------------------------------------------\n",
        "Época 21/30\n",
        "  Train Loss: 1.3570 | Val Loss: 1.3775\n",
        "  Train - Inicial: 0.4268, Final: 0.5212, Cap: 0.4089\n",
        "  Val   - Inicial: 0.4271, Final: 0.5217, Cap: 0.4287\n",
        "  LR: 0.000008\n",
        "  Sin mejora (9/30)\n",
        "------------------------------------------------------------\n",
        "Época 22/30\n",
        "  Train Loss: 1.3564 | Val Loss: 1.3776\n",
        "  Train - Inicial: 0.4268, Final: 0.5208, Cap: 0.4088\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4289\n",
        "  LR: 0.000008\n",
        "  Sin mejora (10/30)\n",
        "------------------------------------------------------------\n",
        "Época 23/30\n",
        "  Train Loss: 1.3562 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4266, Final: 0.5209, Cap: 0.4086\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4291\n",
        "  LR: 0.000008\n",
        "  Sin mejora (11/30)\n",
        "------------------------------------------------------------\n",
        "Época 24/30\n",
        "  Train Loss: 1.3562 | Val Loss: 1.3777\n",
        "  Train - Inicial: 0.4266, Final: 0.5209, Cap: 0.4086\n",
        "  Val   - Inicial: 0.4271, Final: 0.5217, Cap: 0.4290\n",
        "  LR: 0.000008\n",
        "  Sin mejora (12/30)\n",
        "------------------------------------------------------------\n",
        "Época 25/30\n",
        "  Train Loss: 1.3560 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4266, Final: 0.5207, Cap: 0.4086\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4290\n",
        "  LR: 0.000002\n",
        "  Sin mejora (13/30)\n",
        "------------------------------------------------------------\n",
        "Época 26/30\n",
        "  Train Loss: 1.3561 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4266, Final: 0.5209, Cap: 0.4086\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4290\n",
        "  LR: 0.000002\n",
        "  Sin mejora (14/30)\n",
        "------------------------------------------------------------\n",
        "Época 27/30\n",
        "  Train Loss: 1.3561 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4267, Final: 0.5209, Cap: 0.4085\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4290\n",
        "  LR: 0.000002\n",
        "  Sin mejora (15/30)\n",
        "------------------------------------------------------------\n",
        "Época 28/30\n",
        "  Train Loss: 1.3562 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4267, Final: 0.5210, Cap: 0.4085\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4291\n",
        "  LR: 0.000002\n",
        "  Sin mejora (16/30)\n",
        "------------------------------------------------------------\n",
        "Época 29/30\n",
        "  Train Loss: 1.3560 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4266, Final: 0.5209, Cap: 0.4085\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4291\n",
        "  LR: 0.000000\n",
        "  Sin mejora (17/30)\n",
        "------------------------------------------------------------\n",
        "Época 30/30\n",
        "  Train Loss: 1.3561 | Val Loss: 1.3778\n",
        "  Train - Inicial: 0.4266, Final: 0.5209, Cap: 0.4086\n",
        "  Val   - Inicial: 0.4271, Final: 0.5216, Cap: 0.4291\n",
        "  LR: 0.000000\n",
        "  Sin mejora (18/30)\n",
        "------------------------------------------------------------\n",
        "\n",
        "Entrenamiento completado!\n",
        "Mejor modelo guardado en: /content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\n",
        "Mejor Val Loss: 1.3747\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T6miBWEAxWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ENTRENAMIENTO BIDIRECCIONAL\n",
        "entrenamos por 30 epochs quedandonos con todos los modelos y viendo cual es el mejor segun como varia la loss, en train y val, epoch por epoch.\n",
        "\n",
        "MEJOR MODELO: ÉPOCA 8"
      ],
      "metadata": {
        "id": "_mvv2Fz3ejX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=64, hidden_dim2=32,  num_layers=2, dropout=0.4) # 64 para que 64*2 sea 128\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Mover BERT a la misma device\n",
        "model_bert = model_bert.to(device)\n",
        "\n",
        "# Optimizador con weight decay (regularización L2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.2, patience=3\n",
        ")\n",
        "\n",
        "# Pesos diferentes para cada tarea (opcional, ajustar según importancia)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
        "\n",
        "# Gradient clipping\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "early_stop_patience = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ENTRENAMIENTO\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_loss_inicial = 0\n",
        "    total_loss_final = 0\n",
        "    total_loss_cap = 0\n",
        "\n",
        "    for embeddings, labels in train_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # le ponemos 2* a loss inicial para equiparar en magnitud con las demas\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        # Pérdida total\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping para evitar gradientes explosivos\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_loss_inicial += loss_inicial.item()\n",
        "        total_loss_final += loss_final.item()\n",
        "        total_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # VALIDACIÓN\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_loss_inicial = 0\n",
        "    val_loss_final = 0\n",
        "    val_loss_cap = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in val_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "\n",
        "            loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])  # le ponemos 2* a loss inicial para equiparar en magnitud con las demas\n",
        "\n",
        "            loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "\n",
        "            loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "            loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            val_loss_inicial += loss_inicial.item()\n",
        "            val_loss_final += loss_final.item()\n",
        "            val_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # Actualizar learning rate\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Imprimir métricas detalladas\n",
        "    print(f\"Época {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Train - Inicial: {total_loss_inicial/len(train_loader):.4f}, Final: {total_loss_final/len(train_loader):.4f}, Cap: {total_loss_cap/len(train_loader):.4f}\")\n",
        "    print(f\"  Val   - Inicial: {val_loss_inicial/len(val_loader):.4f}, Final: {val_loss_final/len(val_loader):.4f}, Cap: {val_loss_cap/len(val_loader):.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "    # Guardar cada epoca\n",
        "    ruta_epoca = f\"/content/drive/MyDrive/colab/modelosB/modelo_epoca_{epoch+1}.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': avg_val_loss,\n",
        "        }, ruta_epoca)\n",
        "\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        ruta_mejor_modelo = \"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, ruta_mejor_modelo)\n",
        "        print(f\"  ✓ Mejor modelo actualizado (Val Loss: {avg_val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  Sin mejora ({patience_counter}/{early_stop_patience})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= early_stop_patience:\n",
        "        print(f\"\\nEarly stopping en época {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "print(f\"\\nEntrenamiento completado!\")\n",
        "print(f\"Mejor modelo guardado en: {ruta_mejor_modelo}\")\n",
        "print(f\"Mejor Val Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "#Mostramos lo que printeo durante el entrenamiento\n",
        "\n",
        "\"\"\"\n",
        "Época 1/30\n",
        "  Train Loss: 1.3584 | Val Loss: 1.2807\n",
        "  Train - Inicial: 0.4258, Final: 0.4811, Cap: 0.4515\n",
        "  Val   - Inicial: 0.4152, Final: 0.4467, Cap: 0.4187\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2807)\n",
        "------------------------------------------------------------\n",
        "Época 2/30\n",
        "  Train Loss: 1.2907 | Val Loss: 1.2589\n",
        "  Train - Inicial: 0.4165, Final: 0.4519, Cap: 0.4222\n",
        "  Val   - Inicial: 0.4133, Final: 0.4370, Cap: 0.4085\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2589)\n",
        "------------------------------------------------------------\n",
        "Época 3/30\n",
        "  Train Loss: 1.2730 | Val Loss: 1.2504\n",
        "  Train - Inicial: 0.4148, Final: 0.4442, Cap: 0.4140\n",
        "  Val   - Inicial: 0.4126, Final: 0.4334, Cap: 0.4044\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2504)\n",
        "------------------------------------------------------------\n",
        "Época 4/30\n",
        "  Train Loss: 1.2615 | Val Loss: 1.2464\n",
        "  Train - Inicial: 0.4136, Final: 0.4396, Cap: 0.4084\n",
        "  Val   - Inicial: 0.4120, Final: 0.4315, Cap: 0.4029\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2464)\n",
        "------------------------------------------------------------\n",
        "Época 5/30\n",
        "  Train Loss: 1.2524 | Val Loss: 1.2430\n",
        "  Train - Inicial: 0.4128, Final: 0.4360, Cap: 0.4036\n",
        "  Val   - Inicial: 0.4117, Final: 0.4297, Cap: 0.4015\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2430)\n",
        "------------------------------------------------------------\n",
        "Época 6/30\n",
        "  Train Loss: 1.2451 | Val Loss: 1.2428\n",
        "  Train - Inicial: 0.4120, Final: 0.4334, Cap: 0.3997\n",
        "  Val   - Inicial: 0.4117, Final: 0.4293, Cap: 0.4019\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2428)\n",
        "------------------------------------------------------------\n",
        "Época 7/30\n",
        "  Train Loss: 1.2389 | Val Loss: 1.2419\n",
        "  Train - Inicial: 0.4113, Final: 0.4311, Cap: 0.3965\n",
        "  Val   - Inicial: 0.4115, Final: 0.4286, Cap: 0.4017\n",
        "  LR: 0.001000\n",
        "  ✓ Mejor modelo actualizado (Val Loss: 1.2419)\n",
        "------------------------------------------------------------\n",
        "Época 8/30\n",
        "  Train Loss: 1.2340 | Val Loss: 1.2425\n",
        "  Train - Inicial: 0.4107, Final: 0.4294, Cap: 0.3940\n",
        "  Val   - Inicial: 0.4114, Final: 0.4285, Cap: 0.4026\n",
        "  LR: 0.001000\n",
        "  Sin mejora (1/30)\n",
        "------------------------------------------------------------\n",
        "Época 9/30\n",
        "  Train Loss: 1.2293 | Val Loss: 1.2430\n",
        "  Train - Inicial: 0.4101, Final: 0.4276, Cap: 0.3917\n",
        "  Val   - Inicial: 0.4116, Final: 0.4287, Cap: 0.4026\n",
        "  LR: 0.001000\n",
        "  Sin mejora (2/30)\n",
        "------------------------------------------------------------\n",
        "Época 10/30\n",
        "  Train Loss: 1.2254 | Val Loss: 1.2444\n",
        "  Train - Inicial: 0.4096, Final: 0.4260, Cap: 0.3898\n",
        "  Val   - Inicial: 0.4118, Final: 0.4295, Cap: 0.4031\n",
        "  LR: 0.001000\n",
        "  Sin mejora (3/30)\n",
        "------------------------------------------------------------\n",
        "Época 11/30\n",
        "  Train Loss: 1.2221 | Val Loss: 1.2438\n",
        "  Train - Inicial: 0.4092, Final: 0.4247, Cap: 0.3883\n",
        "  Val   - Inicial: 0.4118, Final: 0.4289, Cap: 0.4031\n",
        "  LR: 0.000200\n",
        "  Sin mejora (4/30)\n",
        "------------------------------------------------------------\n",
        "Época 12/30\n",
        "  Train Loss: 1.2103 | Val Loss: 1.2445\n",
        "  Train - Inicial: 0.4075, Final: 0.4195, Cap: 0.3832\n",
        "  Val   - Inicial: 0.4119, Final: 0.4292, Cap: 0.4034\n",
        "  LR: 0.000200\n",
        "  Sin mejora (5/30)\n",
        "------------------------------------------------------------\n",
        "Época 13/30\n",
        "  Train Loss: 1.2064 | Val Loss: 1.2456\n",
        "  Train - Inicial: 0.4070, Final: 0.4178, Cap: 0.3816\n",
        "  Val   - Inicial: 0.4122, Final: 0.4294, Cap: 0.4040\n",
        "  LR: 0.000200\n",
        "  Sin mejora (6/30)\n",
        "------------------------------------------------------------\n",
        "Época 14/30\n",
        "  Train Loss: 1.2045 | Val Loss: 1.2464\n",
        "  Train - Inicial: 0.4067, Final: 0.4170, Cap: 0.3808\n",
        "  Val   - Inicial: 0.4123, Final: 0.4295, Cap: 0.4046\n",
        "  LR: 0.000200\n",
        "  Sin mejora (7/30)\n",
        "------------------------------------------------------------\n",
        "Época 15/30\n",
        "  Train Loss: 1.2027 | Val Loss: 1.2470\n",
        "  Train - Inicial: 0.4065, Final: 0.4161, Cap: 0.3801\n",
        "  Val   - Inicial: 0.4122, Final: 0.4299, Cap: 0.4048\n",
        "  LR: 0.000040\n",
        "  Sin mejora (8/30)\n",
        "------------------------------------------------------------\n",
        "Época 16/30\n",
        "  Train Loss: 1.1998 | Val Loss: 1.2487\n",
        "  Train - Inicial: 0.4061, Final: 0.4147, Cap: 0.3791\n",
        "  Val   - Inicial: 0.4128, Final: 0.4303, Cap: 0.4055\n",
        "  LR: 0.000040\n",
        "  Sin mejora (9/30)\n",
        "------------------------------------------------------------\n",
        "Época 17/30\n",
        "  Train Loss: 1.1990 | Val Loss: 1.2490\n",
        "  Train - Inicial: 0.4060, Final: 0.4143, Cap: 0.3788\n",
        "  Val   - Inicial: 0.4126, Final: 0.4304, Cap: 0.4060\n",
        "  LR: 0.000040\n",
        "  Sin mejora (10/30)\n",
        "------------------------------------------------------------\n",
        "Época 18/30\n",
        "  Train Loss: 1.1983 | Val Loss: 1.2492\n",
        "  Train - Inicial: 0.4058, Final: 0.4141, Cap: 0.3784\n",
        "  Val   - Inicial: 0.4127, Final: 0.4304, Cap: 0.4061\n",
        "  LR: 0.000040\n",
        "  Sin mejora (11/30)\n",
        "------------------------------------------------------------\n",
        "Época 19/30\n",
        "  Train Loss: 1.1979 | Val Loss: 1.2498\n",
        "  Train - Inicial: 0.4057, Final: 0.4139, Cap: 0.3783\n",
        "  Val   - Inicial: 0.4127, Final: 0.4308, Cap: 0.4063\n",
        "  LR: 0.000008\n",
        "  Sin mejora (12/30)\n",
        "------------------------------------------------------------\n",
        "Época 20/30\n",
        "  Train Loss: 1.1975 | Val Loss: 1.2496\n",
        "  Train - Inicial: 0.4056, Final: 0.4137, Cap: 0.3782\n",
        "  Val   - Inicial: 0.4127, Final: 0.4306, Cap: 0.4063\n",
        "  LR: 0.000008\n",
        "  Sin mejora (13/30)\n",
        "------------------------------------------------------------\n",
        "Época 21/30\n",
        "  Train Loss: 1.1973 | Val Loss: 1.2495\n",
        "  Train - Inicial: 0.4057, Final: 0.4136, Cap: 0.3781\n",
        "  Val   - Inicial: 0.4128, Final: 0.4306, Cap: 0.4062\n",
        "  LR: 0.000008\n",
        "  Sin mejora (14/30)\n",
        "------------------------------------------------------------\n",
        "Época 22/30\n",
        "  Train Loss: 1.1970 | Val Loss: 1.2499\n",
        "  Train - Inicial: 0.4057, Final: 0.4134, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4128, Final: 0.4307, Cap: 0.4064\n",
        "  LR: 0.000008\n",
        "  Sin mejora (15/30)\n",
        "------------------------------------------------------------\n",
        "Época 23/30\n",
        "  Train Loss: 1.1972 | Val Loss: 1.2499\n",
        "  Train - Inicial: 0.4057, Final: 0.4135, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4128, Final: 0.4307, Cap: 0.4065\n",
        "  LR: 0.000002\n",
        "  Sin mejora (16/30)\n",
        "------------------------------------------------------------\n",
        "Época 24/30\n",
        "  Train Loss: 1.1971 | Val Loss: 1.2499\n",
        "  Train - Inicial: 0.4057, Final: 0.4135, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4127, Final: 0.4307, Cap: 0.4064\n",
        "  LR: 0.000002\n",
        "  Sin mejora (17/30)\n",
        "------------------------------------------------------------\n",
        "Época 25/30\n",
        "  Train Loss: 1.1970 | Val Loss: 1.2500\n",
        "  Train - Inicial: 0.4056, Final: 0.4134, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4128, Final: 0.4308, Cap: 0.4064\n",
        "  LR: 0.000002\n",
        "  Sin mejora (18/30)\n",
        "------------------------------------------------------------\n",
        "Época 26/30\n",
        "  Train Loss: 1.1969 | Val Loss: 1.2500\n",
        "  Train - Inicial: 0.4056, Final: 0.4134, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4128, Final: 0.4308, Cap: 0.4065\n",
        "  LR: 0.000002\n",
        "  Sin mejora (19/30)\n",
        "------------------------------------------------------------\n",
        "Época 27/30\n",
        "  Train Loss: 1.1969 | Val Loss: 1.2500\n",
        "  Train - Inicial: 0.4057, Final: 0.4134, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4128, Final: 0.4308, Cap: 0.4064\n",
        "  LR: 0.000000\n",
        "  Sin mejora (20/30)\n",
        "------------------------------------------------------------\n",
        "Época 28/30\n",
        "  Train Loss: 1.1968 | Val Loss: 1.2500\n",
        "  Train - Inicial: 0.4056, Final: 0.4134, Cap: 0.3778\n",
        "  Val   - Inicial: 0.4128, Final: 0.4308, Cap: 0.4065\n",
        "  LR: 0.000000\n",
        "  Sin mejora (21/30)\n",
        "------------------------------------------------------------\n",
        "Época 29/30\n",
        "  Train Loss: 1.1969 | Val Loss: 1.2500\n",
        "  Train - Inicial: 0.4056, Final: 0.4134, Cap: 0.3779\n",
        "  Val   - Inicial: 0.4128, Final: 0.4308, Cap: 0.4065\n",
        "  LR: 0.000000\n",
        "  Sin mejora (22/30)\n",
        "------------------------------------------------------------\n",
        "Época 30/30\n",
        "  Train Loss: 1.1968 | Val Loss: 1.2500\n",
        "  Train - Inicial: 0.4056, Final: 0.4134, Cap: 0.3778\n",
        "  Val   - Inicial: 0.4128, Final: 0.4308, Cap: 0.4065\n",
        "  LR: 0.000000\n",
        "  Sin mejora (23/30)\n",
        "------------------------------------------------------------\n",
        "\n",
        "Entrenamiento completado!\n",
        "Mejor modelo guardado en: /content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\n",
        "Mejor Val Loss: 1.2419\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ao6KGtwqezvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EVALUACIÓN DE MODELOS"
      ],
      "metadata": {
        "id": "tQ4v7GKIqK_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUACIÓN UNIDIRECCIONAL"
      ],
      "metadata": {
        "id": "AOMWXohoqPyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# ---------------------------------------\n",
        "# EVALUACIÓN SOBRE EL TEST SET\n",
        "# ---------------------------------------\n",
        "\n",
        "test_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\") #modelosU para unidireccional\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n=== Evaluando en TEST SET con mejor modelo Unidireccional ===\\n\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "total_test_loss = 0\n",
        "loss_init = 0\n",
        "loss_final = 0\n",
        "loss_cap = 0\n",
        "\n",
        "# Para F1\n",
        "all_true_init  = []\n",
        "all_pred_init  = []\n",
        "\n",
        "all_true_final = []\n",
        "all_pred_final = []\n",
        "\n",
        "all_true_cap   = []\n",
        "all_pred_cap   = []\n",
        "\n",
        "correct_init = 0\n",
        "correct_final = 0\n",
        "correct_cap = 0\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        # Loss\n",
        "        l_i = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # 2* para equipararla con las otras\n",
        "        l_f = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        l_c = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = l_i + l_f + l_c\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "        loss_init += l_i.item()\n",
        "        loss_final += l_f.item()\n",
        "        loss_cap += l_c.item()\n",
        "\n",
        "        # Predicciones (batch, seq)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        # Guardar para F1\n",
        "        all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "        all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "        all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "        all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "        all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "        all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "        # Accuracy por tarea\n",
        "        # PREDICCIONES → (batch, seq, clases)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        correct_init += ((pred_init == labels[\"punt_inicial\"]) * mask).sum().item()\n",
        "        correct_final += ((pred_final == labels[\"punt_final\"]) * mask).sum().item()\n",
        "        correct_cap += ((pred_cap == labels[\"capitalizacion\"]) * mask).sum().item()\n",
        "\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#   CÁLCULO DE F1 MACRO\n",
        "# ----------------------------\n",
        "\n",
        "# Puntuación inicial: 2 clases → ¿, \"\"\n",
        "f1_init = f1_score(all_true_init, all_pred_init, average=\"macro\")\n",
        "\n",
        "# Puntuación final: 4 clases → ,  .  ?  \"\"\n",
        "f1_final = f1_score(all_true_final, all_pred_final, average=\"macro\")\n",
        "\n",
        "# Capitalización: 4 clases → 0 1 2 3\n",
        "f1_cap = f1_score(all_true_cap, all_pred_cap, average=\"macro\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#     PRINT RESULTADOS\n",
        "# ----------------------------\n",
        "print(\"\\n===== RESULTADOS TEST =====\")\n",
        "print(f\"Test Loss Total: {total_test_loss / len(test_loader):.4f}\")\n",
        "print(f\"  Inicial: {loss_init / len(test_loader):.4f}\")\n",
        "print(f\"  Final  : {loss_final / len(test_loader):.4f}\")\n",
        "print(f\"  Capital: {loss_cap / len(test_loader):.4f}\")\n",
        "\n",
        "print(\"\\n===== F1 MACRO =====\")\n",
        "print(f\"  Puntuación inicial (¿, \\\"\\\"): {f1_init:.4f}\")\n",
        "print(f\"  Puntuación final   (,, ., ?, \\\"\\\"): {f1_final:.4f}\")\n",
        "print(f\"  Capitalización     (0,1,2,3): {f1_cap:.4f}\")\n",
        "\n",
        "print(\"\\n===== ACCURACY =====\")\n",
        "print(f\" Inicial: {correct_init / total_tokens:.4f}\")\n",
        "print(f\" Final : {correct_final / total_tokens:.4f}\")\n",
        "print(f\" Capital: {correct_cap / total_tokens:.4f}\")\n",
        "\n",
        "# Mostramos lo que printeo\n",
        "\"\"\"\n",
        "=== Evaluando en TEST SET con mejor modelo Unidireccional ===\n",
        "\n",
        "\n",
        "===== RESULTADOS TEST =====\n",
        "Test Loss Total: 0.6204\n",
        "  Inicial: 0.1444\n",
        "  Final  : 0.3020\n",
        "  Capital: 0.1741\n",
        "\n",
        "===== F1 MACRO =====\n",
        "  Puntuación inicial (¿, \"\"): 0.7869\n",
        "  Puntuación final   (,, ., ?, \"\"): 0.3820\n",
        "  Capitalización     (0,1,2,3): 0.7420\n",
        "\n",
        "===== ACCURACY =====\n",
        " Inicial: 0.9920\n",
        " Final : 0.9171\n",
        " Capital: 0.9658\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NygS9JcBqNwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUACIÓN BIDIRECCIONAL\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0xtKHht1vht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# ---------------------------------------\n",
        "# EVALUACIÓN SOBRE EL TEST SET\n",
        "# ---------------------------------------\n",
        "\n",
        "test_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\") #modelosB para bidireccional\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n=== Evaluando en TEST SET con mejor modelo Bidireccional ===\\n\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "total_test_loss = 0\n",
        "loss_init = 0\n",
        "loss_final = 0\n",
        "loss_cap = 0\n",
        "\n",
        "# Para F1\n",
        "all_true_init  = []\n",
        "all_pred_init  = []\n",
        "\n",
        "all_true_final = []\n",
        "all_pred_final = []\n",
        "\n",
        "all_true_cap   = []\n",
        "all_pred_cap   = []\n",
        "\n",
        "correct_init = 0\n",
        "correct_final = 0\n",
        "correct_cap = 0\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        # Loss\n",
        "        l_i = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # 2* para equipararla con las otras\n",
        "        l_f = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        l_c = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = l_i + l_f + l_c\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "        loss_init += l_i.item()\n",
        "        loss_final += l_f.item()\n",
        "        loss_cap += l_c.item()\n",
        "\n",
        "        # Predicciones (batch, seq)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        # Guardar para F1\n",
        "        all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "        all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "        all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "        all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "        all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "        all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "        # Accuracy por tarea\n",
        "        # PREDICCIONES → (batch, seq, clases)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        correct_init += ((pred_init == labels[\"punt_inicial\"]) * mask).sum().item()\n",
        "        correct_final += ((pred_final == labels[\"punt_final\"]) * mask).sum().item()\n",
        "        correct_cap += ((pred_cap == labels[\"capitalizacion\"]) * mask).sum().item()\n",
        "\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#   CÁLCULO DE F1 MACRO\n",
        "# ----------------------------\n",
        "\n",
        "# Puntuación inicial: 2 clases → ¿, \"\"\n",
        "f1_init = f1_score(all_true_init, all_pred_init, average=\"macro\")\n",
        "\n",
        "# Puntuación final: 4 clases → ,  .  ?  \"\"\n",
        "f1_final = f1_score(all_true_final, all_pred_final, average=\"macro\")\n",
        "\n",
        "# Capitalización: 4 clases → 0 1 2 3\n",
        "f1_cap = f1_score(all_true_cap, all_pred_cap, average=\"macro\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#     PRINT RESULTADOS\n",
        "# ----------------------------\n",
        "print(\"\\n===== RESULTADOS TEST =====\")\n",
        "print(f\"Test Loss Total: {total_test_loss / len(test_loader):.4f}\")\n",
        "print(f\"  Inicial: {loss_init / len(test_loader):.4f}\")\n",
        "print(f\"  Final  : {loss_final / len(test_loader):.4f}\")\n",
        "print(f\"  Capital: {loss_cap / len(test_loader):.4f}\")\n",
        "\n",
        "print(\"\\n===== F1 MACRO =====\")\n",
        "print(f\"  Puntuación inicial (¿, \\\"\\\"): {f1_init:.4f}\")\n",
        "print(f\"  Puntuación final   (,, ., ?, \\\"\\\"): {f1_final:.4f}\")\n",
        "print(f\"  Capitalización     (0,1,2,3): {f1_cap:.4f}\")\n",
        "\n",
        "print(\"\\n===== ACCURACY =====\")\n",
        "print(f\" Inicial: {correct_init / total_tokens:.4f}\")\n",
        "print(f\" Final : {correct_final / total_tokens:.4f}\")\n",
        "print(f\" Capital: {correct_cap / total_tokens:.4f}\")\n",
        "\n",
        "# Mostramos lo que printeo\n",
        "\"\"\"\n",
        "=== Evaluando en TEST SET con mejor modelo Bidireccional ===\n",
        "\n",
        "\n",
        "===== RESULTADOS TEST =====\n",
        "Test Loss Total: 0.4457\n",
        "  Inicial: 0.1223\n",
        "  Final  : 0.1812\n",
        "  Capital: 0.1423\n",
        "\n",
        "===== F1 MACRO =====\n",
        "  Puntuación inicial (¿, \"\"): 0.9208\n",
        "  Puntuación final   (,, ., ?, \"\"): 0.7996\n",
        "  Capitalización     (0,1,2,3): 0.8125\n",
        "\n",
        "===== ACCURACY =====\n",
        " Inicial: 0.9963\n",
        " Final : 0.9574\n",
        " Capital: 0.9762\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "q4tmUjMF1hGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREDICCIONES CON EL MEJOR MODELO (BIDIRECCIONAL)"
      ],
      "metadata": {
        "id": "glQt__qmNvuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapas para convertir clases -> símbolos\n",
        "map_punt_inicial = {\n",
        "    0: \"\",\n",
        "    1: \"¿\"\n",
        "}\n",
        "\n",
        "map_punt_final = {\n",
        "    0: \"\",\n",
        "    1: \".\",\n",
        "    2: \",\",\n",
        "    3: \"?\"\n",
        "}\n",
        "\n",
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32, num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Cargar mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/datos_test.csv\")\n",
        "\n",
        "# Agrupar tokens por párrafo\n",
        "test_df = df_test.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\": list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalización\": list\n",
        "}).reset_index()\n",
        "\n",
        "pred_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert, modo=\"pred\")\n",
        "pred_loader = DataLoader(pred_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "pred_inicial = []\n",
        "pred_final = []\n",
        "pred_cap = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    global_index = 0  # para trackear la fila correcta en test_df\n",
        "\n",
        "    for embeddings, _ in pred_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        pred_init  = outputs[\"puntuación inicial\"].argmax(dim=-1).cpu()\n",
        "        pred_fin   = outputs[\"puntuación final\"].argmax(dim=-1).cpu()\n",
        "        pred_capit = outputs[\"capitalización\"].argmax(dim=-1).cpu()\n",
        "\n",
        "        batch_size = pred_init.shape[0]\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            real_len = len(test_df.loc[global_index, \"token\"])\n",
        "\n",
        "            pred_init_row  = [map_punt_inicial[int(x)] for x in pred_init[i][:real_len]]\n",
        "            pred_fin_row   = [map_punt_final[int(x)]   for x in pred_fin[i][:real_len]]\n",
        "            pred_cap_row   = pred_capit[i][:real_len].tolist()\n",
        "\n",
        "            pred_inicial.append(pred_init_row)\n",
        "            pred_final.append(pred_fin_row)\n",
        "            pred_cap.append(pred_cap_row)\n",
        "\n",
        "            global_index += 1\n",
        "\n",
        "\n",
        "# Expandir listas de listas\n",
        "df_test[\"punt_inicial\"]   = [p for par in pred_inicial for p in par]\n",
        "df_test[\"punt_final\"]     = [p for par in pred_final   for p in par]\n",
        "df_test[\"capitalización\"] = [p for par in pred_cap     for p in par]\n",
        "\n",
        "ruta_salida = \"/content/drive/MyDrive/colab/predicciones_test.csv\"\n",
        "df_test.to_csv(ruta_salida, index=False)\n",
        "\n",
        "print(f\"\\nPredicciones generadas correctamente y guardadas en:\\n{ruta_salida}\")\n"
      ],
      "metadata": {
        "id": "lc8ZiL4vxeSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OTROS"
      ],
      "metadata": {
        "id": "uJTN0UQZ1pyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_parrafo(texto, modelo, tokenizer, bert_model, device):\n",
        "    \"\"\"\n",
        "    Recibe un párrafo de texto y devuelve las predicciones de puntuación y capitalización.\n",
        "\n",
        "    Args:\n",
        "        texto: string con el texto a procesar\n",
        "        modelo: modelo entrenado\n",
        "        tokenizer: tokenizer de BERT\n",
        "        bert_model: modelo BERT para embeddings\n",
        "        device: 'cuda' o 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        dict con tokens y sus predicciones\n",
        "    \"\"\"\n",
        "    modelo.eval()\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "    # Obtener embeddings\n",
        "    token_embeddings = []\n",
        "    for token in tokens:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "            token_id = tokenizer.unk_token_id\n",
        "        emb = bert_model.embeddings.word_embeddings.weight[token_id].detach()\n",
        "        token_embeddings.append(emb)\n",
        "\n",
        "    # Convertir a tensor y añadir dimensión de batch\n",
        "    embeddings = torch.stack(token_embeddings).unsqueeze(0).to(device)  # (1, seq_len, 768)\n",
        "\n",
        "    # Hacer predicción\n",
        "    with torch.no_grad():\n",
        "        outputs = modelo(embeddings)\n",
        "\n",
        "    # Obtener las clases predichas\n",
        "    punt_inicial_pred = torch.argmax(outputs[\"puntuación inicial\"], dim=-1).squeeze().cpu().numpy()\n",
        "    punt_final_pred = torch.argmax(outputs[\"puntuación final\"], dim=-1).squeeze().cpu().numpy()\n",
        "    capital_pred = torch.argmax(outputs[\"capitalización\"], dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Mapeos de clases a etiquetas legibles\n",
        "    punt_inicial_map = {0: \"Sin puntuación\", 1: \"Con puntuación\"}\n",
        "    punt_final_map = {0: \"Ninguna\", 1: \"Punto\", 2: \"Coma\", 3: \"Otro\"}\n",
        "    capital_map = {0: \"Minúscula\", 1: \"Primera mayúscula\", 2: \"Todo mayúsculas\", 3: \"Otro\"}\n",
        "\n",
        "    # Crear resultado\n",
        "    resultados = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        resultados.append({\n",
        "            \"token\": token,\n",
        "            \"puntuación_inicial\": punt_inicial_map.get(int(punt_inicial_pred[i]), \"Desconocido\"),\n",
        "            \"puntuación_final\": punt_final_map.get(int(punt_final_pred[i]), \"Desconocido\"),\n",
        "            \"capitalización\": capital_map.get(int(capital_pred[i]), \"Desconocido\")\n",
        "        })\n",
        "\n",
        "    return resultados\n",
        "\n",
        "def reconstruir_texto(resultados):\n",
        "    \"\"\"\n",
        "    Reconstruye el texto con puntuación y capitalización a partir de las predicciones.\n",
        "    \"\"\"\n",
        "    texto_reconstruido = \"\"\n",
        "\n",
        "    for resultado in resultados:\n",
        "        token = resultado[\"token\"]\n",
        "\n",
        "        # Aplicar capitalización\n",
        "        if resultado[\"capitalización\"] == \"Primera mayúscula\":\n",
        "            token = token.capitalize()\n",
        "        elif resultado[\"capitalización\"] == \"Todo mayúsculas\":\n",
        "            token = token.upper()\n",
        "\n",
        "        # Añadir puntuación inicial\n",
        "        if resultado[\"puntuación_inicial\"] == \"Con puntuación\":\n",
        "            # Aquí podrías decidir qué puntuación añadir (ej. ¿, ¡, etc.)\n",
        "            pass\n",
        "\n",
        "        # Añadir el token\n",
        "        if token.startswith(\"##\"):\n",
        "            texto_reconstruido += token[2:]\n",
        "        else:\n",
        "            if texto_reconstruido:\n",
        "                texto_reconstruido += \" \" + token\n",
        "            else:\n",
        "                texto_reconstruido += token\n",
        "\n",
        "        # Añadir puntuación final\n",
        "        if resultado[\"puntuación_final\"] == \"Punto\":\n",
        "            texto_reconstruido += \".\"\n",
        "        elif resultado[\"puntuación_final\"] == \"Coma\":\n",
        "            texto_reconstruido += \",\"\n",
        "\n",
        "    return texto_reconstruido"
      ],
      "metadata": {
        "id": "jKz-D7Vl_C8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar el mejor modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Cargando el mejor modelo...\")\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/modelos/modelo_final_30e.pt\", map_location=device)\n",
        "modelo_cargado = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3)\n",
        "modelo_cargado.load_state_dict(checkpoint['model_state_dict'])\n",
        "modelo_cargado = modelo_cargado.to(device)\n",
        "print(f\"Modelo cargado (época {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n",
        "\n",
        "# Ejemplo de uso\n",
        "texto_ejemplo = \"había perdido la esperanza pero seguía creyendo porque sabía que era un tenista demasiado bueno para no ganar también aquí pero no me he alegrado cuando  ha perdido lo respeto demasiado estabas nervioso antes de la final\"\n",
        "print(f\"\\nTexto de entrada: '{texto_ejemplo}'\\n\")\n",
        "\n",
        "predicciones = predecir_parrafo(texto_ejemplo, modelo_cargado, tokenizer, model_bert, device)\n",
        "\n",
        "# Mostrar predicciones detalladas\n",
        "print(\"Predicciones por token:\")\n",
        "print(\"-\" * 80)\n",
        "for pred in predicciones:\n",
        "    print(f\"Token: {pred['token']:15s} | Punt. Inicial: {pred['puntuación_inicial']:20s} | \"\n",
        "          f\"Punt. Final: {pred['puntuación_final']:10s} | Cap: {pred['capitalización']}\")\n",
        "\n",
        "# Reconstruir texto\n",
        "texto_reconstruido = reconstruir_texto(predicciones)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Texto reconstruido: '{texto_reconstruido}'\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "vGf4pyY9_JId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruccion_texto(dataSet): #data set con columnas: instancia_id (cte), token_id, token, capitalización, punt_inicial, punt_final\n",
        "  frase = ''\n",
        "  frase_final = ''\n",
        "  n = dataSet.shape[0]\n",
        "  i = 0\n",
        "  capitalizaciones = []\n",
        "  punt_finales = []\n",
        "  punt_iniciales = []\n",
        "\n",
        "  for fila in dataSet.iterrows():\n",
        "    subpalabra = fila[1]['token']\n",
        "\n",
        "    if subpalabra[:2] == '##':\n",
        "      subpalabra = subpalabra[2:]\n",
        "    else:\n",
        "      capitalizaciones.append(fila[1]['capitalización'])\n",
        "      punt_iniciales.append(fila[1]['punt_inicial'])\n",
        "\n",
        "    if i == n-1:\n",
        "      frase += subpalabra\n",
        "      punt_finales.append(fila[1]['punt_final'])\n",
        "    else:\n",
        "      if dataSet.iloc[i+1]['token'][:2] == '##':\n",
        "        frase += subpalabra\n",
        "      else:\n",
        "        frase += subpalabra + ' '\n",
        "        punt_finales.append(fila[1]['punt_final'])\n",
        "    i +=1\n",
        "\n",
        "  palabras = frase.split()\n",
        "  for i, palabra in enumerate(palabras):\n",
        "\n",
        "    if punt_iniciales[i] == 1:\n",
        "      frase_final += '¿'\n",
        "\n",
        "    if capitalizaciones[i] == 0:\n",
        "      frase_final += palabra\n",
        "    elif capitalizaciones[i] == 1:\n",
        "      frase_final += palabra.capitalize()\n",
        "    elif capitalizaciones[i] == 3:\n",
        "      frase_final += palabra.upper()\n",
        "    else:\n",
        "      frase_final += palabra[:-1].capitalize() + palabra[-1].upper()\n",
        "\n",
        "    if punt_finales[i] == 1:\n",
        "      frase_final += '.'\n",
        "    elif punt_finales[i] == 2:\n",
        "      frase_final += ','\n",
        "    elif punt_finales[i] == 3:\n",
        "      frase_final += '?'\n",
        "\n",
        "\n",
        "\n",
        "    if i != len(palabras) - 1 and palabras[i+1] != \"'\" and palabras[i] != \"'\":\n",
        "        frase_final += ' '\n",
        "\n",
        "  return frase_final"
      ],
      "metadata": {
        "id": "m56_uOtIfLQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,100):\n",
        "  print(reconstruccion_texto(df_test[df_test[\"instancia_id\"]==i]))"
      ],
      "metadata": {
        "id": "SpbZtikvfMrQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}