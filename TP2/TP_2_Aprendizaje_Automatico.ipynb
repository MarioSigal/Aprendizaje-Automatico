{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP2/TP_2_Aprendizaje_Automatico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports\n"
      ],
      "metadata": {
        "id": "GJ8imGsu_vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjfXeaWE_og8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FRAME\n"
      ],
      "metadata": {
        "id": "L_xHy5FQ_5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rgdLKjU1JUOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/colab/dataset_datos_total3.csv\")"
      ],
      "metadata": {
        "id": "T8Hgq2B7uiQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar tokens por párrafo\n",
        "df_por_parrafos = df.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\":list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalización\": list\n",
        "}).reset_index()\n"
      ],
      "metadata": {
        "id": "I3yuvW8Esk5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER & BERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "uVHH_Ow__8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert.eval()"
      ],
      "metadata": {
        "id": "LYMyfzMG_3JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CREAR CLASE DATASET PARA PYTORCH"
      ],
      "metadata": {
        "id": "A1Zpp0aEjuXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DynamicEmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset de pytorch que calcula embeddings dinámicamente\"\"\"\n",
        "    def __init__(self, df, tokenizer, bert_model, modo=\"train\"):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.embedding_matrix = bert_model.embeddings.word_embeddings.weight\n",
        "        self.embedding_matrix.requires_grad = False\n",
        "        self.modo = modo  # \"train\" o \"pred\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        token_id_list = row[\"token_id\"]\n",
        "\n",
        "        # Calcular embeddings dinámicamente\n",
        "        token_embeddings = []\n",
        "        for token_id in token_id_list:\n",
        "            if token_id is None or token_id == self.tokenizer.unk_token_id:\n",
        "                token_id = self.tokenizer.unk_token_id\n",
        "            emb = self.embedding_matrix[token_id].detach()\n",
        "            token_embeddings.append(emb)\n",
        "\n",
        "        # Convertir a tensor\n",
        "        if token_embeddings:\n",
        "            embeddings = torch.stack(token_embeddings)\n",
        "        else:\n",
        "            embeddings = torch.empty(0, self.embedding_matrix.shape[1])\n",
        "\n",
        "        if self.modo == \"pred\":\n",
        "            return embeddings, None\n",
        "\n",
        "        # Preparar labels\n",
        "        labels = {\n",
        "            \"punt_inicial\": torch.tensor(row[\"punt_inicial\"], dtype=torch.long),\n",
        "            \"punt_final\": torch.tensor(row[\"punt_final\"], dtype=torch.long),\n",
        "            \"capitalización\": torch.tensor(row[\"capitalización\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        return embeddings, labels"
      ],
      "metadata": {
        "id": "OBX8JOtxkzcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PADDING"
      ],
      "metadata": {
        "id": "x65RnXFBwyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    if labels_list[0] is None:\n",
        "        return embeddings_padded, None\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    capitalizacion = pad_sequence([l[\"capitalización\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "a66xammXw1L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Unidireccional"
      ],
      "metadata": {
        "id": "uhpy5I7nAGf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Unidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim = 768, hidden_dim1 = 128, hidden_dim2 = 32, num_layers= 2, dropout= 0.4):\n",
        "        super(RNN_Unidireccional, self).__init__()\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim1,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # Unidireccional\n",
        "            )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Salidas\n",
        "        self.punt_inicial_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 2)\n",
        "            )\n",
        "        self.punt_final_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "        self.capital_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Pasamos los embeddings por la lstm\n",
        "        outputs, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Les hacemos dropout a los outputs\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # Pasamos los outputs de la lstm por las salidas\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "rKk-_53cAtTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Bidireccional\n"
      ],
      "metadata": {
        "id": "swyed0hyHn2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Bidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim = 768, hidden_dim1 = 64, hidden_dim2 = 32, num_layers= 2, dropout= 0.4): #reducimos hidden_dim1 para equiparar con la red unidireccional( 64*2 = 128)\n",
        "        super(RNN_Bidireccional, self).__init__()\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim1,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # Bidireccional\n",
        "            )\n",
        "\n",
        "        lstm_output_dim = hidden_dim1 * 2 #128 igual que en la Unidireccinal\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Salidas\n",
        "        self.punt_inicial_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 2)\n",
        "            )\n",
        "        self.punt_final_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "        self.capital_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Pasamos los embeddings por la lstm\n",
        "        outputs, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Les hacemos dropout a los outputs\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # Pasamos los outputs de la lstm por las salidas\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }"
      ],
      "metadata": {
        "id": "shJOmhfqmuuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ENTRENAMIENTO DE LOS MODELOS\n",
        "cambiando algunas letras del codigo (Uni por Bi) y algunos parametros más, variamos el entrenamiento entre la red Unidireccional y Bidireccional."
      ],
      "metadata": {
        "id": "gg_COwTOxHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "# Dividir en train/validation/test\n",
        "train_df, temp = train_test_split(df_por_parrafos, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "val_df, test_df = train_test_split(temp, test_size=0.5, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = DynamicEmbeddingDataset(train_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "val_dataset = DynamicEmbeddingDataset(val_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "3zALNZA7iF2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Mover BERT a la misma device\n",
        "model_bert = model_bert.to(device)\n",
        "\n",
        "# Optimizador con weight decay (regularización L2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.2, patience=3\n",
        ")\n",
        "\n",
        "# Pesos diferentes para cada tarea (opcional, ajustar según importancia)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
        "\n",
        "# Gradient clipping\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "early_stop_patience = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ENTRENAMIENTO\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_loss_inicial = 0\n",
        "    total_loss_final = 0\n",
        "    total_loss_cap = 0\n",
        "\n",
        "    for embeddings, labels in train_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) #testeando, le pongo 100* a los inicial porque es muy chica\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        # Pérdida total\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping para evitar gradientes explosivos\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_loss_inicial += loss_inicial.item()\n",
        "        total_loss_final += loss_final.item()\n",
        "        total_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # VALIDACIÓN\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_loss_inicial = 0\n",
        "    val_loss_final = 0\n",
        "    val_loss_cap = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in val_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "\n",
        "            loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])  #testeando, le pongo 100* a los inicial porque es muy chica\n",
        "\n",
        "            loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "\n",
        "            loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "            loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            val_loss_inicial += loss_inicial.item()\n",
        "            val_loss_final += loss_final.item()\n",
        "            val_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # Actualizar learning rate\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Imprimir métricas detalladas\n",
        "    print(f\"Época {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Train - Inicial: {total_loss_inicial/len(train_loader):.4f}, Final: {total_loss_final/len(train_loader):.4f}, Cap: {total_loss_cap/len(train_loader):.4f}\")\n",
        "    print(f\"  Val   - Inicial: {val_loss_inicial/len(val_loader):.4f}, Final: {val_loss_final/len(val_loader):.4f}, Cap: {val_loss_cap/len(val_loader):.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "    # Guardar cada epoca\n",
        "    ruta_epoca = f\"/content/drive/MyDrive/colab/modelosU/modelo_epoca_{epoch+1}.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': avg_val_loss,\n",
        "        }, ruta_epoca)\n",
        "\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        ruta_mejor_modelo = \"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, ruta_mejor_modelo)\n",
        "        print(f\"  ✓ Mejor modelo actualizado (Val Loss: {avg_val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  Sin mejora ({patience_counter}/{early_stop_patience})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= early_stop_patience:\n",
        "        print(f\"\\nEarly stopping en época {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "print(f\"\\nEntrenamiento completado!\")\n",
        "print(f\"Mejor modelo guardado en: {ruta_mejor_modelo}\")\n",
        "print(f\"Mejor Val Loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "T6miBWEAxWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUACION UNIDIRECCIONAL"
      ],
      "metadata": {
        "id": "AOMWXohoqPyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# ---------------------------------------\n",
        "# EVALUACIÓN SOBRE EL TEST SET\n",
        "# ---------------------------------------\n",
        "\n",
        "test_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\") #modelosU para unidireccional\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n=== Evaluando en TEST SET con mejor modelo Unidireccional ===\\n\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "total_test_loss = 0\n",
        "loss_init = 0\n",
        "loss_final = 0\n",
        "loss_cap = 0\n",
        "\n",
        "# Para F1\n",
        "all_true_init  = []\n",
        "all_pred_init  = []\n",
        "\n",
        "all_true_final = []\n",
        "all_pred_final = []\n",
        "\n",
        "all_true_cap   = []\n",
        "all_pred_cap   = []\n",
        "\n",
        "correct_init = 0\n",
        "correct_final = 0\n",
        "correct_cap = 0\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        # Loss\n",
        "        l_i = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # 2* para equipararla con las otras\n",
        "        l_f = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        l_c = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = l_i + l_f + l_c\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "        loss_init += l_i.item()\n",
        "        loss_final += l_f.item()\n",
        "        loss_cap += l_c.item()\n",
        "\n",
        "        # Predicciones (batch, seq)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        # Guardar para F1\n",
        "        all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "        all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "        all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "        all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "        all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "        all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "        # Accuracy por tarea\n",
        "        # PREDICCIONES → (batch, seq, clases)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        correct_init += ((pred_init == labels[\"punt_inicial\"]) * mask).sum().item()\n",
        "        correct_final += ((pred_final == labels[\"punt_final\"]) * mask).sum().item()\n",
        "        correct_cap += ((pred_cap == labels[\"capitalizacion\"]) * mask).sum().item()\n",
        "\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#   CÁLCULO DE F1 MACRO\n",
        "# ----------------------------\n",
        "\n",
        "# Puntuación inicial: 2 clases → ¿, \"\"\n",
        "f1_init = f1_score(all_true_init, all_pred_init, average=\"macro\")\n",
        "\n",
        "# Puntuación final: 4 clases → ,  .  ?  \"\"\n",
        "f1_final = f1_score(all_true_final, all_pred_final, average=\"macro\")\n",
        "\n",
        "# Capitalización: 4 clases → 0 1 2 3\n",
        "f1_cap = f1_score(all_true_cap, all_pred_cap, average=\"macro\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#     PRINT RESULTADOS\n",
        "# ----------------------------\n",
        "print(\"\\n===== RESULTADOS TEST =====\")\n",
        "print(f\"Test Loss Total: {total_test_loss / len(test_loader):.4f}\")\n",
        "print(f\"  Inicial: {loss_init / len(test_loader):.4f}\")\n",
        "print(f\"  Final  : {loss_final / len(test_loader):.4f}\")\n",
        "print(f\"  Capital: {loss_cap / len(test_loader):.4f}\")\n",
        "\n",
        "print(\"\\n===== F1 MACRO =====\")\n",
        "print(f\"  Puntuación inicial (¿, \\\"\\\"): {f1_init:.4f}\")\n",
        "print(f\"  Puntuación final   (,, ., ?, \\\"\\\"): {f1_final:.4f}\")\n",
        "print(f\"  Capitalización     (0,1,2,3): {f1_cap:.4f}\")\n",
        "\n",
        "print(\"\\n===== ACCURACY =====\")\n",
        "print(f\" Inicial: {correct_init / total_tokens:.4f}\")\n",
        "print(f\" Final : {correct_final / total_tokens:.4f}\")\n",
        "print(f\" Capital: {correct_cap / total_tokens:.4f}\")"
      ],
      "metadata": {
        "id": "NygS9JcBqNwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUACIÓN BIDIRECCIONAL\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0xtKHht1vht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# ---------------------------------------\n",
        "# EVALUACIÓN SOBRE EL TEST SET\n",
        "# ---------------------------------------\n",
        "\n",
        "test_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\") #modelosB para bidireccional\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n=== Evaluando en TEST SET con mejor modelo Bidireccional ===\\n\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "total_test_loss = 0\n",
        "loss_init = 0\n",
        "loss_final = 0\n",
        "loss_cap = 0\n",
        "\n",
        "# Para F1\n",
        "all_true_init  = []\n",
        "all_pred_init  = []\n",
        "\n",
        "all_true_final = []\n",
        "all_pred_final = []\n",
        "\n",
        "all_true_cap   = []\n",
        "all_pred_cap   = []\n",
        "\n",
        "correct_init = 0\n",
        "correct_final = 0\n",
        "correct_cap = 0\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        # Loss\n",
        "        l_i = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # 2* para equipararla con las otras\n",
        "        l_f = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        l_c = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = l_i + l_f + l_c\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "        loss_init += l_i.item()\n",
        "        loss_final += l_f.item()\n",
        "        loss_cap += l_c.item()\n",
        "\n",
        "        # Predicciones (batch, seq)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        # Guardar para F1\n",
        "        all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "        all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "        all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "        all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "        all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "        all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "        # Accuracy por tarea\n",
        "        # PREDICCIONES → (batch, seq, clases)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        correct_init += ((pred_init == labels[\"punt_inicial\"]) * mask).sum().item()\n",
        "        correct_final += ((pred_final == labels[\"punt_final\"]) * mask).sum().item()\n",
        "        correct_cap += ((pred_cap == labels[\"capitalizacion\"]) * mask).sum().item()\n",
        "\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#   CÁLCULO DE F1 MACRO\n",
        "# ----------------------------\n",
        "\n",
        "# Puntuación inicial: 2 clases → ¿, \"\"\n",
        "f1_init = f1_score(all_true_init, all_pred_init, average=\"macro\")\n",
        "\n",
        "# Puntuación final: 4 clases → ,  .  ?  \"\"\n",
        "f1_final = f1_score(all_true_final, all_pred_final, average=\"macro\")\n",
        "\n",
        "# Capitalización: 4 clases → 0 1 2 3\n",
        "f1_cap = f1_score(all_true_cap, all_pred_cap, average=\"macro\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#     PRINT RESULTADOS\n",
        "# ----------------------------\n",
        "print(\"\\n===== RESULTADOS TEST =====\")\n",
        "print(f\"Test Loss Total: {total_test_loss / len(test_loader):.4f}\")\n",
        "print(f\"  Inicial: {loss_init / len(test_loader):.4f}\")\n",
        "print(f\"  Final  : {loss_final / len(test_loader):.4f}\")\n",
        "print(f\"  Capital: {loss_cap / len(test_loader):.4f}\")\n",
        "\n",
        "print(\"\\n===== F1 MACRO =====\")\n",
        "print(f\"  Puntuación inicial (¿, \\\"\\\"): {f1_init:.4f}\")\n",
        "print(f\"  Puntuación final   (,, ., ?, \\\"\\\"): {f1_final:.4f}\")\n",
        "print(f\"  Capitalización     (0,1,2,3): {f1_cap:.4f}\")\n",
        "\n",
        "print(\"\\n===== ACCURACY =====\")\n",
        "print(f\" Inicial: {correct_init / total_tokens:.4f}\")\n",
        "print(f\" Final : {correct_final / total_tokens:.4f}\")\n",
        "print(f\" Capital: {correct_cap / total_tokens:.4f}\")\n"
      ],
      "metadata": {
        "id": "q4tmUjMF1hGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predicciones con mejor modelo bidireccional"
      ],
      "metadata": {
        "id": "glQt__qmNvuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Cargar mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/datos_test.csv\")\n",
        "\n",
        "# Agrupar tokens por párrafo\n",
        "test_df = df_test.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\": list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalización\": list\n",
        "}).reset_index()\n",
        "\n",
        "pred_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert, modo = \"pred\")\n",
        "pred_loader = DataLoader(pred_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "pred_inicial = []\n",
        "pred_final = []\n",
        "pred_cap = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (embeddings, _) in enumerate(pred_loader):\n",
        "        embeddings = embeddings.to(device)\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        pred_init  = outputs[\"puntuación inicial\"].argmax(dim=-1).cpu()\n",
        "        pred_fin   = outputs[\"puntuación final\"].argmax(dim=-1).cpu()\n",
        "        pred_capit = outputs[\"capitalización\"].argmax(dim=-1).cpu()\n",
        "\n",
        "        # Para cada párrafo dentro del batch\n",
        "        for i in range(pred_init.shape[0]):\n",
        "            # largo real del párrafo (sin padding)\n",
        "            real_len = len(test_df.loc[len(pred_inicial), \"token\"])\n",
        "\n",
        "            pred_inicial.append(pred_init[i][:real_len].tolist())\n",
        "            pred_final.append(pred_fin[i][:real_len].tolist())\n",
        "            pred_cap.append(pred_capit[i][:real_len].tolist())\n",
        "\n",
        "# Expandir listas de listas (párrafos) a 1 pred por token\n",
        "df_test[\"punt_inicial\"]     = [p for par in pred_inicial for p in par]\n",
        "df_test[\"punt_final\"]       = [p for par in pred_final   for p in par]\n",
        "df_test[\"capitalización\"]   = [p for par in pred_cap     for p in par]\n",
        "\n",
        "ruta_salida = \"/content/drive/MyDrive/colab/predicciones_test.csv\"\n",
        "df_test.to_csv(ruta_salida, index=False)\n",
        "\n",
        "print(f\"\\nPredicciones generadas correctamente y guardadas en:\\n{ruta_salida}\")\n"
      ],
      "metadata": {
        "id": "G5OfP2mJPf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OTROS"
      ],
      "metadata": {
        "id": "uJTN0UQZ1pyV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80d62e9d"
      },
      "source": [
        "def check_df_consistency(dataframe):\n",
        "    \"\"\"\n",
        "    Verifica la consistencia de las longitudes de 'token', 'punt_inicial',\n",
        "    'punt_final' y 'capitalización' en cada fila del DataFrame.\n",
        "    \"\"\"\n",
        "    mismatched_instances = []\n",
        "    for idx, row in dataframe.iterrows():\n",
        "        len_tokens = len(row[\"token\"])\n",
        "        len_punt_inicial = len(row[\"punt_inicial\"])\n",
        "        len_punt_final = len(row[\"punt_final\"])\n",
        "        len_capitalizacion = len(row[\"capitalización\"])\n",
        "\n",
        "        if not (len_tokens == len_punt_inicial == len_punt_final == len_capitalizacion):\n",
        "            mismatched_instances.append(row[\"instancia_id\"])\n",
        "    return mismatched_instances\n",
        "\n",
        "print(\"Realizando verificación de consistencia en df_por_parrafos...\")\n",
        "inconsistent_ids = check_df_consistency(df_por_parrafos)\n",
        "\n",
        "if inconsistent_ids:\n",
        "    print(f\"¡Advertencia! Se encontraron {len(inconsistent_ids)} instancias con longitudes inconsistentes.\")\n",
        "    print(f\"Primeras 10 instancia_id con problemas: {inconsistent_ids[:10]}\")\n",
        "    print(\"Es recomendable filtrar estas instancias antes de proceder con el entrenamiento.\")\n",
        "else:\n",
        "    print(\"¡Excelente! Todas las instancias en df_por_parrafos tienen longitudes consistentes.\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##INFERENCIA - CARGAR MODELO Y HACER PREDICCIONES\"\"\"\n",
        "\n",
        "def predecir_parrafo(texto, modelo, tokenizer, bert_model, device):\n",
        "    \"\"\"\n",
        "    Recibe un párrafo de texto y devuelve las predicciones de puntuación y capitalización.\n",
        "\n",
        "    Args:\n",
        "        texto: string con el texto a procesar\n",
        "        modelo: modelo entrenado\n",
        "        tokenizer: tokenizer de BERT\n",
        "        bert_model: modelo BERT para embeddings\n",
        "        device: 'cuda' o 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        dict con tokens y sus predicciones\n",
        "    \"\"\"\n",
        "    modelo.eval()\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "    # Obtener embeddings\n",
        "    token_embeddings = []\n",
        "    for token in tokens:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "            token_id = tokenizer.unk_token_id\n",
        "        emb = bert_model.embeddings.word_embeddings.weight[token_id].detach()\n",
        "        token_embeddings.append(emb)\n",
        "\n",
        "    # Convertir a tensor y añadir dimensión de batch\n",
        "    embeddings = torch.stack(token_embeddings).unsqueeze(0).to(device)  # (1, seq_len, 768)\n",
        "\n",
        "    # Hacer predicción\n",
        "    with torch.no_grad():\n",
        "        outputs = modelo(embeddings)\n",
        "\n",
        "    # Obtener las clases predichas\n",
        "    punt_inicial_pred = torch.argmax(outputs[\"puntuación inicial\"], dim=-1).squeeze().cpu().numpy()\n",
        "    punt_final_pred = torch.argmax(outputs[\"puntuación final\"], dim=-1).squeeze().cpu().numpy()\n",
        "    capital_pred = torch.argmax(outputs[\"capitalización\"], dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Mapeos de clases a etiquetas legibles\n",
        "    punt_inicial_map = {0: \"Sin puntuación\", 1: \"Con puntuación\"}\n",
        "    punt_final_map = {0: \"Ninguna\", 1: \"Punto\", 2: \"Coma\", 3: \"Otro\"}\n",
        "    capital_map = {0: \"Minúscula\", 1: \"Primera mayúscula\", 2: \"Todo mayúsculas\", 3: \"Otro\"}\n",
        "\n",
        "    # Crear resultado\n",
        "    resultados = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        resultados.append({\n",
        "            \"token\": token,\n",
        "            \"puntuación_inicial\": punt_inicial_map.get(int(punt_inicial_pred[i]), \"Desconocido\"),\n",
        "            \"puntuación_final\": punt_final_map.get(int(punt_final_pred[i]), \"Desconocido\"),\n",
        "            \"capitalización\": capital_map.get(int(capital_pred[i]), \"Desconocido\")\n",
        "        })\n",
        "\n",
        "    return resultados\n",
        "\n",
        "def reconstruir_texto(resultados):\n",
        "    \"\"\"\n",
        "    Reconstruye el texto con puntuación y capitalización a partir de las predicciones.\n",
        "    \"\"\"\n",
        "    texto_reconstruido = \"\"\n",
        "\n",
        "    for resultado in resultados:\n",
        "        token = resultado[\"token\"]\n",
        "\n",
        "        # Aplicar capitalización\n",
        "        if resultado[\"capitalización\"] == \"Primera mayúscula\":\n",
        "            token = token.capitalize()\n",
        "        elif resultado[\"capitalización\"] == \"Todo mayúsculas\":\n",
        "            token = token.upper()\n",
        "\n",
        "        # Añadir puntuación inicial\n",
        "        if resultado[\"puntuación_inicial\"] == \"Con puntuación\":\n",
        "            # Aquí podrías decidir qué puntuación añadir (ej. ¿, ¡, etc.)\n",
        "            pass\n",
        "\n",
        "        # Añadir el token\n",
        "        if token.startswith(\"##\"):\n",
        "            texto_reconstruido += token[2:]\n",
        "        else:\n",
        "            if texto_reconstruido:\n",
        "                texto_reconstruido += \" \" + token\n",
        "            else:\n",
        "                texto_reconstruido += token\n",
        "\n",
        "        # Añadir puntuación final\n",
        "        if resultado[\"puntuación_final\"] == \"Punto\":\n",
        "            texto_reconstruido += \".\"\n",
        "        elif resultado[\"puntuación_final\"] == \"Coma\":\n",
        "            texto_reconstruido += \",\"\n",
        "\n",
        "    return texto_reconstruido"
      ],
      "metadata": {
        "id": "jKz-D7Vl_C8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar el mejor modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Cargando el mejor modelo...\")\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/modelos/modelo_final_30e.pt\", map_location=device)\n",
        "modelo_cargado = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3)\n",
        "modelo_cargado.load_state_dict(checkpoint['model_state_dict'])\n",
        "modelo_cargado = modelo_cargado.to(device)\n",
        "print(f\"Modelo cargado (época {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n",
        "\n",
        "# Ejemplo de uso\n",
        "texto_ejemplo = \"había perdido la esperanza pero seguía creyendo porque sabía que era un tenista demasiado bueno para no ganar también aquí pero no me he alegrado cuando  ha perdido lo respeto demasiado estabas nervioso antes de la final\"\n",
        "print(f\"\\nTexto de entrada: '{texto_ejemplo}'\\n\")\n",
        "\n",
        "predicciones = predecir_parrafo(texto_ejemplo, modelo_cargado, tokenizer, model_bert, device)\n",
        "\n",
        "# Mostrar predicciones detalladas\n",
        "print(\"Predicciones por token:\")\n",
        "print(\"-\" * 80)\n",
        "for pred in predicciones:\n",
        "    print(f\"Token: {pred['token']:15s} | Punt. Inicial: {pred['puntuación_inicial']:20s} | \"\n",
        "          f\"Punt. Final: {pred['puntuación_final']:10s} | Cap: {pred['capitalización']}\")\n",
        "\n",
        "# Reconstruir texto\n",
        "texto_reconstruido = reconstruir_texto(predicciones)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Texto reconstruido: '{texto_reconstruido}'\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "vGf4pyY9_JId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos)"
      ],
      "metadata": {
        "id": "6nfDDceeA72h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruccion_texto(dataSet): #data set con columnas: instancia_id (cte), token_id, token, capitalización, punt_inicial, punt_final\n",
        "  frase = ''\n",
        "  frase_final = ''\n",
        "  n = dataSet.shape[0]\n",
        "  i = 0\n",
        "  capitalizaciones = []\n",
        "  punt_finales = []\n",
        "  punt_iniciales = []\n",
        "\n",
        "  for fila in dataSet.iterrows():\n",
        "    subpalabra = fila[1]['token']\n",
        "\n",
        "    if subpalabra[:2] == '##':\n",
        "      subpalabra = subpalabra[2:]\n",
        "    else:\n",
        "      capitalizaciones.append(fila[1]['capitalización'])\n",
        "      punt_iniciales.append(fila[1]['punt_inicial'])\n",
        "\n",
        "    if i == n-1:\n",
        "      frase += subpalabra\n",
        "      punt_finales.append(fila[1]['punt_final'])\n",
        "    else:\n",
        "      if dataSet.iloc[i+1]['token'][:2] == '##':\n",
        "        frase += subpalabra\n",
        "      else:\n",
        "        frase += subpalabra + ' '\n",
        "        punt_finales.append(fila[1]['punt_final'])\n",
        "    i +=1\n",
        "\n",
        "  palabras = frase.split()\n",
        "  for i, palabra in enumerate(palabras):\n",
        "\n",
        "    if punt_iniciales[i] == 1:\n",
        "      frase_final += '¿'\n",
        "\n",
        "    if capitalizaciones[i] == 0:\n",
        "      frase_final += palabra\n",
        "    elif capitalizaciones[i] == 1:\n",
        "      frase_final += palabra.capitalize()\n",
        "    elif capitalizaciones[i] == 3:\n",
        "      frase_final += palabra.upper()\n",
        "    else:\n",
        "      frase_final += palabra[:-1].capitalize() + palabra[-1].upper()\n",
        "\n",
        "    if punt_finales[i] == 1:\n",
        "      frase_final += '.'\n",
        "    elif punt_finales[i] == 2:\n",
        "      frase_final += ','\n",
        "    elif punt_finales[i] == 3:\n",
        "      frase_final += '?'\n",
        "\n",
        "\n",
        "\n",
        "    if i != len(palabras) - 1 and palabras[i+1] != \"'\" and palabras[i] != \"'\":\n",
        "        frase_final += ' '\n",
        "\n",
        "  return frase_final"
      ],
      "metadata": {
        "id": "m56_uOtIfLQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,100):\n",
        "  print(reconstruccion_texto(df_test[df_test[\"instancia_id\"]==i]))"
      ],
      "metadata": {
        "id": "SpbZtikvfMrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-heUKzzifl5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}