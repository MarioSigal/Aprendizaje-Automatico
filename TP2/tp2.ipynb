{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVnY3L1zccZb28jITn5+fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/TPs/tp2/tp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wxeGjIn6uDF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "nwupEbruutCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones Auxiliares"
      ],
      "metadata": {
        "id": "IKP6njfDK4e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rp3SnSPox-4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "gYwlSGEGyMqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-Záéíóúüñ0-9¿?,.' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "sWI74TYAyWNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_etiqueta(y, indice):\n",
        "  puntuacion_iniciales = []\n",
        "  puntuacion_finales = []\n",
        "  capitalizaciones = []\n",
        "  instancia_ids = []\n",
        "  token_ids = []\n",
        "  tokens_l = []\n",
        "\n",
        "  for parrafo in y:\n",
        "    inicio_pregunta = False\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "\n",
        "    for palabra in palabras:\n",
        "      tokens = tokenizer.tokenize(palabra.lower())\n",
        "\n",
        "      for i in range(len(tokens)):\n",
        "        if tokens[i] == \"¿\":\n",
        "          inicio_pregunta = True\n",
        "          continue\n",
        "        if tokens[i] == \"?\" or tokens[i] == \".\" or tokens[i] == \",\":\n",
        "          continue\n",
        "\n",
        "        instancia_ids.append(indice)\n",
        "        token_ids.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "        tokens_l.append(tokens[i])\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuacion_iniciales.append(1) #('\"¿\"')\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuacion_iniciales.append(0) #(\"\")\n",
        "\n",
        "        if i != len(tokens) - 1:\n",
        "\n",
        "          if tokens[i+1] == \"?\":\n",
        "            puntuacion_finales.append(3) #('\"?\"')\n",
        "          elif tokens[i+1] == \".\":\n",
        "            puntuacion_finales.append(1) #('\".\"')\n",
        "          elif tokens[i+1] == \",\":\n",
        "            puntuacion_finales.append(2) #('\",\"')\n",
        "          else:\n",
        "            puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        else:\n",
        "          puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        if palabra.islower():\n",
        "          capitalizaciones.append(0)\n",
        "          ultimo_numero = 0\n",
        "        elif palabra.istitle():\n",
        "          capitalizaciones.append(1)\n",
        "          ultimo_numero = 1\n",
        "        elif palabra.isupper():\n",
        "          capitalizaciones.append(3)\n",
        "          ultimo_numero = 3\n",
        "        else:\n",
        "          capitalizaciones.append(2)\n",
        "          ultimo_numero = 2\n",
        "\n",
        "  etiquetas = np.column_stack([instancia_ids, token_ids, tokens_l, puntuacion_iniciales, puntuacion_finales, capitalizaciones])\n",
        "  return etiquetas"
      ],
      "metadata": {
        "id": "dWVD-ZHu2LUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_csv(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8ukgJIVyuEjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSet(libro):\n",
        "  convertir_epub_a_csv(libro)\n",
        "  df = pd.read_csv('libro_parrafos.csv')\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  columnas = ['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "  datos = pd.DataFrame(columns=columnas)\n",
        "\n",
        "  i = 0\n",
        "  for p in parrafos['default']:\n",
        "    etiquetas = transformar_etiqueta([p], i)\n",
        "    etiquetas = pd.DataFrame(etiquetas, columns=columnas)\n",
        "    datos = pd.concat([datos, etiquetas], ignore_index=True)\n",
        "    i += 1\n",
        "\n",
        "  print('Data set creado de tamaño: ', datos.shape)\n",
        "\n",
        "  numeric_cols = ['instancia_id', 'token_id', 'punt_inicial', 'punt_final', 'capitalización']\n",
        "  for col in numeric_cols:\n",
        "    datos[col] = pd.to_numeric(datos[col], errors='coerce')\n",
        "\n",
        "  return parrafos, datos"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0zgQ4kTNeFIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agregar_embeddings(df_X):\n",
        "  token_ids = df_X['token_id'].tolist()\n",
        "  embeddings_list = []\n",
        "  for token_id  in token_ids:\n",
        "      if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "        token_id = tokenizer.unk_token_id\n",
        "      # Detach the tensor before converting to list\n",
        "      embedding = model.embeddings.word_embeddings.weight[token_id].detach().tolist()\n",
        "      embeddings_list.append(embedding)\n",
        "  df_X['embeddings'] = embeddings_list\n",
        "  # The embeddings are already lists of floats, no need to convert to int\n",
        "  # df_X['embeddings'] = df_X['embeddings'].apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "  return df_X"
      ],
      "metadata": {
        "id": "5BRwbGA7sCsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trasformar_df_dfPyTorch(datos_X, datos_Y):\n",
        "  # Convert the list of lists in the 'embeddings' column to a NumPy array of floats\n",
        "  embeddings_array = np.array(datos_X['embeddings'].tolist(), dtype=np.float32)\n",
        "  X = torch.tensor(embeddings_array, dtype=torch.float32)\n",
        "  Y = torch.tensor(datos_Y[['punt_inicial', 'punt_final', 'capitalización']].values, dtype=torch.float32)\n",
        "  dataSetPT = TensorDataset(X, Y)\n",
        "\n",
        "  return dataSetPT"
      ],
      "metadata": {
        "id": "pKAgz6wenZVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar libros"
      ],
      "metadata": {
        "id": "ISGg2S2RK0Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busca libro en formato epub de mi github"
      ],
      "metadata": {
        "id": "P2WDL7S3PtV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'"
      ],
      "metadata": {
        "id": "NxMriymePxNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'"
      ],
      "metadata": {
        "id": "HbrjtNdCP4EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = path + libro1"
      ],
      "metadata": {
        "id": "6LlG5OLZQO8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AwL64C6xIo6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Codigo"
      ],
      "metadata": {
        "id": "4rJ0jhmDQAgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos, dataSet = crearDataSet('libro1.epub')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2AWjTOSfQc9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X = dataSet[['instancia_id', 'token_id', 'token']]\n",
        "datos_Y = dataSet[['punt_inicial', 'punt_final', 'capitalización']]"
      ],
      "metadata": {
        "id": "QKYNGgoxtpw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X_ext = agregar_embeddings(datos_X)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fzP6Q4CWwEEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Puntuacion inicial:\n",
        "*   \"\": 0\n",
        "*   \"¿\": 1\n",
        "\n",
        "### Puntuacion final:\n",
        "*   \"\": 0\n",
        "*   \".\": 1\n",
        "*   \",\": 2\n",
        "*   \"?\": 3\n",
        "\n"
      ],
      "metadata": {
        "id": "YPEqeNwVoiTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataSetPT = trasformar_df_dfPyTorch(datos_X, datos_Y)"
      ],
      "metadata": {
        "id": "7X_CWg4gqPkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X0, Y0 = dataSetPT[0]\n",
        "print(\"X[0]:\", X0)\n",
        "print(\"Y[0]:\", Y0)"
      ],
      "metadata": {
        "id": "PRdL2VUnqRQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}