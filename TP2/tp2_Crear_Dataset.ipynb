{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP2/tp2_Crear_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports\n"
      ],
      "metadata": {
        "id": "mFNfrypGbChM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wxeGjIn6uDF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n"
      ],
      "metadata": {
        "id": "oKvkcCX0Bo_A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pdfplumber\n",
        "import os"
      ],
      "metadata": {
        "id": "nwupEbruutCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "L2--jEFj8Vsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones Auxiliares"
      ],
      "metadata": {
        "id": "IKP6njfDK4e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rp3SnSPox-4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a min√∫sculas y quitar puntuaci√≥n\n",
        "    t = str(t) if t is not None else \"\"\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-z√°√©√≠√≥√∫√º√±0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "gYwlSGEGyMqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    # Convertir a min√∫sculas y quitar puntuaci√≥n\n",
        "    t = str(t) if t is not None else \"\"\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-Z√°√©√≠√≥√∫√º√±0-9¬ø?,.' -√Å√â√ç√ì√ö√ú√ë]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "sWI74TYAyWNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_etiqueta(y, indice):\n",
        "  puntuacion_iniciales = []\n",
        "  puntuacion_finales = []\n",
        "  capitalizaciones = []\n",
        "  instancia_ids = []\n",
        "  token_ids = []\n",
        "  tokens_l = []\n",
        "\n",
        "  for parrafo in y:\n",
        "    inicio_pregunta = False\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "\n",
        "    for palabra in palabras:\n",
        "      tokens = tokenizer.tokenize(palabra.lower())\n",
        "\n",
        "      for i in range(len(tokens)):\n",
        "        if tokens[i] == \"¬ø\":\n",
        "          inicio_pregunta = True\n",
        "          continue\n",
        "        if tokens[i] == \"?\" or tokens[i] == \".\" or tokens[i] == \",\":\n",
        "          continue\n",
        "\n",
        "        instancia_ids.append(indice)\n",
        "        token_ids.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "        tokens_l.append(tokens[i])\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuacion_iniciales.append(1) #('\"¬ø\"')\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuacion_iniciales.append(0) #(\"\")\n",
        "\n",
        "        if i != len(tokens) - 1:\n",
        "\n",
        "          if tokens[i+1] == \"?\":\n",
        "            puntuacion_finales.append(3) #('\"?\"')\n",
        "          elif tokens[i+1] == \".\":\n",
        "            puntuacion_finales.append(1) #('\".\"')\n",
        "          elif tokens[i+1] == \",\":\n",
        "            puntuacion_finales.append(2) #('\",\"')\n",
        "          else:\n",
        "            puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        else:\n",
        "          puntuacion_finales.append(0) #(\"\")\n",
        "\n",
        "        if palabra.islower():\n",
        "          capitalizaciones.append(0)\n",
        "\n",
        "        elif palabra.istitle():\n",
        "          capitalizaciones.append(1)\n",
        "\n",
        "        elif palabra.isupper():\n",
        "          capitalizaciones.append(3)\n",
        "\n",
        "        else:\n",
        "          capitalizaciones.append(2)\n",
        "\n",
        "\n",
        "  etiquetas = np.column_stack([instancia_ids, token_ids, tokens_l, puntuacion_iniciales, puntuacion_finales, capitalizaciones])\n",
        "  return etiquetas"
      ],
      "metadata": {
        "id": "dWVD-ZHu2LUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_csv(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardar√°n los p√°rrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los √≠tems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los p√°rrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos p√°rrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} p√°rrafos y se guardaron en 'libro_parrafos.csv'.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8ukgJIVyuEjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_pdf_a_csv(pdf_path):\n",
        "    \"\"\"\n",
        "    Convierte un archivo PDF en un CSV con una columna 'parrafo'.\n",
        "    Divide el texto en los tipos de oraci√≥nes que queremos apra entrenar\n",
        "    \"\"\"\n",
        "\n",
        "    # Extraer texto del PDF\n",
        "    texto_total = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for pagina in pdf.pages:\n",
        "            texto = pagina.extract_text()\n",
        "            if texto:\n",
        "                texto_total += \" \" + texto\n",
        "\n",
        "    # Limpieza b√°sica\n",
        "    texto_total = re.sub(r\"-\\n\", \"\", texto_total)   # une palabras cortadas\n",
        "    texto_total = re.sub(r\"\\s+\", \" \", texto_total)  # espacios normales\n",
        "    texto_total = texto_total.strip()\n",
        "\n",
        "    # Dividir el texto en oraciones por \".\", \"?\", \"!\"\n",
        "    # Pero conservamos los signos al final\n",
        "    oraciones = re.split(r'(?<=[\\.\\?\\!])\\s+', texto_total)\n",
        "    oraciones = [o.strip() for o in oraciones if len(o.strip()) > 2]\n",
        "\n",
        "    #Agrupar cada 1‚Äì3 oraciones en un bloque\n",
        "    bloques = []\n",
        "    buffer = []\n",
        "    for o in oraciones:\n",
        "        buffer.append(o)\n",
        "        # si ya hay 3 oraciones, o la actual termina en \"?\" o \"!\"\n",
        "        if len(buffer) >= 3 or re.search(r'[?!]$', o):\n",
        "            bloques.append(\" \".join(buffer))\n",
        "            buffer = []\n",
        "\n",
        "    # lo que quede\n",
        "    if buffer:\n",
        "        bloques.append(\" \".join(buffer))\n",
        "\n",
        "    # Filtrar solo las que contienen puntuaci√≥n real\n",
        "    bloques = [b.strip() for b in bloques if re.search(r'[.,¬ø?!]|[a-zA-Z√°√©√≠√≥√∫√Å√â√ç√ì√ö√±]$', b)]\n",
        "\n",
        "    df = pd.DataFrame(bloques, columns=['parrafo'])\n",
        "    df.to_csv('libro_parrafos.csv', index=False, encoding='utf-8-sig')\n",
        "    return df\n",
        "\n",
        "#_______________________________________________________________________________________________\n",
        "# VERSI√ìN 2 DE PDF-> CSV\n",
        "\n",
        "def convertir_pdf_a_csv2(pdf_path):\n",
        "    \"\"\"\n",
        "    Convierte un PDF en CSV pero corta los textos de forma aleatoria:\n",
        "    - A mitad de una oraci√≥n\n",
        "    - Cada 1 oraci√≥n\n",
        "    - Cada 2 oraciones\n",
        "    - Siempre corta al final de una pregunta\n",
        "    Adem√°s filtra p√°rrafos con muchos n√∫meros.\n",
        "    \"\"\"\n",
        "\n",
        "    # -------- Extraer texto --------\n",
        "    texto_total = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for pagina in pdf.pages:\n",
        "            txt = pagina.extract_text()\n",
        "            if txt:\n",
        "                texto_total += \" \" + txt\n",
        "\n",
        "    texto_total = re.sub(r\"-\\n\", \"\", texto_total)\n",
        "    texto_total = re.sub(r\"\\s+\", \" \", texto_total).strip()\n",
        "\n",
        "    # -------- Dividir en oraciones --------\n",
        "    oraciones = re.split(r'(?<=[\\.\\?\\!])\\s+', texto_total)\n",
        "    oraciones = [o.strip() for o in oraciones if len(o.strip()) > 2]\n",
        "\n",
        "    bloques = []\n",
        "    i = 0\n",
        "    while i < len(oraciones):\n",
        "\n",
        "        # si aparece una pregunta ‚Üí cortar ah√≠\n",
        "        if \"¬ø\" in oraciones[i] or \"?\" in oraciones[i]:\n",
        "            bloques.append(oraciones[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # decisi√≥n aleatoria\n",
        "        modo = np.random.choice([\"mitad\", \"1\", \"2\"], p=[0.2, 0.4, 0.4])\n",
        "\n",
        "        if modo == \"mitad\":\n",
        "            # cortar a la mitad de la oraci√≥n\n",
        "            oracion = oraciones[i]\n",
        "            palabras = oracion.split()\n",
        "            mitad = max(3, len(palabras) // 2)\n",
        "            bloque = \" \".join(palabras[:mitad])  # mitad truncada\n",
        "            bloques.append(bloque)\n",
        "            i += 1\n",
        "\n",
        "        elif modo == \"1\":\n",
        "            bloques.append(oraciones[i])\n",
        "            i += 1\n",
        "\n",
        "        else:  # modo = \"2\"\n",
        "            if i + 1 < len(oraciones):\n",
        "                bloques.append(oraciones[i] + \" \" + oraciones[i+1])\n",
        "                i += 2\n",
        "            else:\n",
        "                bloques.append(oraciones[i])\n",
        "                i += 1\n",
        "\n",
        "    # -------- Filtrar p√°rrafos con demasiados n√∫meros --------\n",
        "    def demasiado_numeros(texto):\n",
        "        nums = sum(c.isdigit() for c in texto)\n",
        "        return nums / max(len(texto), 1) > 0.15 or bool(re.search(r\"\\b\\d+(\\.\\d+)*\\b\", texto))\n",
        "\n",
        "    bloques = [b for b in bloques if not demasiado_numeros(b)]\n",
        "\n",
        "    # -------- Balancear preguntas --------\n",
        "    preg = [b for b in bloques if \"¬ø\" in b or \"?\" in b]\n",
        "    nopreg = [b for b in bloques if \"¬ø\" not in b and \"?\" not in b]\n",
        "\n",
        "    # eliminar 75% de nopreg\n",
        "    cant = int(len(nopreg) * 0.25)\n",
        "    nopreg_reducidas = list(np.random.choice(nopreg, cant, replace=False)) if cant > 0 else []\n",
        "\n",
        "    # combinar todo\n",
        "    bloques_final = preg + nopreg_reducidas\n",
        "\n",
        "    # mezclar aleatoriamente\n",
        "    np.random.shuffle(bloques_final)\n",
        "\n",
        "    df = pd.DataFrame(bloques_final, columns=['parrafo'])\n",
        "    df.to_csv('libro_parrafos.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "0OXEsoKR81zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSet(libro,i,pdf=False):\n",
        "  if pdf:\n",
        "    convertir_pdf_a_csv2(libro)\n",
        "  else:\n",
        "    convertir_epub_a_csv(libro)\n",
        "\n",
        "  df = pd.read_csv('libro_parrafos.csv')\n",
        "\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  columnas = ['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalizaci√≥n']\n",
        "  datos = pd.DataFrame(columns=columnas)\n",
        "\n",
        "  for p in parrafos['default']:\n",
        "    etiquetas = transformar_etiqueta([p], i)\n",
        "    etiquetas = pd.DataFrame(etiquetas, columns=columnas)\n",
        "    datos = pd.concat([datos, etiquetas], ignore_index=True)\n",
        "    i += 1\n",
        "\n",
        "  print('Data set creado de tama√±o: ', datos.shape)\n",
        "\n",
        "  numeric_cols = ['instancia_id', 'token_id', 'punt_inicial', 'punt_final', 'capitalizaci√≥n']\n",
        "  for col in numeric_cols:\n",
        "    datos[col] = pd.to_numeric(datos[col], errors='coerce')\n",
        "\n",
        "  return parrafos, datos, i"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0zgQ4kTNeFIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agregar_embeddings(df_X):\n",
        "  token_ids = df_X['token_id'].tolist()\n",
        "  embeddings_list = []\n",
        "  for token_id  in token_ids:\n",
        "      if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "        token_id = tokenizer.unk_token_id\n",
        "      # Detach the tensor before converting to list\n",
        "      embedding = model.embeddings.word_embeddings.weight[token_id].detach().tolist()\n",
        "      embeddings_list.append(embedding)\n",
        "  df_X['embeddings'] = embeddings_list\n",
        "  # The embeddings are already lists of floats, no need to convert to int\n",
        "  # df_X['embeddings'] = df_X['embeddings'].apply(lambda x: [int(i) for i in x])\n",
        "\n",
        "  return df_X"
      ],
      "metadata": {
        "id": "5BRwbGA7sCsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trasformar_df_dfPyTorch(datos_X, datos_Y):\n",
        "  # Convert the list of lists in the 'embeddings' column to a NumPy array of floats\n",
        "  embeddings_array = np.array(datos_X['embeddings'].tolist(), dtype=np.float32)\n",
        "  X = torch.tensor(embeddings_array, dtype=torch.float32)\n",
        "  Y = torch.tensor(datos_Y[['punt_inicial', 'punt_final', 'capitalizaci√≥n']].values, dtype=torch.float32)\n",
        "  dataSetPT = TensorDataset(X, Y)\n",
        "\n",
        "  return dataSetPT"
      ],
      "metadata": {
        "id": "pKAgz6wenZVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Puntuacion inicial:\n",
        "*   \"\": 0\n",
        "*   \"¬ø\": 1\n",
        "\n",
        "### Puntuacion final:\n",
        "*   \"\": 0\n",
        "*   \".\": 1\n",
        "*   \",\": 2\n",
        "*   \"?\": 3\n",
        "\n"
      ],
      "metadata": {
        "id": "YPEqeNwVoiTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset basico con un libro de Harry Potter"
      ],
      "metadata": {
        "id": "ISGg2S2RK0Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busca libro en formato epub de mi github"
      ],
      "metadata": {
        "id": "P2WDL7S3PtV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'"
      ],
      "metadata": {
        "id": "NxMriymePxNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'"
      ],
      "metadata": {
        "id": "HbrjtNdCP4EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = path + libro1"
      ],
      "metadata": {
        "id": "6LlG5OLZQO8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AwL64C6xIo6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos, dataSet = crearDataSet('libro1.epub')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2AWjTOSfQc9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X = dataSet[['instancia_id', 'token_id', 'token']]\n",
        "datos_Y = dataSet[['punt_inicial', 'punt_final', 'capitalizaci√≥n']]"
      ],
      "metadata": {
        "id": "QKYNGgoxtpw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_X_ext = agregar_embeddings(datos_X)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fzP6Q4CWwEEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSetPT = trasformar_df_dfPyTorch(datos_X, datos_Y)"
      ],
      "metadata": {
        "id": "7X_CWg4gqPkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X0, Y0 = dataSetPT[0]\n",
        "print(\"X[0]:\", X0)\n",
        "print(\"Y[0]:\", Y0)"
      ],
      "metadata": {
        "id": "PRdL2VUnqRQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataser definitivo con muchos libros pdf"
      ],
      "metadata": {
        "id": "iIxSNYcM9s_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "carpeta = \"libros_pdf\"\n",
        "#todos_parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "#todos_datos = pd.DataFrame(columns=['instancia_id', 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalizaci√≥n'])\n",
        "\n",
        "todos_parrafos = pd.read_csv(\"/content/drive/MyDrive/colab/dataset_parrafos_total3.csv\")\n",
        "todos_datos    = pd.read_csv(\"/content/drive/MyDrive/colab/dataset_datos_total3.csv\")\n",
        "\n",
        "i = len(todos_parrafos) #0\n",
        "l = 129 #0\n",
        "lista_de_libros = os.listdir(carpeta)\n",
        "libros_restantes = lista_de_libros[l:]\n",
        "for nombre in libros_restantes: #os.listdir(carpeta):\n",
        "    ruta = os.path.join(carpeta, nombre)\n",
        "    try:\n",
        "      if nombre.lower().endswith(\".pdf\"):\n",
        "        parrafos, datos, i = crearDataSet(ruta,i, pdf=True)\n",
        "      elif nombre.lower().endswith(\".epub\"):\n",
        "        parrafos, datos = crearDataSet(ruta, pdf=False)\n",
        "      else:\n",
        "        print(f\"‚ö†Ô∏è Saltando {nombre} (no es PDF ni EPUB)\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "      print(f\"‚ùå Error procesando {nombre}: {e}\")\n",
        "      continue\n",
        "    todos_parrafos = pd.concat([todos_parrafos, parrafos], ignore_index=True)\n",
        "    todos_datos = pd.concat([todos_datos, datos], ignore_index=True)\n",
        "    print(f\"‚úÖ Procesado el libro {l}: {nombre} ({len(parrafos)} p√°rrafos, {len(datos)} tokens)\")\n",
        "    print(f\"Cantidad de parrafos totales: {len(todos_parrafos)}\")\n",
        "    print(f\"Cantidad de tokens totales: {len(todos_datos)}\")\n",
        "    l += 1\n",
        "    todos_parrafos.to_csv(\"/content/drive/MyDrive/colab/dataset_parrafos_total3.csv\", index=False)\n",
        "    todos_datos.to_csv(\"/content/drive/MyDrive/colab/dataset_datos_total3.csv\", index=False)\n",
        "    print(f\"Se guard√≥ autom√°ticamente en Drive\")\n",
        "\n",
        "print(f\"\\nüíæ Dataset total creado: {len(todos_parrafos)} p√°rrafos y {len(todos_datos)} tokens.\")\n"
      ],
      "metadata": {
        "id": "iNRShDAY96h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostramos el print de la celda anterior de como va creando el dataset\n",
        "\"\"\"\n",
        "Data set creado de tama√±o:  (6960, 6)\n",
        "‚úÖ Procesado el libro 129: El juego de la vida y c√≥mo jugarlo autor Florence Scovel Shinn.pdf (234 p√°rrafos, 6960 tokens)\n",
        "Cantidad de parrafos totales: 149044\n",
        "Cantidad de tokens totales: 4199231\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (20676, 6)\n",
        "‚úÖ Procesado el libro 130: El libro de la selva, Rudyard Kipling.pdf (968 p√°rrafos, 20676 tokens)\n",
        "Cantidad de parrafos totales: 150012\n",
        "Cantidad de tokens totales: 4219907\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (34832, 6)\n",
        "‚úÖ Procesado el libro 131: 1984 Autor George Orwell.pdf (1436 p√°rrafos, 34832 tokens)\n",
        "Cantidad de parrafos totales: 151448\n",
        "Cantidad de tokens totales: 4254739\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (25964, 6)\n",
        "‚úÖ Procesado el libro 132: 2  La isla del tesoro autor Robert Louise Stenvenson.pdf (968 p√°rrafos, 25964 tokens)\n",
        "Cantidad de parrafos totales: 152416\n",
        "Cantidad de tokens totales: 4280703\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (15679, 6)\n",
        "‚úÖ Procesado el libro 133: 11. El castillo de Otranto, Horace Walpole.pdf (688 p√°rrafos, 15679 tokens)\n",
        "Cantidad de parrafos totales: 153104\n",
        "Cantidad de tokens totales: 4296382\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (15673, 6)\n",
        "‚úÖ Procesado el libro 134: 14. The Turn of the Screw, Henry James.pdf (543 p√°rrafos, 15673 tokens)\n",
        "Cantidad de parrafos totales: 153647\n",
        "Cantidad de tokens totales: 4312055\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (16910, 6)\n",
        "‚úÖ Procesado el libro 135: 27 Las penas del joven Werther autor Johann Wolfgang von Goethe.pdf (557 p√°rrafos, 16910 tokens)\n",
        "Cantidad de parrafos totales: 154204\n",
        "Cantidad de tokens totales: 4328965\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "‚ùå Error procesando 126 - El inversor inteligente - Warren Buffett.pdf: Unexpected EOF\n",
        "Data set creado de tama√±o:  (103824, 6)\n",
        "‚úÖ Procesado el libro 136: 06. Melmoth el errabundo Autor Charles Robert Maturin.pdf (2574 p√°rrafos, 103824 tokens)\n",
        "Cantidad de parrafos totales: 156778\n",
        "Cantidad de tokens totales: 4432789\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (35110, 6)\n",
        "‚úÖ Procesado el libro 137: 127-Coaching para Dummies - Jeni Mumford.pdf (1491 p√°rrafos, 35110 tokens)\n",
        "Cantidad de parrafos totales: 158269\n",
        "Cantidad de tokens totales: 4467899\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (3754, 6)\n",
        "‚úÖ Procesado el libro 138: La confianza en uno mismo autor Ralph Waldo Emerson.pdf (115 p√°rrafos, 3754 tokens)\n",
        "Cantidad de parrafos totales: 158384\n",
        "Cantidad de tokens totales: 4471653\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (70477, 6)\n",
        "‚úÖ Procesado el libro 139: La Rep√∫blica Autor Plat√≥n.pdf (2377 p√°rrafos, 70477 tokens)\n",
        "Cantidad de parrafos totales: 160761\n",
        "Cantidad de tokens totales: 4542130\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (41475, 6)\n",
        "‚úÖ Procesado el libro 140: 05. Los siete locos Autor Roberto Arlt.pdf (1845 p√°rrafos, 41475 tokens)\n",
        "Cantidad de parrafos totales: 162606\n",
        "Cantidad de tokens totales: 4583605\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (52870, 6)\n",
        "‚úÖ Procesado el libro 141: Rayuela Autor Julio Cortazar.pdf (2012 p√°rrafos, 52870 tokens)\n",
        "Cantidad de parrafos totales: 164618\n",
        "Cantidad de tokens totales: 4636475\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (8659, 6)\n",
        "‚úÖ Procesado el libro 142: 31 La historia del doctor Dolittle autor Hugh Lofting.pdf (386 p√°rrafos, 8659 tokens)\n",
        "Cantidad de parrafos totales: 165004\n",
        "Cantidad de tokens totales: 4645134\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (20017, 6)\n",
        "‚úÖ Procesado el libro 143: las-aventuras-de-tom-sawyer-mark-twain.pdf (1082 p√°rrafos, 20017 tokens)\n",
        "Cantidad de parrafos totales: 166086\n",
        "Cantidad de tokens totales: 4665151\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (67334, 6)\n",
        "‚úÖ Procesado el libro 144: PODER SIN LIMITES - ANTHONY ROBBINS 1.pdf (1880 p√°rrafos, 67334 tokens)\n",
        "Cantidad de parrafos totales: 167966\n",
        "Cantidad de tokens totales: 4732485\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (27173, 6)\n",
        "‚úÖ Procesado el libro 145: el-talon-de-hierro-jack-london.pdf (923 p√°rrafos, 27173 tokens)\n",
        "Cantidad de parrafos totales: 168889\n",
        "Cantidad de tokens totales: 4759658\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (3655, 6)\n",
        "‚úÖ Procesado el libro 146: EMPRENDEDOR - ANTHONY ROBBINS.pdf (162 p√°rrafos, 3655 tokens)\n",
        "Cantidad de parrafos totales: 169051\n",
        "Cantidad de tokens totales: 4763313\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (11676, 6)\n",
        "‚úÖ Procesado el libro 147: El Kybalion Autor Tres Iniciados.pdf (266 p√°rrafos, 11676 tokens)\n",
        "Cantidad de parrafos totales: 169317\n",
        "Cantidad de tokens totales: 4774989\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (93789, 6)\n",
        "‚úÖ Procesado el libro 148: 10 Mujeres enamoradas autor D H Lawrence.pdf (4221 p√°rrafos, 93789 tokens)\n",
        "Cantidad de parrafos totales: 173538\n",
        "Cantidad de tokens totales: 4868778\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (24138, 6)\n",
        "‚úÖ Procesado el libro 149: Un Mundo Feliz Autor Aldous Huxley.pdf (1148 p√°rrafos, 24138 tokens)\n",
        "Cantidad de parrafos totales: 174686\n",
        "Cantidad de tokens totales: 4892916\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (68461, 6)\n",
        "‚úÖ Procesado el libro 150: 15 Emma autor Jane Austen.pdf (2354 p√°rrafos, 68461 tokens)\n",
        "Cantidad de parrafos totales: 177040\n",
        "Cantidad de tokens totales: 4961377\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (24318, 6)\n",
        "‚úÖ Procesado el libro 151: un-mundo-feliz-aldous-huxley.pdf (1140 p√°rrafos, 24318 tokens)\n",
        "Cantidad de parrafos totales: 178180\n",
        "Cantidad de tokens totales: 4985695\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (33096, 6)\n",
        "‚úÖ Procesado el libro 152: 18 Mujercitas autor Louisa May Alcott.pdf (1142 p√°rrafos, 33096 tokens)\n",
        "Cantidad de parrafos totales: 179322\n",
        "Cantidad de tokens totales: 5018791\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (35748, 6)\n",
        "‚úÖ Procesado el libro 153: El Amor en Tiempo de C√≥lera Autor Gabriel Garc√≠a M√°rquez.pdf (744 p√°rrafos, 35748 tokens)\n",
        "Cantidad de parrafos totales: 180066\n",
        "Cantidad de tokens totales: 5054539\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (7531, 6)\n",
        "‚úÖ Procesado el libro 154: Los Cuatro Acuerdos Autor Miguel Ruiz.pdf (322 p√°rrafos, 7531 tokens)\n",
        "Cantidad de parrafos totales: 180388\n",
        "Cantidad de tokens totales: 5062070\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (9046, 6)\n",
        "‚úÖ Procesado el libro 155: 024-El vendedor m√°s grande del mundo- Og Mandin.pdf (364 p√°rrafos, 9046 tokens)\n",
        "Cantidad de parrafos totales: 180752\n",
        "Cantidad de tokens totales: 5071116\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (49259, 6)\n",
        "‚úÖ Procesado el libro 156: VENTAS PARA DUMMIES - TOM HOPKINS.pdf (1538 p√°rrafos, 49259 tokens)\n",
        "Cantidad de parrafos totales: 182290\n",
        "Cantidad de tokens totales: 5120375\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (53762, 6)\n",
        "‚úÖ Procesado el libro 157: Perdiendo la virginidad- Richard Branson.pdf (1810 p√°rrafos, 53762 tokens)\n",
        "Cantidad de parrafos totales: 184100\n",
        "Cantidad de tokens totales: 5174137\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (37750, 6)\n",
        "‚úÖ Procesado el libro 158: El Diario de Ana Frank Autor Ana Frank.pdf (1329 p√°rrafos, 37750 tokens)\n",
        "Cantidad de parrafos totales: 185429\n",
        "Cantidad de tokens totales: 5211887\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (23577, 6)\n",
        "‚úÖ Procesado el libro 159: 13 Peter Pan y Wendy autor James Matthew Barrie.pdf (989 p√°rrafos, 23577 tokens)\n",
        "Cantidad de parrafos totales: 186418\n",
        "Cantidad de tokens totales: 5235464\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (41857, 6)\n",
        "‚úÖ Procesado el libro 160: 21 El rey del mar autor Emilio Salgari.pdf (1866 p√°rrafos, 41857 tokens)\n",
        "Cantidad de parrafos totales: 188284\n",
        "Cantidad de tokens totales: 5277321\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (9683, 6)\n",
        "‚úÖ Procesado el libro 161: Bhagavad Gita autor Viasa (1).pdf (255 p√°rrafos, 9683 tokens)\n",
        "Cantidad de parrafos totales: 188539\n",
        "Cantidad de tokens totales: 5287004\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (44359, 6)\n",
        "‚úÖ Procesado el libro 162: 01. Dona Barbara  Autor Romulo Gallegos.pdf (1384 p√°rrafos, 44359 tokens)\n",
        "Cantidad de parrafos totales: 189923\n",
        "Cantidad de tokens totales: 5331363\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (27560, 6)\n",
        "‚úÖ Procesado el libro 163: 20 Persuasi√≥n autor Jane Austen.pdf (994 p√°rrafos, 27560 tokens)\n",
        "Cantidad de parrafos totales: 190917\n",
        "Cantidad de tokens totales: 5358923\n",
        "WARNING:pdfplumber.pdf:[WARNING] Metadata key \"CreationDate\" could not be parsed due to exception: maximum recursion depth exceeded\n",
        "WARNING:pdfplumber.pdf:[WARNING] Metadata key \"Creator\" could not be parsed due to exception: maximum recursion depth exceeded\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (2278, 6)\n",
        "‚úÖ Procesado el libro 164: Dhammapada Autor Buda.pdf (89 p√°rrafos, 2278 tokens)\n",
        "Cantidad de parrafos totales: 191006\n",
        "Cantidad de tokens totales: 5361201\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (16814, 6)\n",
        "‚úÖ Procesado el libro 165: 096-El Arte De Cautivar - Guy Kawasaki.pdf (601 p√°rrafos, 16814 tokens)\n",
        "Cantidad de parrafos totales: 191607\n",
        "Cantidad de tokens totales: 5378015\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (52854, 6)\n",
        "‚úÖ Procesado el libro 166: 10. El Monje, Matthew Lewis.pdf (2101 p√°rrafos, 52854 tokens)\n",
        "Cantidad de parrafos totales: 193708\n",
        "Cantidad de tokens totales: 5430869\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (21659, 6)\n",
        "‚úÖ Procesado el libro 167: el-proceso-franz-kafka.pdf (927 p√°rrafos, 21659 tokens)\n",
        "Cantidad de parrafos totales: 194635\n",
        "Cantidad de tokens totales: 5452528\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (28822, 6)\n",
        "‚úÖ Procesado el libro 168: 30 Una Habitaci√≥n con Vistas autor E. M. Foster.pdf (1487 p√°rrafos, 28822 tokens)\n",
        "Cantidad de parrafos totales: 196122\n",
        "Cantidad de tokens totales: 5481350\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (22414, 6)\n",
        "‚úÖ Procesado el libro 169: 20907-el-proceso-franz-kafka.pdf (926 p√°rrafos, 22414 tokens)\n",
        "Cantidad de parrafos totales: 197048\n",
        "Cantidad de tokens totales: 5503764\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (45459, 6)\n",
        "‚úÖ Procesado el libro 170: 32 Las alegres aventuras autor Robin Hood Howard Pyle.pdf (1414 p√°rrafos, 45459 tokens)\n",
        "Cantidad de parrafos totales: 198462\n",
        "Cantidad de tokens totales: 5549223\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (43618, 6)\n",
        "‚úÖ Procesado el libro 171: 150 - Pensar R√°pido, Pensar Despacio - Daniel K.pdf (1318 p√°rrafos, 43618 tokens)\n",
        "Cantidad de parrafos totales: 199780\n",
        "Cantidad de tokens totales: 5592841\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (11921, 6)\n",
        "‚úÖ Procesado el libro 172: Meditaciones Autor Marco Aurelio.pdf (458 p√°rrafos, 11921 tokens)\n",
        "Cantidad de parrafos totales: 200238\n",
        "Cantidad de tokens totales: 5604762\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (26061, 6)\n",
        "‚úÖ Procesado el libro 173: TUS ZONAS ERRONEAS - WAYNE W. DYER - 243 PAGINA.pdf (886 p√°rrafos, 26061 tokens)\n",
        "Cantidad de parrafos totales: 201124\n",
        "Cantidad de tokens totales: 5630823\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (22507, 6)\n",
        "‚úÖ Procesado el libro 174: PIENSE Y HAGASE RICO - NAPOLEON HILL - 174 PAGI.pdf (729 p√°rrafos, 22507 tokens)\n",
        "Cantidad de parrafos totales: 201853\n",
        "Cantidad de tokens totales: 5653330\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (10117, 6)\n",
        "‚úÖ Procesado el libro 175: El Kybalion Autor Tres Iniciados (1).pdf (262 p√°rrafos, 10117 tokens)\n",
        "Cantidad de parrafos totales: 202115\n",
        "Cantidad de tokens totales: 5663447\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (27098, 6)\n",
        "‚úÖ Procesado el libro 176: 1 La isla del tesoro autor Robert Louise Stenvenson.pdf (963 p√°rrafos, 27098 tokens)\n",
        "Cantidad de parrafos totales: 203078\n",
        "Cantidad de tokens totales: 5690545\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (33724, 6)\n",
        "‚úÖ Procesado el libro 177: La rueda de la vida- Elisabeth K√ºbler-Ross.pdf (1230 p√°rrafos, 33724 tokens)\n",
        "Cantidad de parrafos totales: 204308\n",
        "Cantidad de tokens totales: 5724269\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (27363, 6)\n",
        "‚úÖ Procesado el libro 178: mujercitas-louisa-may-alcott.pdf (1014 p√°rrafos, 27363 tokens)\n",
        "Cantidad de parrafos totales: 205322\n",
        "Cantidad de tokens totales: 5751632\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (43309, 6)\n",
        "‚úÖ Procesado el libro 179: As√≠ habl√≥ Zaratustra Autor Friedrich Wilhelm Nietzsche (2).pdf (1542 p√°rrafos, 43309 tokens)\n",
        "Cantidad de parrafos totales: 206864\n",
        "Cantidad de tokens totales: 5794941\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (3541, 6)\n",
        "‚úÖ Procesado el libro 180: 40 El fantasma de Canterville autor Oscar Wilde.pdf (96 p√°rrafos, 3541 tokens)\n",
        "Cantidad de parrafos totales: 206960\n",
        "Cantidad de tokens totales: 5798482\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (22903, 6)\n",
        "‚úÖ Procesado el libro 181: EL ARTE DE CERRAR LA VENTA - BRIAN TRACY - 212 .pdf (636 p√°rrafos, 22903 tokens)\n",
        "Cantidad de parrafos totales: 207596\n",
        "Cantidad de tokens totales: 5821385\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (46030, 6)\n",
        "‚úÖ Procesado el libro 182: 3 Madame Bovary autor Gustave Flaubert.pdf (1518 p√°rrafos, 46030 tokens)\n",
        "Cantidad de parrafos totales: 209114\n",
        "Cantidad de tokens totales: 5867415\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (9389, 6)\n",
        "‚úÖ Procesado el libro 183: 068-Gopro, 7 pasos para convertirse en un profe.pdf (462 p√°rrafos, 9389 tokens)\n",
        "Cantidad de parrafos totales: 209576\n",
        "Cantidad de tokens totales: 5876804\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (6463, 6)\n",
        "‚úÖ Procesado el libro 184: 08. Carmilla, Sheridan Le Fanu.pdf (292 p√°rrafos, 6463 tokens)\n",
        "Cantidad de parrafos totales: 209868\n",
        "Cantidad de tokens totales: 5883267\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (18170, 6)\n",
        "‚úÖ Procesado el libro 185: Piense y H√°gase Rico Autor Napoleon Hill.pdf (653 p√°rrafos, 18170 tokens)\n",
        "Cantidad de parrafos totales: 210521\n",
        "Cantidad de tokens totales: 5901437\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (9548, 6)\n",
        "‚úÖ Procesado el libro 186: 160-Padre rico, padre pobre para j√≥venes - Robe.pdf (389 p√°rrafos, 9548 tokens)\n",
        "Cantidad de parrafos totales: 210910\n",
        "Cantidad de tokens totales: 5910985\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (34586, 6)\n",
        "‚úÖ Procesado el libro 187: Leviat√°n Autor Thomas Hobbes.pdf (622 p√°rrafos, 34586 tokens)\n",
        "Cantidad de parrafos totales: 211532\n",
        "Cantidad de tokens totales: 5945571\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (20902, 6)\n",
        "‚úÖ Procesado el libro 188: El Lider Que No Tenia Cargo.pdf (994 p√°rrafos, 20902 tokens)\n",
        "Cantidad de parrafos totales: 212526\n",
        "Cantidad de tokens totales: 5966473\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (12620, 6)\n",
        "‚úÖ Procesado el libro 189: El Alquimista Autor Paulo Coelho.pdf (598 p√°rrafos, 12620 tokens)\n",
        "Cantidad de parrafos totales: 213124\n",
        "Cantidad de tokens totales: 5979093\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (72611, 6)\n",
        "‚úÖ Procesado el libro 190: 29 Grandes esperanzas autor Charles Dickens.pdf (2580 p√°rrafos, 72611 tokens)\n",
        "Cantidad de parrafos totales: 215704\n",
        "Cantidad de tokens totales: 6051704\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (29041, 6)\n",
        "‚úÖ Procesado el libro 191: 090-Tus Zonas Err√≥neas - Wayne Dyer.pdf (918 p√°rrafos, 29041 tokens)\n",
        "Cantidad de parrafos totales: 216622\n",
        "Cantidad de tokens totales: 6080745\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (45987, 6)\n",
        "‚úÖ Procesado el libro 192: 6 Madame Bovary autor Gustave Flaubert.pdf (1527 p√°rrafos, 45987 tokens)\n",
        "Cantidad de parrafos totales: 218149\n",
        "Cantidad de tokens totales: 6126732\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (16046, 6)\n",
        "‚úÖ Procesado el libro 193: 020-El Hombre Mas Rico De Babilonia - George S..pdf (586 p√°rrafos, 16046 tokens)\n",
        "Cantidad de parrafos totales: 218735\n",
        "Cantidad de tokens totales: 6142778\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (43398, 6)\n",
        "‚úÖ Procesado el libro 194: Michael Jordan- Maximo Jose Tobias.pdf (963 p√°rrafos, 43398 tokens)\n",
        "Cantidad de parrafos totales: 219698\n",
        "Cantidad de tokens totales: 6186176\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (25212, 6)\n",
        "‚úÖ Procesado el libro 195: El color de la libertad - Nelson Mandela.pdf (687 p√°rrafos, 25212 tokens)\n",
        "Cantidad de parrafos totales: 220385\n",
        "Cantidad de tokens totales: 6211388\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (4629, 6)\n",
        "‚úÖ Procesado el libro 196: 2 Los cr√≠menes de la calle Morgue autor Edgar Allan Poe.pdf (152 p√°rrafos, 4629 tokens)\n",
        "Cantidad de parrafos totales: 220537\n",
        "Cantidad de tokens totales: 6216017\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (43786, 6)\n",
        "‚úÖ Procesado el libro 197: As√≠ habl√≥ Zaratustra Autor Friedrich Wilhelm Nietzsche (1).pdf (1546 p√°rrafos, 43786 tokens)\n",
        "Cantidad de parrafos totales: 222083\n",
        "Cantidad de tokens totales: 6259803\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (23556, 6)\n",
        "‚úÖ Procesado el libro 198: pobres-gentes-fyodor-dostoyevsky (1).pdf (862 p√°rrafos, 23556 tokens)\n",
        "Cantidad de parrafos totales: 222945\n",
        "Cantidad de tokens totales: 6283359\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (16519, 6)\n",
        "‚úÖ Procesado el libro 199: EL HOMBRE MAS RICO DE BABILONIA - GEORGE S. CLA.pdf (587 p√°rrafos, 16519 tokens)\n",
        "Cantidad de parrafos totales: 223532\n",
        "Cantidad de tokens totales: 6299878\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (26518, 6)\n",
        "‚úÖ Procesado el libro 200: El Retrato de Dorian Gray Autor Oscar Wilde.pdf (1483 p√°rrafos, 26518 tokens)\n",
        "Cantidad de parrafos totales: 225015\n",
        "Cantidad de tokens totales: 6326396\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (42223, 6)\n",
        "‚úÖ Procesado el libro 201: 077-MBA personal - Josh Kaufman.pdf (1383 p√°rrafos, 42223 tokens)\n",
        "Cantidad de parrafos totales: 226398\n",
        "Cantidad de tokens totales: 6368619\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (17380, 6)\n",
        "‚úÖ Procesado el libro 202: 29 Azabache autor Anna Sewell.pdf (603 p√°rrafos, 17380 tokens)\n",
        "Cantidad de parrafos totales: 227001\n",
        "Cantidad de tokens totales: 6385999\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (26727, 6)\n",
        "‚úÖ Procesado el libro 203: La Divina Comedia Autor Dante Alighieri.pdf (508 p√°rrafos, 26727 tokens)\n",
        "Cantidad de parrafos totales: 227509\n",
        "Cantidad de tokens totales: 6412726\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (5750, 6)\n",
        "‚úÖ Procesado el libro 204: El Principito Autor Antoine de Saint-Exup√©ry.pdf (356 p√°rrafos, 5750 tokens)\n",
        "Cantidad de parrafos totales: 227865\n",
        "Cantidad de tokens totales: 6418476\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (44002, 6)\n",
        "‚úÖ Procesado el libro 205: 047-Einstein, Su Vida y su Universo - Walter Is.pdf (1264 p√°rrafos, 44002 tokens)\n",
        "Cantidad de parrafos totales: 229129\n",
        "Cantidad de tokens totales: 6462478\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (23642, 6)\n",
        "‚úÖ Procesado el libro 206: pobres-gentes-fyodor-dostoyevsky.pdf (858 p√°rrafos, 23642 tokens)\n",
        "Cantidad de parrafos totales: 229987\n",
        "Cantidad de tokens totales: 6486120\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (23064, 6)\n",
        "‚úÖ Procesado el libro 207: FUERA DE SERIE - PORQUE UNAS PERSONAS TIENEN EX.pdf (842 p√°rrafos, 23064 tokens)\n",
        "Cantidad de parrafos totales: 230829\n",
        "Cantidad de tokens totales: 6509184\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (40646, 6)\n",
        "‚úÖ Procesado el libro 208: 02. Mar√≠a Autor Jorge Isaacs.pdf (1598 p√°rrafos, 40646 tokens)\n",
        "Cantidad de parrafos totales: 232427\n",
        "Cantidad de tokens totales: 6549830\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (30156, 6)\n",
        "‚úÖ Procesado el libro 209: El codigo Federer- Stefano Semeraro y Ana Ciura.pdf (916 p√°rrafos, 30156 tokens)\n",
        "Cantidad de parrafos totales: 233343\n",
        "Cantidad de tokens totales: 6579986\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (51266, 6)\n",
        "‚úÖ Procesado el libro 210: Dr√°cula Autor Bram Stoker.pdf (1757 p√°rrafos, 51266 tokens)\n",
        "Cantidad de parrafos totales: 235100\n",
        "Cantidad de tokens totales: 6631252\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (17727, 6)\n",
        "‚úÖ Procesado el libro 211: 117-Incrementa Tu IQ Financiero - Robert T Kiyo.pdf (537 p√°rrafos, 17727 tokens)\n",
        "Cantidad de parrafos totales: 235637\n",
        "Cantidad de tokens totales: 6648979\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (25681, 6)\n",
        "‚úÖ Procesado el libro 212: 064-El Toque de Midas - Donald Trump y Robert K.pdf (945 p√°rrafos, 25681 tokens)\n",
        "Cantidad de parrafos totales: 236582\n",
        "Cantidad de tokens totales: 6674660\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (14059, 6)\n",
        "‚úÖ Procesado el libro 213: 11 Las aventuras de Alicia en el pa√≠s de las maravillas autor  Lewis Carroll.pdf (519 p√°rrafos, 14059 tokens)\n",
        "Cantidad de parrafos totales: 237101\n",
        "Cantidad de tokens totales: 6688719\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (23249, 6)\n",
        "‚úÖ Procesado el libro 214: 03. Frankenstein, Mary Shelley.pdf (689 p√°rrafos, 23249 tokens)\n",
        "Cantidad de parrafos totales: 237790\n",
        "Cantidad de tokens totales: 6711968\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (60661, 6)\n",
        "‚úÖ Procesado el libro 215: 33 Tess, D‚ÄôUrberville autor Thomas Hardy.pdf (2036 p√°rrafos, 60661 tokens)\n",
        "Cantidad de parrafos totales: 239826\n",
        "Cantidad de tokens totales: 6772629\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "Data set creado de tama√±o:  (82390, 6)\n",
        "‚úÖ Procesado el libro 216: 049-El Lobo de Wall Street - Jordan Belfort.pdf (3809 p√°rrafos, 82390 tokens)\n",
        "Cantidad de parrafos totales: 243635\n",
        "Cantidad de tokens totales: 6855019\n",
        "Se guard√≥ autom√°ticamente en Drive\n",
        "\n",
        "üíæ Dataset total creado: 243635 p√°rrafos y 6855019 tokens.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dfTO5bbocHGP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}