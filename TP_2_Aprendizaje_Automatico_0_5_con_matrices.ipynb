{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioSigal/Aprendizaje-Automatico-I-y-II/blob/main/TP_2_Aprendizaje_Automatico_0_5_con_matrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports\n"
      ],
      "metadata": {
        "id": "GJ8imGsu_vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjfXeaWE_og8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FRAME\n"
      ],
      "metadata": {
        "id": "L_xHy5FQ_5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rgdLKjU1JUOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "lCrU1I06J0fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/colab/dataset_datos_total3.csv\")"
      ],
      "metadata": {
        "id": "T8Hgq2B7uiQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar tokens por párrafo\n",
        "df_por_parrafos = df.groupby(\"instancia_id\").agg({\n",
        "    \"token_id\":list,\n",
        "    \"token\": list,\n",
        "    \"punt_inicial\": list,\n",
        "    \"punt_final\": list,\n",
        "    \"capitalización\": list\n",
        "}).reset_index()\n"
      ],
      "metadata": {
        "id": "I3yuvW8Esk5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows(): # Correctly iterate over DataFrame rows\n",
        "  bi = 0\n",
        "  bf = 0\n",
        "  bc = 0\n",
        "  if row[\"punt_inicial\"] is None:\n",
        "    bi += 1\n",
        "  if row[\"punt_final\"] is None:\n",
        "    bf += 1\n",
        "  if row[\"capitalización\"] is None:\n",
        "    bc += 1\n",
        "\n",
        "print(bi,bf,bc)"
      ],
      "metadata": {
        "id": "W-ZElgGrwoyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b14c685"
      },
      "source": [
        "import math\n",
        "\n",
        "def count_nans_in_df_lists(dataframe, columns_to_check=None):\n",
        "    \"\"\"\n",
        "    Cuenta los valores NaN en las listas contenidas en las columnas especificadas de un DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): El DataFrame a revisar.\n",
        "        columns_to_check (list, optional): Lista de nombres de columnas a revisar.\n",
        "                                          Si es None, revisa 'punt_inicial', 'punt_final', 'capitalización'.\n",
        "\n",
        "    Returns:\n",
        "        dict: Un diccionario con el conteo de NaN por columna.\n",
        "    \"\"\"\n",
        "    if columns_to_check is None:\n",
        "        columns_to_check = ['punt_inicial', 'punt_final', 'capitalización']\n",
        "\n",
        "    nan_counts = {col: 0 for col in columns_to_check}\n",
        "    total_nans_found = 0\n",
        "\n",
        "    for col in columns_to_check:\n",
        "        if col not in dataframe.columns:\n",
        "            print(f\"Advertencia: La columna '{col}' no se encuentra en el DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        for index, row_list in dataframe[col].items():\n",
        "            if isinstance(row_list, list):\n",
        "                for item in row_list:\n",
        "                    if isinstance(item, float) and math.isnan(item):\n",
        "                        nan_counts[col] += 1\n",
        "                        total_nans_found += 1\n",
        "            elif isinstance(row_list, float) and math.isnan(row_list): # Handle cases where the cell itself is NaN, not a list\n",
        "                nan_counts[col] += 1\n",
        "                total_nans_found += 1\n",
        "\n",
        "    print(f\"Resumen de NaN encontrados por columna:\")\n",
        "    for col, count in nan_counts.items():\n",
        "        print(f\"  - '{col}': {count} NaN(s)\")\n",
        "    print(f\"Total de NaN encontrados en todas las columnas: {total_nans_found}\")\n",
        "\n",
        "    return nan_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f09ac04"
      },
      "source": [
        "import math\n",
        "\n",
        "def filter_rows_with_nans_in_lists(df, columns):\n",
        "    \"\"\"\n",
        "    Filtra las filas de un DataFrame donde alguna de las listas en las columnas especificadas\n",
        "    contenga valores NaN.\n",
        "    \"\"\"\n",
        "    initial_rows = len(df)\n",
        "    print(f\"Filas iniciales en el DataFrame: {initial_rows}\")\n",
        "\n",
        "    # Crear una máscara booleana para las filas a mantener (True si NO hay NaN)\n",
        "    mask = pd.Series([True] * len(df), index=df.index)\n",
        "\n",
        "    for col in columns:\n",
        "        # Para cada fila, comprueba si alguna lista contiene NaN\n",
        "        # Aplicamos `.all()` para asegurarnos de que solo se considere False si hay al menos un NaN\n",
        "        col_mask = df[col].apply(lambda lst: not any(isinstance(x, float) and math.isnan(x) for x in lst) if isinstance(lst, list) else (not (isinstance(lst, float) and math.isnan(lst))))\n",
        "        mask = mask & col_mask\n",
        "\n",
        "    df_filtered = df[mask].reset_index(drop=True)\n",
        "    removed_rows = initial_rows - len(df_filtered)\n",
        "    print(f\"Filas con NaN eliminadas: {removed_rows}\")\n",
        "    print(f\"Filas restantes en el DataFrame: {len(df_filtered)}\")\n",
        "    return df_filtered\n",
        "\n",
        "# Columnas donde buscar NaN en las listas\n",
        "columns_to_check = ['punt_inicial', 'punt_final', 'capitalización']\n",
        "\n",
        "# Filtrar df_por_parrafos\n",
        "df_por_parrafos = filter_rows_with_nans_in_lists(df_por_parrafos, columns_to_check)\n",
        "\n",
        "# Opcional: Volver a verificar los NaN después de filtrar\n",
        "print(\"\\nVerificando NaN después de filtrar:\")\n",
        "count_nans_in_df_lists(df_por_parrafos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "164dc533"
      },
      "source": [
        "print(\"Contando NaN en df_por_parrafos...\")\n",
        "count_nans_in_df_lists(df_por_parrafos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos[\"token\"][1])\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "eXGAW5MJvkds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TOKENIZER & BERT EMBEDDINGS"
      ],
      "metadata": {
        "id": "uVHH_Ow__8fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##TOKENIZER & BERT EMBEDDINGS\"\"\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_bert.eval()\n"
      ],
      "metadata": {
        "id": "LYMyfzMG_3JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelar parámetros de BERT para ahorrar memoria\n",
        "for param in model_bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "89bEgteB49eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CREAR CLASE DATASET PARA PYTORCH"
      ],
      "metadata": {
        "id": "A1Zpp0aEjuXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DynamicEmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset de pytorch que calcula embeddings dinámicamente\"\"\"\n",
        "    def __init__(self, df, tokenizer, bert_model):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.embedding_matrix = bert_model.embeddings.word_embeddings.weight\n",
        "        self.embedding_matrix.requires_grad = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        token_id_list = row[\"token_id\"]\n",
        "\n",
        "        # Calcular embeddings dinámicamente\n",
        "        token_embeddings = []\n",
        "        for token_id in token_id_list:\n",
        "            if token_id is None or token_id == self.tokenizer.unk_token_id:\n",
        "                token_id = self.tokenizer.unk_token_id\n",
        "            emb = self.embedding_matrix[token_id].detach()\n",
        "            token_embeddings.append(emb)\n",
        "\n",
        "        # Convertir a tensor\n",
        "        if token_embeddings:\n",
        "            embeddings = torch.stack(token_embeddings)\n",
        "        else:\n",
        "            embeddings = torch.empty(0, self.embedding_matrix.shape[1])\n",
        "\n",
        "        # Preparar labels\n",
        "        labels = {\n",
        "            \"punt_inicial\": torch.tensor(row[\"punt_inicial\"], dtype=torch.long),\n",
        "            \"punt_final\": torch.tensor(row[\"punt_final\"], dtype=torch.long),\n",
        "            \"capitalización\": torch.tensor(row[\"capitalización\"], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        return embeddings, labels"
      ],
      "metadata": {
        "id": "OBX8JOtxkzcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PADDING"
      ],
      "metadata": {
        "id": "x65RnXFBwyB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (embeddings, labels)\n",
        "    \"\"\"\n",
        "    embeddings_list, labels_list = zip(*batch)\n",
        "\n",
        "    # Pad embeddings (seq_len, embedding_dim) -> (batch_size, max_seq_len, embedding_dim)\n",
        "    embeddings_padded = pad_sequence(embeddings_list, batch_first=True, padding_value=0.0)\n",
        "\n",
        "    # Pad labels\n",
        "    punt_inicial = pad_sequence([l[\"punt_inicial\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    punt_final = pad_sequence([l[\"punt_final\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "    capitalizacion = pad_sequence([l[\"capitalización\"] for l in labels_list], batch_first=True, padding_value=-100).long()\n",
        "\n",
        "    return embeddings_padded, {\n",
        "        \"punt_inicial\": punt_inicial,\n",
        "        \"punt_final\": punt_final,\n",
        "        \"capitalizacion\": capitalizacion\n",
        "    }"
      ],
      "metadata": {
        "id": "a66xammXw1L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA LOADER"
      ],
      "metadata": {
        "id": "B627zo2Af9ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "#print(int(len(df_por_parrafos)*0.9)) # da 122454, despues agarrar tail de eso\n",
        "df_por_parrafos_test = df_por_parrafos\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Usar el nuevo dataset con embeddings dinámicos\n",
        "dataset = DynamicEmbeddingDataset(df_por_parrafos_test, tokenizer, model_bert)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "OX_Q5HbTHeSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Unidireccional"
      ],
      "metadata": {
        "id": "uhpy5I7nAGf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Unidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim = 768, hidden_dim1 = 128, hidden_dim2 = 32, num_layers= 2, dropout= 0.4):\n",
        "        super(RNN_Unidireccional, self).__init__()\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim1,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # Unidireccional\n",
        "            )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Salidas\n",
        "        self.punt_inicial_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 2)\n",
        "            )\n",
        "        self.punt_final_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "        self.capital_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Pasamos los embeddings por la lstm\n",
        "        outputs, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Les hacemos dropout a los outputs\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # Pasamos los outputs de la lstm por las salidas\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "rKk-_53cAtTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchview\n",
        "\n",
        "from torchview import draw_graph\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32, num_layers=2, dropout=0.4)\n",
        "\n",
        "model_graph = draw_graph(\n",
        "    model,\n",
        "    input_size=(1, 50, 768),  # (batch_size, seq_len, embedding_dim)\n",
        "    expand_nested= False,\n",
        "    graph_name='RNN_Unidireccional'\n",
        ")\n",
        "\n",
        "model_graph.visual_graph.render(\"rnn_arquitectura\", format=\"png\")\n",
        "\n",
        "print(\"✓ Imagen guardada como 'rnn_arquitectura.png'\")"
      ],
      "metadata": {
        "id": "Tcc9JPGEjCJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Bidireccional\n"
      ],
      "metadata": {
        "id": "swyed0hyHn2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Bidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim = 768, hidden_dim1 = 64, hidden_dim2 = 32, num_layers= 2, dropout= 0.4): #reducimos hidden_dim1 para equiparar con la red unidireccional( 64*2 = 128)\n",
        "        super(RNN_Bidireccional, self).__init__()\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim1,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # Bidireccional\n",
        "            )\n",
        "\n",
        "        lstm_output_dim = hidden_dim1 * 2 #128 igual que en la Unidireccinal\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Salidas\n",
        "        self.punt_inicial_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 2)\n",
        "            )\n",
        "        self.punt_final_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "        self.capital_ff = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim2, 4)\n",
        "            )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Pasamos los embeddings por la lstm\n",
        "        outputs, _ = self.lstm(embeddings)\n",
        "\n",
        "        # Les hacemos dropout a los outputs\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        # Pasamos los outputs de la lstm por las salidas\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }"
      ],
      "metadata": {
        "id": "shJOmhfqmuuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo\n",
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=64, hidden_dim2=32, num_layers=2, dropout=0.4)\n",
        "\n",
        "# Generar visualización\n",
        "model_graph = draw_graph(\n",
        "    model,\n",
        "    input_size=(1, 50, 768),  # (batch_size, seq_len, embedding_dim)\n",
        "    expand_nested=True,\n",
        "    graph_name='RNN_Bidireccional'\n",
        ")\n",
        "\n",
        "# Guardar como PNG\n",
        "model_graph.visual_graph.render(\"rnn_bidireccional\", format=\"png\")\n",
        "\n",
        "print(\"✓ Imagen guardada como 'rnn_bidireccional.png'\")"
      ],
      "metadata": {
        "id": "pkzzseBGlJBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ENTRENAR MODELO Unidireccional\n"
      ],
      "metadata": {
        "id": "gg_COwTOxHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "# Dividir en train/validation/test\n",
        "train_df, temp = train_test_split(df_por_parrafos, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "val_df, test_df = train_test_split(temp, test_size=0.5, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = DynamicEmbeddingDataset(train_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "val_dataset = DynamicEmbeddingDataset(val_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "3zALNZA7iF2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Mover BERT a la misma device\n",
        "model_bert = model_bert.to(device)\n",
        "\n",
        "# Optimizador con weight decay (regularización L2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.2, patience=3\n",
        ")\n",
        "\n",
        "# Pesos diferentes para cada tarea (opcional, ajustar según importancia)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1)\n",
        "\n",
        "# Gradient clipping\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "early_stop_patience = 30\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ENTRENAMIENTO\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_loss_inicial = 0\n",
        "    total_loss_final = 0\n",
        "    total_loss_cap = 0\n",
        "\n",
        "    for embeddings, labels in train_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) #testeando, le pongo 100* a los inicial porque es muy chica\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        # Pérdida total\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping para evitar gradientes explosivos\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        total_loss_inicial += loss_inicial.item()\n",
        "        total_loss_final += loss_final.item()\n",
        "        total_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # VALIDACIÓN\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    val_loss_inicial = 0\n",
        "    val_loss_final = 0\n",
        "    val_loss_cap = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in val_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "\n",
        "            loss_inicial = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])  #testeando, le pongo 100* a los inicial porque es muy chica\n",
        "\n",
        "            loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "\n",
        "            loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "            loss = loss_inicial + loss_final + loss_cap\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            val_loss_inicial += loss_inicial.item()\n",
        "            val_loss_final += loss_final.item()\n",
        "            val_loss_cap += loss_cap.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # Actualizar learning rate\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Imprimir métricas detalladas\n",
        "    print(f\"Época {epoch+1}/{num_epochs}\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Train - Inicial: {total_loss_inicial/len(train_loader):.4f}, Final: {total_loss_final/len(train_loader):.4f}, Cap: {total_loss_cap/len(train_loader):.4f}\")\n",
        "    print(f\"  Val   - Inicial: {val_loss_inicial/len(val_loader):.4f}, Final: {val_loss_final/len(val_loader):.4f}, Cap: {val_loss_cap/len(val_loader):.4f}\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "    # Guardar cada epoca\n",
        "    ruta_epoca = f\"/content/drive/MyDrive/colab/modelosU/modelo_epoca_{epoch+1}.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': avg_val_loss,\n",
        "        }, ruta_epoca)\n",
        "\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        ruta_mejor_modelo = \"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': avg_val_loss,\n",
        "        }, ruta_mejor_modelo)\n",
        "        print(f\"  ✓ Mejor modelo actualizado (Val Loss: {avg_val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  Sin mejora ({patience_counter}/{early_stop_patience})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= early_stop_patience:\n",
        "        print(f\"\\nEarly stopping en época {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "print(f\"\\nEntrenamiento completado!\")\n",
        "print(f\"Mejor modelo guardado en: {ruta_mejor_modelo}\")\n",
        "print(f\"Mejor Val Loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "T6miBWEAxWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUACION UNIDIRECCIONAL"
      ],
      "metadata": {
        "id": "AOMWXohoqPyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# ---------------------------------------\n",
        "# EVALUACIÓN SOBRE EL TEST SET\n",
        "# ---------------------------------------\n",
        "\n",
        "test_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\") #modelosU para unidireccional\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n=== Evaluando en TEST SET con mejor modelo Unidireccional ===\\n\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "total_test_loss = 0\n",
        "loss_init = 0\n",
        "loss_final = 0\n",
        "loss_cap = 0\n",
        "\n",
        "# Para F1\n",
        "all_true_init  = []\n",
        "all_pred_init  = []\n",
        "\n",
        "all_true_final = []\n",
        "all_pred_final = []\n",
        "\n",
        "all_true_cap   = []\n",
        "all_pred_cap   = []\n",
        "\n",
        "correct_init = 0\n",
        "correct_final = 0\n",
        "correct_cap = 0\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        # Loss\n",
        "        l_i = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # 2* para equipararla con las otras\n",
        "        l_f = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        l_c = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = l_i + l_f + l_c\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "        loss_init += l_i.item()\n",
        "        loss_final += l_f.item()\n",
        "        loss_cap += l_c.item()\n",
        "\n",
        "        # Predicciones (batch, seq)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        # Guardar para F1\n",
        "        all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "        all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "        all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "        all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "        all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "        all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "        # Accuracy por tarea\n",
        "        # PREDICCIONES → (batch, seq, clases)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        correct_init += ((pred_init == labels[\"punt_inicial\"]) * mask).sum().item()\n",
        "        correct_final += ((pred_final == labels[\"punt_final\"]) * mask).sum().item()\n",
        "        correct_cap += ((pred_cap == labels[\"capitalizacion\"]) * mask).sum().item()\n",
        "\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#   CÁLCULO DE F1 MACRO\n",
        "# ----------------------------\n",
        "\n",
        "# Puntuación inicial: 2 clases → ¿, \"\"\n",
        "f1_init = f1_score(all_true_init, all_pred_init, average=\"macro\")\n",
        "\n",
        "# Puntuación final: 4 clases → ,  .  ?  \"\"\n",
        "f1_final = f1_score(all_true_final, all_pred_final, average=\"macro\")\n",
        "\n",
        "# Capitalización: 4 clases → 0 1 2 3\n",
        "f1_cap = f1_score(all_true_cap, all_pred_cap, average=\"macro\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#     PRINT RESULTADOS\n",
        "# ----------------------------\n",
        "print(\"\\n===== RESULTADOS TEST =====\")\n",
        "print(f\"Test Loss Total: {total_test_loss / len(test_loader):.4f}\")\n",
        "print(f\"  Inicial: {loss_init / len(test_loader):.4f}\")\n",
        "print(f\"  Final  : {loss_final / len(test_loader):.4f}\")\n",
        "print(f\"  Capital: {loss_cap / len(test_loader):.4f}\")\n",
        "\n",
        "print(\"\\n===== F1 MACRO =====\")\n",
        "print(f\"  Puntuación inicial (¿, \\\"\\\"): {f1_init:.4f}\")\n",
        "print(f\"  Puntuación final   (,, ., ?, \\\"\\\"): {f1_final:.4f}\")\n",
        "print(f\"  Capitalización     (0,1,2,3): {f1_cap:.4f}\")\n",
        "\n",
        "print(\"\\n===== ACCURACY =====\")\n",
        "print(f\" Inicial: {correct_init / total_tokens:.4f}\")\n",
        "print(f\" Final : {correct_final / total_tokens:.4f}\")\n",
        "print(f\" Capital: {correct_cap / total_tokens:.4f}\")"
      ],
      "metadata": {
        "id": "NygS9JcBqNwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EVALUACIÓN BIDIRECCIONAL\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p0xtKHht1vht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model = RNN_Bidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# ---------------------------------------\n",
        "# EVALUACIÓN SOBRE EL TEST SET\n",
        "# ---------------------------------------\n",
        "\n",
        "test_dataset = DynamicEmbeddingDataset(test_df.reset_index(drop=True), tokenizer, model_bert)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\") #modelosB para bidireccional\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n=== Evaluando en TEST SET con mejor modelo Bidireccional ===\\n\")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "total_test_loss = 0\n",
        "loss_init = 0\n",
        "loss_final = 0\n",
        "loss_cap = 0\n",
        "\n",
        "# Para F1\n",
        "all_true_init  = []\n",
        "all_pred_init  = []\n",
        "\n",
        "all_true_final = []\n",
        "all_pred_final = []\n",
        "\n",
        "all_true_cap   = []\n",
        "all_pred_cap   = []\n",
        "\n",
        "correct_init = 0\n",
        "correct_final = 0\n",
        "correct_cap = 0\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for embeddings, labels in test_loader:\n",
        "        embeddings = embeddings.to(device)\n",
        "        labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "        outputs = model(embeddings)\n",
        "\n",
        "        # Loss\n",
        "        l_i = 2*criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"]) # 2* para equipararla con las otras\n",
        "        l_f = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        l_c = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = l_i + l_f + l_c\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "        loss_init += l_i.item()\n",
        "        loss_final += l_f.item()\n",
        "        loss_cap += l_c.item()\n",
        "\n",
        "        # Predicciones (batch, seq)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        # Guardar para F1\n",
        "        all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "        all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "        all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "        all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "        all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "        all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "        # Accuracy por tarea\n",
        "        # PREDICCIONES → (batch, seq, clases)\n",
        "        pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "        pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "        pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "        mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "        correct_init += ((pred_init == labels[\"punt_inicial\"]) * mask).sum().item()\n",
        "        correct_final += ((pred_final == labels[\"punt_final\"]) * mask).sum().item()\n",
        "        correct_cap += ((pred_cap == labels[\"capitalizacion\"]) * mask).sum().item()\n",
        "\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#   CÁLCULO DE F1 MACRO\n",
        "# ----------------------------\n",
        "\n",
        "# Puntuación inicial: 2 clases → ¿, \"\"\n",
        "f1_init = f1_score(all_true_init, all_pred_init, average=\"macro\")\n",
        "\n",
        "# Puntuación final: 4 clases → ,  .  ?  \"\"\n",
        "f1_final = f1_score(all_true_final, all_pred_final, average=\"macro\")\n",
        "\n",
        "# Capitalización: 4 clases → 0 1 2 3\n",
        "f1_cap = f1_score(all_true_cap, all_pred_cap, average=\"macro\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "#     PRINT RESULTADOS\n",
        "# ----------------------------\n",
        "print(\"\\n===== RESULTADOS TEST =====\")\n",
        "print(f\"Test Loss Total: {total_test_loss / len(test_loader):.4f}\")\n",
        "print(f\"  Inicial: {loss_init / len(test_loader):.4f}\")\n",
        "print(f\"  Final  : {loss_final / len(test_loader):.4f}\")\n",
        "print(f\"  Capital: {loss_cap / len(test_loader):.4f}\")\n",
        "\n",
        "print(\"\\n===== F1 MACRO =====\")\n",
        "print(f\"  Puntuación inicial (¿, \\\"\\\"): {f1_init:.4f}\")\n",
        "print(f\"  Puntuación final   (,, ., ?, \\\"\\\"): {f1_final:.4f}\")\n",
        "print(f\"  Capitalización     (0,1,2,3): {f1_cap:.4f}\")\n",
        "\n",
        "print(\"\\n===== ACCURACY =====\")\n",
        "print(f\" Inicial: {correct_init / total_tokens:.4f}\")\n",
        "print(f\" Final : {correct_final / total_tokens:.4f}\")\n",
        "print(f\" Capital: {correct_cap / total_tokens:.4f}\")\n"
      ],
      "metadata": {
        "id": "q4tmUjMF1hGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OTROS"
      ],
      "metadata": {
        "id": "uJTN0UQZ1pyV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80d62e9d"
      },
      "source": [
        "def check_df_consistency(dataframe):\n",
        "    \"\"\"\n",
        "    Verifica la consistencia de las longitudes de 'token', 'punt_inicial',\n",
        "    'punt_final' y 'capitalización' en cada fila del DataFrame.\n",
        "    \"\"\"\n",
        "    mismatched_instances = []\n",
        "    for idx, row in dataframe.iterrows():\n",
        "        len_tokens = len(row[\"token\"])\n",
        "        len_punt_inicial = len(row[\"punt_inicial\"])\n",
        "        len_punt_final = len(row[\"punt_final\"])\n",
        "        len_capitalizacion = len(row[\"capitalización\"])\n",
        "\n",
        "        if not (len_tokens == len_punt_inicial == len_punt_final == len_capitalizacion):\n",
        "            mismatched_instances.append(row[\"instancia_id\"])\n",
        "    return mismatched_instances\n",
        "\n",
        "print(\"Realizando verificación de consistencia en df_por_parrafos...\")\n",
        "inconsistent_ids = check_df_consistency(df_por_parrafos)\n",
        "\n",
        "if inconsistent_ids:\n",
        "    print(f\"¡Advertencia! Se encontraron {len(inconsistent_ids)} instancias con longitudes inconsistentes.\")\n",
        "    print(f\"Primeras 10 instancia_id con problemas: {inconsistent_ids[:10]}\")\n",
        "    print(\"Es recomendable filtrar estas instancias antes de proceder con el entrenamiento.\")\n",
        "else:\n",
        "    print(\"¡Excelente! Todas las instancias en df_por_parrafos tienen longitudes consistentes.\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"##INFERENCIA - CARGAR MODELO Y HACER PREDICCIONES\"\"\"\n",
        "\n",
        "def predecir_parrafo(texto, modelo, tokenizer, bert_model, device):\n",
        "    \"\"\"\n",
        "    Recibe un párrafo de texto y devuelve las predicciones de puntuación y capitalización.\n",
        "\n",
        "    Args:\n",
        "        texto: string con el texto a procesar\n",
        "        modelo: modelo entrenado\n",
        "        tokenizer: tokenizer de BERT\n",
        "        bert_model: modelo BERT para embeddings\n",
        "        device: 'cuda' o 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        dict con tokens y sus predicciones\n",
        "    \"\"\"\n",
        "    modelo.eval()\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "    # Obtener embeddings\n",
        "    token_embeddings = []\n",
        "    for token in tokens:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "            token_id = tokenizer.unk_token_id\n",
        "        emb = bert_model.embeddings.word_embeddings.weight[token_id].detach()\n",
        "        token_embeddings.append(emb)\n",
        "\n",
        "    # Convertir a tensor y añadir dimensión de batch\n",
        "    embeddings = torch.stack(token_embeddings).unsqueeze(0).to(device)  # (1, seq_len, 768)\n",
        "\n",
        "    # Hacer predicción\n",
        "    with torch.no_grad():\n",
        "        outputs = modelo(embeddings)\n",
        "\n",
        "    # Obtener las clases predichas\n",
        "    punt_inicial_pred = torch.argmax(outputs[\"puntuación inicial\"], dim=-1).squeeze().cpu().numpy()\n",
        "    punt_final_pred = torch.argmax(outputs[\"puntuación final\"], dim=-1).squeeze().cpu().numpy()\n",
        "    capital_pred = torch.argmax(outputs[\"capitalización\"], dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Mapeos de clases a etiquetas legibles\n",
        "    punt_inicial_map = {0: \"Sin puntuación\", 1: \"Con puntuación\"}\n",
        "    punt_final_map = {0: \"Ninguna\", 1: \"Punto\", 2: \"Coma\", 3: \"Otro\"}\n",
        "    capital_map = {0: \"Minúscula\", 1: \"Primera mayúscula\", 2: \"Todo mayúsculas\", 3: \"Otro\"}\n",
        "\n",
        "    # Crear resultado\n",
        "    resultados = []\n",
        "    for i, token in enumerate(tokens):\n",
        "        resultados.append({\n",
        "            \"token\": token,\n",
        "            \"puntuación_inicial\": punt_inicial_map.get(int(punt_inicial_pred[i]), \"Desconocido\"),\n",
        "            \"puntuación_final\": punt_final_map.get(int(punt_final_pred[i]), \"Desconocido\"),\n",
        "            \"capitalización\": capital_map.get(int(capital_pred[i]), \"Desconocido\")\n",
        "        })\n",
        "\n",
        "    return resultados\n",
        "\n",
        "def reconstruir_texto(resultados):\n",
        "    \"\"\"\n",
        "    Reconstruye el texto con puntuación y capitalización a partir de las predicciones.\n",
        "    \"\"\"\n",
        "    texto_reconstruido = \"\"\n",
        "\n",
        "    for resultado in resultados:\n",
        "        token = resultado[\"token\"]\n",
        "\n",
        "        # Aplicar capitalización\n",
        "        if resultado[\"capitalización\"] == \"Primera mayúscula\":\n",
        "            token = token.capitalize()\n",
        "        elif resultado[\"capitalización\"] == \"Todo mayúsculas\":\n",
        "            token = token.upper()\n",
        "\n",
        "        # Añadir puntuación inicial\n",
        "        if resultado[\"puntuación_inicial\"] == \"Con puntuación\":\n",
        "            # Aquí podrías decidir qué puntuación añadir (ej. ¿, ¡, etc.)\n",
        "            pass\n",
        "\n",
        "        # Añadir el token\n",
        "        if token.startswith(\"##\"):\n",
        "            texto_reconstruido += token[2:]\n",
        "        else:\n",
        "            if texto_reconstruido:\n",
        "                texto_reconstruido += \" \" + token\n",
        "            else:\n",
        "                texto_reconstruido += token\n",
        "\n",
        "        # Añadir puntuación final\n",
        "        if resultado[\"puntuación_final\"] == \"Punto\":\n",
        "            texto_reconstruido += \".\"\n",
        "        elif resultado[\"puntuación_final\"] == \"Coma\":\n",
        "            texto_reconstruido += \",\"\n",
        "\n",
        "    return texto_reconstruido"
      ],
      "metadata": {
        "id": "jKz-D7Vl_C8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar el mejor modelo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Cargando el mejor modelo...\")\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/modelos/modelo_final_30e.pt\", map_location=device)\n",
        "modelo_cargado = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3)\n",
        "modelo_cargado.load_state_dict(checkpoint['model_state_dict'])\n",
        "modelo_cargado = modelo_cargado.to(device)\n",
        "print(f\"Modelo cargado (época {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n",
        "\n",
        "# Ejemplo de uso\n",
        "texto_ejemplo = \"había perdido la esperanza pero seguía creyendo porque sabía que era un tenista demasiado bueno para no ganar también aquí pero no me he alegrado cuando  ha perdido lo respeto demasiado estabas nervioso antes de la final\"\n",
        "print(f\"\\nTexto de entrada: '{texto_ejemplo}'\\n\")\n",
        "\n",
        "predicciones = predecir_parrafo(texto_ejemplo, modelo_cargado, tokenizer, model_bert, device)\n",
        "\n",
        "# Mostrar predicciones detalladas\n",
        "print(\"Predicciones por token:\")\n",
        "print(\"-\" * 80)\n",
        "for pred in predicciones:\n",
        "    print(f\"Token: {pred['token']:15s} | Punt. Inicial: {pred['puntuación_inicial']:20s} | \"\n",
        "          f\"Punt. Final: {pred['puntuación_final']:10s} | Cap: {pred['capitalización']}\")\n",
        "\n",
        "# Reconstruir texto\n",
        "texto_reconstruido = reconstruir_texto(predicciones)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Texto reconstruido: '{texto_reconstruido}'\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "vGf4pyY9_JId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_por_parrafos)"
      ],
      "metadata": {
        "id": "6nfDDceeA72h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# ==========================================\n",
        "# FUNCIÓN PARA MATRICES DE CONFUSIÓN CON SEABORN\n",
        "# ==========================================\n",
        "\n",
        "def plot_confusion_matrix_styled(y_true, y_pred, labels, title, filename, cmap='Blues'):\n",
        "    \"\"\"\n",
        "    Genera una matriz de confusión con estilo mejorado usando seaborn\n",
        "\n",
        "    Args:\n",
        "        y_true: etiquetas verdaderas\n",
        "        y_pred: etiquetas predichas\n",
        "        labels: nombres de las clases\n",
        "        title: título del gráfico\n",
        "        filename: ruta donde guardar la imagen\n",
        "        cmap: mapa de colores (puede ser nombre o lista de colores)\n",
        "    \"\"\"\n",
        "    # Calcular matriz de confusión\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Crear figura\n",
        "    plt.figure(figsize=(8, 7))\n",
        "\n",
        "    # Usar seaborn para mejor visualización\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
        "                cbar_kws={'label': 'Cantidad'},\n",
        "                linewidths=0.5, linecolor='gray',\n",
        "                xticklabels=labels, yticklabels=labels,\n",
        "                annot_kws={'size': 14, 'weight': 'bold'})\n",
        "\n",
        "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.ylabel('Real', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Predicho', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Ajustar etiquetas\n",
        "    plt.xticks(fontsize=11, rotation=0)\n",
        "    plt.yticks(fontsize=11, rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"✓ Matriz guardada en: {filename}\")\n",
        "\n",
        "    # Calcular y mostrar métricas\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Total predicciones: {np.sum(cm):,}\")\n",
        "    print()\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# GENERAR MATRICES PARA AMBOS MODELOS\n",
        "# ==========================================\n",
        "\n",
        "def generate_all_confusion_matrices(model, test_loader, device, model_name=\"Unidireccional\"):\n",
        "    \"\"\"\n",
        "    Genera las 3 matrices de confusión para un modelo.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Recolectar predicciones\n",
        "    all_true_init = []\n",
        "    all_pred_init = []\n",
        "    all_true_final = []\n",
        "    all_pred_final = []\n",
        "    all_true_cap = []\n",
        "    all_pred_cap = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in test_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = {k: v.to(device) for k, v in labels.items()}\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "\n",
        "            pred_init = outputs[\"puntuación inicial\"].argmax(dim=-1)\n",
        "            pred_final = outputs[\"puntuación final\"].argmax(dim=-1)\n",
        "            pred_cap = outputs[\"capitalización\"].argmax(dim=-1)\n",
        "\n",
        "            mask = (labels[\"punt_inicial\"] != -100)\n",
        "\n",
        "            all_true_init.extend(labels[\"punt_inicial\"][mask].cpu().tolist())\n",
        "            all_pred_init.extend(pred_init[mask].cpu().tolist())\n",
        "\n",
        "            all_true_final.extend(labels[\"punt_final\"][mask].cpu().tolist())\n",
        "            all_pred_final.extend(pred_final[mask].cpu().tolist())\n",
        "\n",
        "            all_true_cap.extend(labels[\"capitalizacion\"][mask].cpu().tolist())\n",
        "            all_pred_cap.extend(pred_cap[mask].cpu().tolist())\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  MATRICES DE CONFUSIÓN - {model_name.upper()}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Crear mapas de color personalizados\n",
        "    # Color 1: #9370DB (Púrpura medio)\n",
        "    cmap1 = sns.light_palette(\"#9370DB\", as_cmap=True)\n",
        "\n",
        "    # Color 2: #6495ED (Azul aciano)\n",
        "    cmap2 = sns.light_palette(\"#6495ED\", as_cmap=True)\n",
        "\n",
        "    # Color 3: #DA70D6 (Orquídea)\n",
        "    cmap3 = sns.light_palette(\"#DA70D6\", as_cmap=True)\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. PUNTUACIÓN INICIAL - Color #9370DB\n",
        "    # ==========================================\n",
        "    cm_init = plot_confusion_matrix_styled(\n",
        "        all_true_init, all_pred_init,\n",
        "        labels=['0\\n(Sin puntuación)', '1\\n(¿)'],\n",
        "        title=f'Matriz de confusión para punt_inicial\\n{model_name}',\n",
        "        filename=f'/content/drive/MyDrive/colab/cm_inicial_{model_name.lower()}.png',\n",
        "        cmap=cmap1\n",
        "    )\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. PUNTUACIÓN FINAL - Color #6495ED\n",
        "    # ==========================================\n",
        "    cm_final = plot_confusion_matrix_styled(\n",
        "        all_true_final, all_pred_final,\n",
        "        labels=['0\\n(Ninguna)', '1\\n(,)', '2\\n(.)', '3\\n(?)'],\n",
        "        title=f'Matriz de confusión para punt_final\\n{model_name}',\n",
        "        filename=f'/content/drive/MyDrive/colab/cm_final_{model_name.lower()}.png',\n",
        "        cmap=cmap2\n",
        "    )\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. CAPITALIZACIÓN - Color #DA70D6\n",
        "    # ==========================================\n",
        "    cm_cap = plot_confusion_matrix_styled(\n",
        "        all_true_cap, all_pred_cap,\n",
        "        labels=['0\\n(Minúscula)', '1\\n(Primera May.)', '2\\n(Todo May.)', '3\\n(Otro)'],\n",
        "        title=f'Matriz de confusión para capitalización\\n{model_name}',\n",
        "        filename=f'/content/drive/MyDrive/colab/cm_capitalizacion_{model_name.lower()}.png',\n",
        "        cmap=cmap3\n",
        "    )\n",
        "\n",
        "    return cm_init, cm_final, cm_cap\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# EJECUTAR PARA MODELO UNIDIRECCIONAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  GENERANDO MATRICES - MODELO UNIDIRECCIONAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_uni = RNN_Unidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32, num_layers=2, dropout=0.4)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_uni = model_uni.to(device)\n",
        "\n",
        "checkpoint_uni = torch.load(\"/content/drive/MyDrive/colab/modelosU/mejor_modelo.pt\")\n",
        "model_uni.load_state_dict(checkpoint_uni['model_state_dict'])\n",
        "model_uni.eval()\n",
        "\n",
        "cm_init_u, cm_final_u, cm_cap_u = generate_all_confusion_matrices(\n",
        "    model_uni, test_loader, device, model_name=\"Unidireccional\"\n",
        ")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# EJECUTAR PARA MODELO BIDIRECCIONAL\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  GENERANDO MATRICES - MODELO BIDIRECCIONAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model_bi = RNN_Bidireccional(embedding_dim=768, hidden_dim1=128, hidden_dim2=32,  num_layers=2, dropout=0.4)\n",
        "model_bi = model_bi.to(device)\n",
        "\n",
        "checkpoint_bi = torch.load(\"/content/drive/MyDrive/colab/modelosB/mejor_modelo.pt\")\n",
        "model_bi.load_state_dict(checkpoint_bi['model_state_dict'])\n",
        "model_bi.eval()\n",
        "\n",
        "cm_init_b, cm_final_b, cm_cap_b = generate_all_confusion_matrices(\n",
        "    model_bi, test_loader, device, model_name=\"Bidireccional\"\n",
        ")"
      ],
      "metadata": {
        "id": "EhgMjLGKt3Kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}